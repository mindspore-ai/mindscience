is_training: False
use_jit: True
fixed_size: True
seq_length: 1792
data:
  max_msa_clusters: 512
  max_extra_msa: 5120
  block_deletion:
    msa_fraction_per_block: 0.3
    num_blocks: 5
    randomize_num_blocks: True
  random_recycle: False
  masked_msa:
    use_masked_msa: True
    profile_prob: 0.1
    same_prob: 0.1
    uniform_prob: 0.1
  msa_cluster_features: True
  num_recycle: 4
  reduce_msa_clusters_by_max_templates: True
  use_templates: True
  masked_msa_replace_fraction: 0.15
  max_templates: 4
  num_ensemble: 1
  subsample_templates: True
  keep_extra: True
model:
  msa_channel: 256
  pair_channel: 128
  extra_msa_channel: 64
  max_relative_feature: 32
  recycle_features: True
  recycle_pos: True
  seq_channel: 384
  prev_pos:
    min_bin: 3.25
    max_bin: 20.75
    num_bins: 15
  common:
    target_feat_dim: 22
    msa_feat_dim: 49
    dgram_dim: 15
    pair_in_dim: 65
    msa_first_row_dim: 256
    prev_pair_dim: 128
    extra_msa_dim: 25
    template_feat_dim: 57
  template:
    enabled: True
    embed_torsion_angles: True
    use_template_unit_vector: True
    attention:
      gating: False
      key_dim: 64
      num_head: 4
      value_dim: 64
    dgram_features:
      min_bin: 3.25
      max_bin: 50.75
      num_bins: 39
    template_pair_stack:
      num_block: 2
      triangle_attention_starting_node:
        dropout_rate: 0.25
        gating: True
        key_dim: 64
        num_head: 4
        orientation: 'per_row'
        shared_dropout: True
        value_dim: 64
      triangle_attention_ending_node:
        dropout_rate: 0.25
        gating: True
        key_dim: 64
        num_head: 4
        orientation: 'per_column'
        shared_dropout: True
        value_dim: 64
      triangle_multiplication_outgoing:
        dropout_rate: 0.25
        equation: 'ikc,jkc->ijc'
        num_intermediate_channel: 64
        orientation: 'per_row'
        shared_dropout: True
      triangle_multiplication_incoming:
        dropout_rate: 0.25
        equation: 'kjc,kic->ijc'
        num_intermediate_channel: 64
        orientation: 'per_row'
        shared_dropout: True
      pair_transition:
        dropout_rate: 0.0
        num_intermediate_factor: 2
        orientation: 'per_row'
        shared_dropout: True
  evoformer:
    msa_stack_num: 48
    extra_msa_stack_num: 4
    msa_row_attention_with_pair_bias:
      dropout_rate: 0.15  # 0.15
      gating: True
      num_head: 8
      orientation: 'per_row'
      shared_dropout: True
    msa_column_attention:
      dropout_rate: 0.0
      gating: True
      num_head: 8
      orientation: 'per_column'
      shared_dropout: True
    msa_transition:
      dropout_rate: 0.0
      num_intermediate_factor: 4
      orientation: 'per_row'
      shared_dropout: True
    outer_product_mean:
      chunk_size: 128
      dropout_rate: 0.0
      num_outer_channel: 32
      orientation: 'per_row'
      shared_dropout: True
    triangle_attention_starting_node:
      dropout_rate: 0.25  # 0.25
      gating: True
      num_head: 4
      orientation: 'per_row'
      shared_dropout: True
    triangle_attention_ending_node:
      dropout_rate: 0.25  # 0.25
      gating: True
      num_head: 4
      orientation: 'per_column'
      shared_dropout: True
    triangle_multiplication_outgoing:
      dropout_rate: 0.25  # 0.25
      equation: 'ikc,jkc->ijc'
      num_intermediate_channel: 128
      orientation: 'per_row'
      shared_dropout: True
    triangle_multiplication_incoming:
      dropout_rate: 0.25  # 0.25
      equation: 'kjc,kic->ijc'
      num_intermediate_channel: 128
      orientation: 'per_row'
      shared_dropout: True
    pair_transition:
      dropout_rate: 0.0
      num_intermediate_factor: 4
      orientation: 'per_row'
      shared_dropout: True
  structure_module:
    num_layer: 8
    fape:
      clamp_distance: 10.0
      clamp_type: 'relu'
      loss_unit_distance: 10.0
    angle_norm_weight: 0.01
    chi_weight: 0.5
    clash_overlap_tolerance: 1.5
    compute_in_graph_metrics: True
    dropout: 0.1
    num_channel: 384
    num_head: 12
    num_layer_in_transition: 3
    num_point_qk: 4
    num_point_v: 8
    num_scalar_qk: 16
    num_scalar_v: 16
    position_scale: 10.0
    sidechain:
      atom_clamp_distance: 10.0
      num_channel: 128
      num_residual_block: 2
      weight_frac: 0.5
      length_scale: 10.
    structural_violation_loss_weight: 1.0
    violation_tolerance_factor: 12.0
    weight: 1.0
  heads:
    resolution: 1
    predicted_lddt:
      filter_by_resolution: True
      max_resolution: 3.0
      min_resolution: 0.1
      num_bins: 50
      num_channels: 128
      weight: 0.01
    distogram:
      first_break: 2.3125
      last_break: 21.6875
      num_bins: 64
      weight: 0.3
    masked_msa:
      num_output: 23
      weight: 2.0
    predicted_aligned_error:
      max_error_bin: 31.0
      num_bins: 64
      num_channels: 128
      filter_by_resolution: True
      min_resolution: 0.1
      max_resolution: 3.0
      weight: 0.0
    experimentally_resolved:
      filter_by_resolution: True
      max_resolution: 3.0
      min_resolution: 0.1
      weight: 0.01
    structure_module:
      fape:
        clamp_distance: 10.0
        loss_unit_distance: 10.0
      angle_norm_weight: 0.01
      chi_weight: 0.5
      clash_overlap_tolerance: 1.5
      sidechain:
        atom_clamp_distance: 10.0
        weight_frac: 0.5
        length_scale: 10.0
        structural_violation_loss_weight: 1.0
      violation_tolerance_factor: 12.0
  slice:
    template_embedding: 64 # seq len * seq len
    template_pair_stack:
      triangle_attention_starting_node: 64 # seq len
      triangle_attention_ending_node: 64 # seq len
      pair_transition: 8 # seq len
    extra_msa_stack:
      msa_transition: 8 # 5120
      msa_row_attention_with_pair_bias: 512 # 5120
      msa_column_global_attention: 64 # seq len
      outer_product_mean: 8 # seq len
      triangle_attention_starting_node: 64 # seq len
      triangle_attention_ending_node: 64 # seq len
      pair_transition: 8 # seq len
    msa_stack:
      msa_transition: 8
      msa_row_attention_with_pair_bias: 64
      msa_column_attention: 64
      outer_product_mean: 8
      triangle_attention_starting_node: 64
      triangle_attention_ending_node: 64
      pair_transition: 8
