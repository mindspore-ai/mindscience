model:
  model_config:
    type: BertConfig
    use_one_hot_embeddings: False
    num_labels: 2
    dropout_prob: 0.02
    batch_size: 16
    seq_length: 2048 #length of input sentence
    vocab_size: 25426  #size of vocab
    hidden_size: 256 #size of text feature
    num_hidden_layers: 6 #model depth
    num_attention_heads: 4 #number of attention heads
    intermediate_size: 512  #hidden_size*4
    hidden_act: "relu" #activation
    post_layernorm_residual: True #select postlayernorm or prelayernorm
    hidden_dropout_prob: 0.02
    attention_probs_dropout_prob: 0.02
    max_position_embeddings: 2048
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: False
    use_past: False
    use_moe: False
    compute_dtype: "float32"
    checkpoint_name_or_path: ""
  arch:
    type: BertForPreTraining

lr_schedule:
  type: LinearWithWarmUpLR
  learning_rate: 0.00005  # 5e-5
  lr_end: 0.0000000001  # 1e-6
  warmup_steps: 0
  total_steps: -1 # -1 means it will load the total steps of the dataset
layer_scale: False
layer_decay: 0.65

optimizer:
  type: adamw
  weight_decay: 0.001
  eps: 0.00000001  # 1e-8
lr_scale: False
lr_scale_factor: 256

callbacks:
  - type: MFLossMonitor
  - type: CheckpointMonitor
    prefix: "mindformers"
    keep_checkpoint_max: 100
    save_checkpoint_steps: 500
    integrated_save: True
    async_save: False

runner_config:
  epochs: 1
  batch_size: 12
  sink_mode: False
  sink_size: 2
runner_wrapper:
  type: TrainOneStepCell

# parallel
use_parallel: False
parallel:
  parallel_mode: 0 # 0-standalone, 1-semi, 2-auto, 3-hybrid
  gradients_mean: True
  enable_alltoall: False
  full_batch: False
  search_mode: "sharding_propagation"
  enable_parallel_optimizer: False
  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
parallel_config:
  data_parallel: 1
  model_parallel: 1
  expert_parallel: 1
  pipeline_stage: 1
  micro_batch_num: 1
  gradient_aggregation_group: 4
micro_batch_interleave_num: 1

# profile
profile: False
profile_start_step: 1
profile_stop_step: 10
init_start_profile: False
profile_communication: False
profile_memory: True

# Trainer
trainer:
  type: TokenClassificationTrainer
  model_name: txtcls_bert_base_uncased
do_eval: False

# train dataset
train_dataset: &train_dataset
  input_columns: ["input_ids", "input_mask", "segment_ids", "label_ids"]
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 16
  repeat: 1
  numa_enable: False
  prefetch_size: 1
  seed: 42
train_dataset_task:
  type: TextClassificationDataset
  dataset_config: *train_dataset

# eval dataset
eval_dataset: &eval_dataset
  input_columns: ["input_ids", "input_mask", "segment_ids", "label_ids"]
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 64
  repeat: 1
  numa_enable: False
  prefetch_size: 1
  seed: 42
eval_dataset_task:
  type: TextClassificationDataset
  dataset_config: *eval_dataset

# processor
processor:
  return_tensors: ms
  tokenizer:
    cls_token: '[CLS]'
    do_basic_tokenize: True
    do_lower_case: True
    mask_token: '[MASK]'
    pad_token: '[PAD]'
    sep_token: '[SEP]'
    type: BertTokenizer
    unk_token: '[UNK]'
  type: BertProcessor
top_k: 1