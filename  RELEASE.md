# **MindScience 0.1.0**

## MindScience 0.1.0 Release Notes

### Major Features and Improvements

#### MindSpore Elec

* Provide physics-driven and data-driven neural network for electromagnetic simulation
* Support CSG geometry model construction and CAD format data processing
* Include multiple scale filtering and dynamic adaptive weighted loss for improving performance
* Provide visualization tools for electromagnetic fields and scattering parameters

#### **MindSPONGE**

* Provide basic toolkits for molecular simulation, including MSA dataset, molecular pre-trained model(service on HUAWEI CLOUD), molecular dynamics。
* MSA dataset：Multiple Sequence Alignment Dataset for protein structure and function research
* Molecular Pre-trained Model：Trained with 1.7 billion compounds and its downstream tasks achieve SOTA
* Molecular Dynamics：Support basic MD functions，such as NPT, NVT, NVE and Minimization

#### **MindChemistry**

* Provide a **high-entropy alloy composition design approach**: Based on generation model and ranking model generating high-entropy alloy composition candidates and candidates' ranks, this approach constructs an active learning workflow for enabling chemistry experts to accelerate design of novel materials.
* Provide **molecular energy prediction models**: Based on equivariant computing library, the property prediction models NequIP and Allegro are trained effectively and infer molecular energy with high accuracy given atomic information.
* Provide an **electronic Structure Prediction model**: We integrate the DeephE3nnn model, an equivariant neural network based on E3, to predict a Hamiltonian by using the structure of atoms.
* Provide a **crystalline material properties prediction model**: We integrate the Matformer model, based on graph neural networks and Transformer architectures, for predicting various properties of crystalline materials.
* Provide an **equivariant computing library**: We provide basic modules such as Irreps, Spherical Harmonics as well as user-friendly equivariant layers such as equivarant Activation and Linear layers for easy construction of equivariant neural networks.

#### Contributors

Thanks goes to these wonderful people:

yufan, gaoyiqin, wangzidong, yangkang, lujiale, shibeiji, liuhongsheng, liyang, wengbingya, chuhaotian, huangxiang, wangmin, niningxi, zhangxinfeng, yujialiang, qianjiahong, chenmengyun, yanglijiang, yangyi, huangyupeng, xiayijie, zhangjun, linxiaohan, chendiqing, gongyue, gengchenhua, linghejing, yanchaojie, suyun, wujian, caowenbin

Contributions of any kind are welcome!
