{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用PINNs求解泊松方程\n",
    "\n",
    "本案例要求**MindSpore版本 >= 2.0.0**调用如下接口: *mindspore.jit，mindspore.jit_class，mindspore.jacrev*。\n",
    "\n",
    "## 环境安装\n",
    "\n",
    "本案例要求 **MindSpore >= 2.0.0** 版本以调用如下接口: *mindspore.jit, mindspore.jit_class, mindspore.data_sink*。具体请查看[MindSpore安装](https://www.mindspore.cn/install)。\n",
    "\n",
    "此外，你需要安装 **MindFlow >=0.1.0** 版本。如果当前环境还没有安装，请按照下列方式选择后端和版本进行安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindflow_version = \"0.1.0\"  # update if needed\n",
    "# GPU Comment out the following code if you are using NPU.\n",
    "!pip uninstall -y mindflow-gpu\n",
    "!pip install mindflow-gpu==$mindflow_version\n",
    "\n",
    "# NPU Uncomment if needed.\n",
    "# !pip uninstall -y mindflow-ascend\n",
    "# !pip install mindflow-ascend==$mindflow_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题描述\n",
    "\n",
    "本案例演示如何利用PINNs在不同几何体下求解一维、二维和三维泊松方程。\n",
    "\n",
    "一维泊松方程定义为\n",
    "\n",
    "$$\n",
    "\\Delta u = -\\sin(4\\pi x),\n",
    "$$\n",
    "\n",
    "二维泊松方程定义为\n",
    "\n",
    "$$\n",
    "\\Delta u = -\\sin(4\\pi x)\\sin(4\\pi y),\n",
    "$$\n",
    "\n",
    "而三维方程定义为\n",
    "\n",
    "$$\n",
    "\\Delta u = -\\sin(4\\pi x)\\sin(4\\pi y)\\sin(4\\pi z),\n",
    "$$\n",
    "\n",
    "很容易验证，以下函数分别满足二维和三维泊松方程\n",
    "\n",
    "$$\n",
    "u = \\frac{1}{16\\pi^2} \\sin(4\\pi x)\\\\\n",
    "u = \\frac{1}{32\\pi^2} \\sin(4\\pi x)\\sin(4\\pi y), \\\\\n",
    "u = \\frac{1}{48\\pi^2} \\sin(4\\pi x)\\sin(4\\pi y)\\sin(4\\pi z).\n",
    "$$\n",
    "\n",
    "如果在几何体边界按以上函数取狄利克雷边界条件，那么这些函数就是我们想要得到的解。因而，我们可以利用以上函数来验证结果。\n",
    "对于一维问题，本案例使用一维数轴区间作为求解域，对于二维问题，本例演示在矩形，圆形，三角形，L形和五边形区域求解方程，而对于三维问题，我们将在四面体，圆柱和圆锥区域内求解方程。\n",
    "\n",
    "## 技术路径\n",
    "\n",
    "MindFlow求解该问题的具体流程如下：\n",
    "\n",
    "1. 创建训练数据集。\n",
    "2. 构建模型。\n",
    "3. 优化器。\n",
    "4. 约束。\n",
    "5. 模型训练。\n",
    "6. 模型评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from mindspore import context, nn, ops, jit\n",
    "from mindflow import load_yaml_config\n",
    "\n",
    "from src.model import create_model\n",
    "from src.lr_scheduler import OneCycleLR\n",
    "from src.dataset import create_dataset\n",
    "\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target=\"GPU\")\n",
    "use_ascend = context.get_context(attr_key=\"device_target\") == \"Ascend\"\n",
    "\n",
    "# Load config\n",
    "file_cfg = \"configs/poisson_cfg.yaml\"\n",
    "config = load_yaml_config(file_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据集\n",
    "\n",
    "本案例在求解域及边值条件进行随机采样，生成训练数据集与测试数据集。具体方法见``src/dataset.py``。设值``geom_name``来选择几何体，可选择rectangle, disk, triangle, pentagon, tetrahedon, cylinder和cone。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_name = \"interval\"\n",
    "ds_train, n_dim = create_dataset(geom_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "\n",
    "本案例采用带3个隐藏层的多层感知器，并带有以下特点:\n",
    "\n",
    "- 采用激活函数：$f(x) = x \\exp(-x^2/(2e)) $\n",
    "\n",
    "- 最后一层线性层使用weight normalization。\n",
    "\n",
    "- 所有权重都采用``mindspore``的``HeUniform``初始化。\n",
    "\n",
    "具体定义见``src/model.py``。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(**config['model'][f'{n_dim}d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 约束\n",
    "\n",
    "在利用``mindflow``求解PDE时，我们需要写一个``mindflow.PDEWithLloss``的子类来定义控制方程，边界条件和损失函数。在求解区域内和边界上均采用L2损失，并利用``mindflow``的``MTLWeightedLoss``多目标损失函数将两个损失结合起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the boundary condition.\"\"\"\n",
    "import logging\n",
    "import sympy\n",
    "def dirichlet(n_dim, in_vars, out_vars):\n",
    "    '''\n",
    "        1d dirichlet boundary: a * u(x) = b\n",
    "        2d dirichlet boundary: a * u(x, y) = b\n",
    "        3d dirichlet boundary: a * u(x,y,z) = b\n",
    "        note: a is a constant, the variables x, y, and z\n",
    "              are in the boundary region, and b can be a function or a constant.\n",
    "    '''\n",
    "    bc_term = 1\n",
    "    for var in in_vars:\n",
    "        bc_term *= sympy.sin(4*sympy.pi*var)\n",
    "    try:\n",
    "        bc_term *= 1/(16*n_dim*sympy.pi*sympy.pi)\n",
    "    except ZeroDivisionError:\n",
    "        logging.error(\"Error: The divisor cannot be zero!\")\n",
    "\n",
    "    bc_eq = out_vars[0] - bc_term  # u(x) - bc_term\n",
    "    equations = {\"bc\": bc_eq}\n",
    "    return equations\n",
    "\n",
    "\n",
    "def robin(n_dim, in_vars, out_vars):\n",
    "    '''\n",
    "        1d robin boundary: a * u(x) + b * u'(x) = c\n",
    "        2d robin boundary: a * u(x, y) + b * u'(x, y) = c\n",
    "        3d robin boundary: a * u(x,y,z) + b * u'(x,y,z) = c\n",
    "        note: a, b is a constant, the variables x, y, and z\n",
    "              are in the boundary region,\n",
    "              u' is the number of external wizards of the function,\n",
    "              and c can be a function or a constant.\n",
    "    '''\n",
    "    bc_term = 1\n",
    "    u_x = 0\n",
    "    bc_term_u = 0\n",
    "    for var in in_vars:\n",
    "        bc_term_ux = 1\n",
    "        u_x += sympy.diff(out_vars[0], var)  # Derivation\n",
    "        bc_term *= sympy.sin(4*sympy.pi*var)\n",
    "        for i in in_vars:\n",
    "            if i != var:  # Partial conduction\n",
    "                bc_term_ux *= sympy.sin(4*sympy.pi*i)\n",
    "            elif i == var:\n",
    "                bc_term_ux *= sympy.cos(4*sympy.pi*i)\n",
    "        bc_term_u += bc_term_ux\n",
    "    try:\n",
    "        bc_term *= 1/(16*n_dim*sympy.pi*sympy.pi)  # function\n",
    "        bc_term_u *= 1/(4*n_dim*sympy.pi)\n",
    "    except ZeroDivisionError:\n",
    "        logging.error(\"Error: The divisor cannot be zero!\")\n",
    "\n",
    "    # u(x) + u'(x) - bc_term - bc_term_u\n",
    "    bc_eq = out_vars[0] + u_x - bc_term - bc_term_u\n",
    "    equations = {\"bc\": bc_eq}\n",
    "    return equations\n",
    "\n",
    "\n",
    "def periodic(n_dim, in_vars, out_vars):\n",
    "    '''\n",
    "        Periodic boundary conditions are a special case of Robin boundary conditions.\n",
    "        1d periodic boundary: a * u(x) + b * u'(x) = a * u(x+T) + b * u'(x+T) = c\n",
    "        2d periodic boundary: a * u(x,y) + b * u'(x,y) = a * u(x+T1,y+T2) + b * u'(x+T1,y+T2) = c\n",
    "        3d periodic boundary: a * u(x,y,z) + b * u'(x,y,z) = a * u(x+T1,y+T2,z+T3) + b * u'(x+T1,y+T2,z+T3) = c\n",
    "        note: a, b is a constant, the variables x, y, and z\n",
    "              are in the boundary region,\n",
    "              T1, T2, T3 corresponds to the period size of each variable in the defined interval,\n",
    "              u' is the number of external wizards of the function,\n",
    "              and c can be a function or a constant.\n",
    "    '''\n",
    "    _ = out_vars\n",
    "    bc_term = 1\n",
    "    for _ in in_vars:\n",
    "        bc_term *= 2\n",
    "    try:\n",
    "        bc_term *= 1/(16*n_dim*sympy.pi*sympy.pi)\n",
    "    except ZeroDivisionError:\n",
    "        logging.error(\"Error: The divisor cannot be zero!\")\n",
    "    bc_eq = bc_term  # bc_term\n",
    "    equations = {\"bc\": bc_eq}\n",
    "    return equations\n",
    "\n",
    "\n",
    "bc_type = {\n",
    "    \"dirichlet\": dirichlet,\n",
    "    \"robin\": robin,\n",
    "    \"periodic\": periodic,\n",
    "}\n",
    "\n",
    "\n",
    "def get_bc(bc):\n",
    "    '''return boundary condition'''\n",
    "    try:\n",
    "        boundary = bc_type[bc]\n",
    "    except KeyError:\n",
    "        logging.error(\"Wrong boundary name.\")\n",
    "    return boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisson: sin(4*pi*x) + Derivative(u(x), (x, 2))\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "bc: u(x) - sin(4*pi*x)/(16*pi**2)\n",
      "    Item numbers of current derivative formula nodes: 2\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "from mindspore import numpy as ms_np\n",
    "from mindflow import PDEWithLoss, MTLWeightedLoss, sympy_to_mindspore\n",
    "\n",
    "\n",
    "def pde(out_vars, in_vars):\n",
    "    poisson = 0\n",
    "    src_term = 1\n",
    "    sym_u = out_vars[0]\n",
    "    for var in in_vars:\n",
    "        poisson += sympy.diff(sym_u, (var, 2))\n",
    "        src_term *= sympy.sin(4 * sympy.pi * var)\n",
    "    poisson += src_term\n",
    "    equations = {\"poisson\": poisson}\n",
    "    return equations\n",
    "\n",
    "class Poisson(PDEWithLoss):\n",
    "    \"\"\"Define the loss of the Poisson equation.\"\"\"\n",
    "\n",
    "    def __init__(self, model, n_dim, pde, bc=None):\n",
    "        if n_dim == 1:\n",
    "            var_str = \"x,\"\n",
    "        elif n_dim == 2:\n",
    "            var_str = \"x y\"\n",
    "        elif n_dim == 3:\n",
    "            var_str = \"x y z\"\n",
    "        else:\n",
    "            raise ValueError(\"`n_dim` can only be 2 or 3.\")\n",
    "        self.in_vars = sympy.symbols(var_str)\n",
    "        self.out_vars = (sympy.Function(\"u\")(*self.in_vars),)\n",
    "        self.bc = bc\n",
    "        self.pde_fun = pde\n",
    "        super(Poisson, self).__init__(model, self.in_vars, self.out_vars)\n",
    "        if self.bc:  # boundary\n",
    "            self.bc_nodes = sympy_to_mindspore(\n",
    "                self.bc(n_dim, self.in_vars, self.out_vars), self.in_vars, self.out_vars)\n",
    "        self.loss_fn = MTLWeightedLoss(num_losses=2)\n",
    "\n",
    "    def pde(self):\n",
    "        \"\"\"Define the gonvering equation.\"\"\"\n",
    "        return self.pde_fun(self.out_vars, self.in_vars)  # equations\n",
    "\n",
    "    def get_loss(self, pde_data, bc_data):\n",
    "        \"\"\"Define the loss function.\"\"\"\n",
    "        res_pde = self.parse_node(self.pde_nodes, inputs=pde_data)\n",
    "        res_bc = self.parse_node(self.bc_nodes, inputs=bc_data)\n",
    "        loss_pde = ms_np.mean(ms_np.square(res_pde[0]))\n",
    "        loss_bc = ms_np.mean(ms_np.square(res_bc[0]))\n",
    "        return self.loss_fn((loss_pde, loss_bc))\n",
    "\n",
    "\n",
    "# Create the problem\n",
    "problem = Poisson(model, n_dim, pde, get_bc(config['data']['BC']['BC_type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "\n",
    "本案例采用Adam优化器，并配合[Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)提出的动态学习率进行训练。动态学习率定义参见``src/lr_scheduler.py``。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "params = model.trainable_params() + problem.loss_fn.trainable_params()\n",
    "steps_per_epoch = config['data']['domain']['size']//config[\"data\"][\"train\"][\"batch_size\"]\n",
    "learning_rate = OneCycleLR(total_steps=steps_per_epoch*n_epochs, **config['optimizer'])\n",
    "optimizer = nn.Adam(params, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "使用MindSpore>= 2.0.0的版本，可以使用函数式编程范式训练神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
    "\n",
    "def train():\n",
    "\n",
    "    if use_ascend:\n",
    "        from mindspore.amp import DynamicLossScaler, auto_mixed_precision, all_finite\n",
    "        loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "        auto_mixed_precision(model, 'O1')\n",
    "    else:\n",
    "        loss_scaler = None\n",
    "\n",
    "    # the loss function receives 2 data sources: pde and bc\n",
    "    def forward_fn(pde_data, bc_data):\n",
    "        loss = problem.get_loss(pde_data, bc_data)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.scale(loss)\n",
    "\n",
    "        return loss\n",
    "    # Create\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, opt.parameters, has_aux=False)\n",
    "\n",
    "    @jit\n",
    "    def train_step(pde_data, bc_data):\n",
    "        loss, grads = grad_fn(pde_data, bc_data)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.unscale(loss)\n",
    "            is_finite = all_finite(grads)\n",
    "            if is_finite:\n",
    "                grads = loss_scaler.unscale(grads)\n",
    "                loss = ops.depend(loss, opt(grads))\n",
    "            loss_scaler.adjust(is_finite)\n",
    "        else:\n",
    "            loss = ops.depend(loss, opt(grads))\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(model, dataset, i_epoch):\n",
    "        n_step = dataset.get_dataset_size()\n",
    "        model.set_train()\n",
    "        for i_step, (pde_data, bc_data) in enumerate(dataset):\n",
    "            local_time_beg = time.time()\n",
    "            loss = train_step(pde_data, bc_data)\n",
    "\n",
    "            if i_step%50 == 0 or i_step + 1 == n_step:\n",
    "                print(\"\\repoch: {}, loss: {:>f}, time elapsed: {:.1f}ms [{}/{}]\".format(\n",
    "                    i_epoch, float(loss), (time.time() - local_time_beg)*1000, i_step + 1, n_step))\n",
    "\n",
    "    for i_epoch in range(n_epochs):\n",
    "        train_epoch(model, ds_train, i_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.603493, time elapsed: 8164.0ms [1/200]\n",
      "epoch: 0, loss: 1.591017, time elapsed: 20.0ms [51/200]\n",
      "epoch: 0, loss: 1.563642, time elapsed: 19.7ms [101/200]\n",
      "epoch: 0, loss: 1.494738, time elapsed: 20.1ms [151/200]\n",
      "epoch: 0, loss: 1.394417, time elapsed: 19.3ms [200/200]\n",
      "epoch: 1, loss: 1.394938, time elapsed: 31.2ms [1/200]\n",
      "epoch: 1, loss: 1.376357, time elapsed: 18.9ms [51/200]\n",
      "epoch: 1, loss: 1.366150, time elapsed: 19.5ms [101/200]\n",
      "epoch: 1, loss: 1.356578, time elapsed: 20.0ms [151/200]\n",
      "epoch: 1, loss: 1.346048, time elapsed: 19.5ms [200/200]\n",
      "epoch: 2, loss: 1.345783, time elapsed: 31.6ms [1/200]\n",
      "epoch: 2, loss: 1.333922, time elapsed: 22.0ms [51/200]\n",
      "epoch: 2, loss: 1.320294, time elapsed: 18.8ms [101/200]\n",
      "epoch: 2, loss: 1.304355, time elapsed: 18.5ms [151/200]\n",
      "epoch: 2, loss: 1.286624, time elapsed: 20.3ms [200/200]\n",
      "epoch: 3, loss: 1.286230, time elapsed: 31.6ms [1/200]\n",
      "epoch: 3, loss: 1.265824, time elapsed: 20.0ms [51/200]\n",
      "epoch: 3, loss: 1.243438, time elapsed: 23.4ms [101/200]\n",
      "epoch: 3, loss: 1.218914, time elapsed: 19.7ms [151/200]\n",
      "epoch: 3, loss: 1.190454, time elapsed: 18.7ms [200/200]\n",
      "epoch: 4, loss: 1.189891, time elapsed: 31.8ms [1/200]\n",
      "epoch: 4, loss: 1.159772, time elapsed: 20.1ms [51/200]\n",
      "epoch: 4, loss: 1.128042, time elapsed: 19.6ms [101/200]\n",
      "epoch: 4, loss: 1.092744, time elapsed: 20.4ms [151/200]\n",
      "epoch: 4, loss: 1.058160, time elapsed: 18.9ms [200/200]\n",
      "epoch: 5, loss: 1.057422, time elapsed: 32.6ms [1/200]\n",
      "epoch: 5, loss: 1.017958, time elapsed: 18.5ms [51/200]\n",
      "epoch: 5, loss: 0.977773, time elapsed: 18.5ms [101/200]\n",
      "epoch: 5, loss: 0.936236, time elapsed: 18.1ms [151/200]\n",
      "epoch: 5, loss: 0.894821, time elapsed: 19.3ms [200/200]\n",
      "epoch: 6, loss: 0.894069, time elapsed: 32.1ms [1/200]\n",
      "epoch: 6, loss: 0.850565, time elapsed: 20.5ms [51/200]\n",
      "epoch: 6, loss: 0.807072, time elapsed: 20.8ms [101/200]\n",
      "epoch: 6, loss: 0.767167, time elapsed: 21.1ms [151/200]\n",
      "epoch: 6, loss: 0.720829, time elapsed: 20.1ms [200/200]\n",
      "epoch: 7, loss: 0.719956, time elapsed: 29.0ms [1/200]\n",
      "epoch: 7, loss: 0.677390, time elapsed: 20.0ms [51/200]\n",
      "epoch: 7, loss: 0.635279, time elapsed: 19.5ms [101/200]\n",
      "epoch: 7, loss: 0.594812, time elapsed: 19.4ms [151/200]\n",
      "epoch: 7, loss: 0.557111, time elapsed: 21.1ms [200/200]\n",
      "epoch: 8, loss: 0.556792, time elapsed: 32.2ms [1/200]\n",
      "epoch: 8, loss: 0.519962, time elapsed: 21.6ms [51/200]\n",
      "epoch: 8, loss: 0.486863, time elapsed: 19.8ms [101/200]\n",
      "epoch: 8, loss: 0.445668, time elapsed: 19.4ms [151/200]\n",
      "epoch: 8, loss: 0.414921, time elapsed: 20.0ms [200/200]\n",
      "epoch: 9, loss: 0.412774, time elapsed: 28.1ms [1/200]\n",
      "epoch: 9, loss: 0.380390, time elapsed: 19.2ms [51/200]\n",
      "epoch: 9, loss: 0.351696, time elapsed: 19.3ms [101/200]\n",
      "epoch: 9, loss: 0.322070, time elapsed: 18.4ms [151/200]\n",
      "epoch: 9, loss: 0.300759, time elapsed: 18.9ms [200/200]\n",
      "epoch: 10, loss: 0.310316, time elapsed: 31.0ms [1/200]\n",
      "epoch: 10, loss: 0.270345, time elapsed: 19.8ms [51/200]\n",
      "epoch: 10, loss: 0.244668, time elapsed: 18.9ms [101/200]\n",
      "epoch: 10, loss: 0.229011, time elapsed: 20.0ms [151/200]\n",
      "epoch: 10, loss: 0.202228, time elapsed: 20.2ms [200/200]\n",
      "epoch: 11, loss: 0.202390, time elapsed: 31.9ms [1/200]\n",
      "epoch: 11, loss: 0.182357, time elapsed: 19.6ms [51/200]\n",
      "epoch: 11, loss: 0.163855, time elapsed: 19.2ms [101/200]\n",
      "epoch: 11, loss: 0.172931, time elapsed: 19.5ms [151/200]\n",
      "epoch: 11, loss: 0.136611, time elapsed: 20.2ms [200/200]\n",
      "epoch: 12, loss: 0.135981, time elapsed: 32.7ms [1/200]\n",
      "epoch: 12, loss: 0.121192, time elapsed: 20.6ms [51/200]\n",
      "epoch: 12, loss: 0.108336, time elapsed: 19.9ms [101/200]\n",
      "epoch: 12, loss: 0.098026, time elapsed: 19.4ms [151/200]\n",
      "epoch: 12, loss: 0.087233, time elapsed: 19.7ms [200/200]\n",
      "epoch: 13, loss: 0.088027, time elapsed: 33.1ms [1/200]\n",
      "epoch: 13, loss: 0.078984, time elapsed: 20.0ms [51/200]\n",
      "epoch: 13, loss: 0.070841, time elapsed: 19.9ms [101/200]\n",
      "epoch: 13, loss: 0.065361, time elapsed: 19.4ms [151/200]\n",
      "epoch: 13, loss: 0.057398, time elapsed: 19.6ms [200/200]\n",
      "epoch: 14, loss: 0.058120, time elapsed: 33.0ms [1/200]\n",
      "epoch: 14, loss: 0.050670, time elapsed: 19.8ms [51/200]\n",
      "epoch: 14, loss: 0.043718, time elapsed: 20.1ms [101/200]\n",
      "epoch: 14, loss: 0.084862, time elapsed: 19.8ms [151/200]\n",
      "epoch: 14, loss: 0.037137, time elapsed: 19.6ms [200/200]\n",
      "epoch: 15, loss: 0.036636, time elapsed: 31.3ms [1/200]\n",
      "epoch: 15, loss: 0.042972, time elapsed: 19.9ms [51/200]\n",
      "epoch: 15, loss: 0.028162, time elapsed: 19.1ms [101/200]\n",
      "epoch: 15, loss: 0.057593, time elapsed: 19.7ms [151/200]\n",
      "epoch: 15, loss: 0.085612, time elapsed: 20.4ms [200/200]\n",
      "epoch: 16, loss: 0.138054, time elapsed: 33.1ms [1/200]\n",
      "epoch: 16, loss: 0.025892, time elapsed: 19.5ms [51/200]\n",
      "epoch: 16, loss: 0.021835, time elapsed: 20.5ms [101/200]\n",
      "epoch: 16, loss: 0.018618, time elapsed: 20.3ms [151/200]\n",
      "epoch: 16, loss: 0.048929, time elapsed: 19.2ms [200/200]\n",
      "epoch: 17, loss: 0.066798, time elapsed: 31.2ms [1/200]\n",
      "epoch: 17, loss: 0.017307, time elapsed: 20.4ms [51/200]\n",
      "epoch: 17, loss: 0.021145, time elapsed: 20.0ms [101/200]\n",
      "epoch: 17, loss: 0.015861, time elapsed: 20.8ms [151/200]\n",
      "epoch: 17, loss: 0.014903, time elapsed: 20.6ms [200/200]\n",
      "epoch: 18, loss: 0.014243, time elapsed: 34.6ms [1/200]\n",
      "epoch: 18, loss: 0.148888, time elapsed: 19.7ms [51/200]\n",
      "epoch: 18, loss: 0.015479, time elapsed: 20.8ms [101/200]\n",
      "epoch: 18, loss: 0.013033, time elapsed: 20.2ms [151/200]\n",
      "epoch: 18, loss: 0.011204, time elapsed: 19.1ms [200/200]\n",
      "epoch: 19, loss: 0.011169, time elapsed: 30.4ms [1/200]\n",
      "epoch: 19, loss: 0.016885, time elapsed: 20.2ms [51/200]\n",
      "epoch: 19, loss: 0.010477, time elapsed: 21.2ms [101/200]\n",
      "epoch: 19, loss: 0.051405, time elapsed: 20.2ms [151/200]\n",
      "epoch: 19, loss: 0.012071, time elapsed: 19.6ms [200/200]\n",
      "epoch: 20, loss: 0.011884, time elapsed: 31.3ms [1/200]\n",
      "epoch: 20, loss: 0.010274, time elapsed: 19.6ms [51/200]\n",
      "epoch: 20, loss: 0.085227, time elapsed: 19.7ms [101/200]\n",
      "epoch: 20, loss: 0.010673, time elapsed: 19.9ms [151/200]\n",
      "epoch: 20, loss: 0.009132, time elapsed: 19.9ms [200/200]\n",
      "epoch: 21, loss: 0.009105, time elapsed: 29.2ms [1/200]\n",
      "epoch: 21, loss: 0.008035, time elapsed: 19.9ms [51/200]\n",
      "epoch: 21, loss: 0.012967, time elapsed: 20.1ms [101/200]\n",
      "epoch: 21, loss: 0.009762, time elapsed: 21.3ms [151/200]\n",
      "epoch: 21, loss: 0.008594, time elapsed: 20.7ms [200/200]\n",
      "epoch: 22, loss: 0.008571, time elapsed: 33.3ms [1/200]\n",
      "epoch: 22, loss: 0.007525, time elapsed: 20.2ms [51/200]\n",
      "epoch: 22, loss: 0.020033, time elapsed: 19.8ms [101/200]\n",
      "epoch: 22, loss: 0.007799, time elapsed: 20.4ms [151/200]\n",
      "epoch: 22, loss: 0.006854, time elapsed: 19.4ms [200/200]\n",
      "epoch: 23, loss: 0.006838, time elapsed: 30.9ms [1/200]\n",
      "epoch: 23, loss: 0.012773, time elapsed: 19.2ms [51/200]\n",
      "epoch: 23, loss: 0.007932, time elapsed: 25.5ms [101/200]\n",
      "epoch: 23, loss: 0.006981, time elapsed: 20.5ms [151/200]\n",
      "epoch: 23, loss: 0.006191, time elapsed: 20.4ms [200/200]\n",
      "epoch: 24, loss: 0.006175, time elapsed: 34.7ms [1/200]\n",
      "epoch: 24, loss: 0.010222, time elapsed: 20.7ms [51/200]\n",
      "epoch: 24, loss: 0.007143, time elapsed: 18.2ms [101/200]\n",
      "epoch: 24, loss: 0.006309, time elapsed: 19.9ms [151/200]\n",
      "epoch: 24, loss: 0.019660, time elapsed: 18.5ms [200/200]\n",
      "epoch: 25, loss: 0.036125, time elapsed: 29.0ms [1/200]\n",
      "epoch: 25, loss: 0.007054, time elapsed: 18.6ms [51/200]\n",
      "epoch: 25, loss: 0.005961, time elapsed: 19.1ms [101/200]\n",
      "epoch: 25, loss: 0.005311, time elapsed: 19.3ms [151/200]\n",
      "epoch: 25, loss: 0.226412, time elapsed: 19.0ms [200/200]\n",
      "epoch: 26, loss: 0.234644, time elapsed: 30.8ms [1/200]\n",
      "epoch: 26, loss: 0.007830, time elapsed: 18.4ms [51/200]\n",
      "epoch: 26, loss: 0.006543, time elapsed: 18.8ms [101/200]\n",
      "epoch: 26, loss: 0.005914, time elapsed: 18.4ms [151/200]\n",
      "epoch: 26, loss: 0.005357, time elapsed: 18.4ms [200/200]\n",
      "epoch: 27, loss: 0.005346, time elapsed: 29.3ms [1/200]\n",
      "epoch: 27, loss: 0.036335, time elapsed: 18.6ms [51/200]\n",
      "epoch: 27, loss: 0.005787, time elapsed: 19.0ms [101/200]\n",
      "epoch: 27, loss: 0.005150, time elapsed: 19.3ms [151/200]\n",
      "epoch: 27, loss: 0.004676, time elapsed: 19.2ms [200/200]\n",
      "epoch: 28, loss: 0.004665, time elapsed: 31.8ms [1/200]\n",
      "epoch: 28, loss: 0.010830, time elapsed: 19.9ms [51/200]\n",
      "epoch: 28, loss: 0.005435, time elapsed: 20.1ms [101/200]\n",
      "epoch: 28, loss: 0.004836, time elapsed: 19.7ms [151/200]\n",
      "epoch: 28, loss: 0.004392, time elapsed: 19.6ms [200/200]\n",
      "epoch: 29, loss: 0.004383, time elapsed: 31.7ms [1/200]\n",
      "epoch: 29, loss: 0.034536, time elapsed: 19.6ms [51/200]\n",
      "epoch: 29, loss: 0.004564, time elapsed: 20.0ms [101/200]\n",
      "epoch: 29, loss: 0.004118, time elapsed: 21.1ms [151/200]\n",
      "epoch: 29, loss: 0.003755, time elapsed: 20.1ms [200/200]\n",
      "epoch: 30, loss: 0.003748, time elapsed: 33.4ms [1/200]\n",
      "epoch: 30, loss: 0.009549, time elapsed: 21.4ms [51/200]\n",
      "epoch: 30, loss: 0.004050, time elapsed: 19.3ms [101/200]\n",
      "epoch: 30, loss: 0.003682, time elapsed: 20.6ms [151/200]\n",
      "epoch: 30, loss: 0.147198, time elapsed: 19.6ms [200/200]\n",
      "epoch: 31, loss: 0.104858, time elapsed: 32.1ms [1/200]\n",
      "epoch: 31, loss: 0.004705, time elapsed: 19.8ms [51/200]\n",
      "epoch: 31, loss: 0.004090, time elapsed: 19.3ms [101/200]\n",
      "epoch: 31, loss: 0.003774, time elapsed: 19.7ms [151/200]\n",
      "epoch: 31, loss: 0.003490, time elapsed: 19.3ms [200/200]\n",
      "epoch: 32, loss: 0.003485, time elapsed: 29.9ms [1/200]\n",
      "epoch: 32, loss: 0.003219, time elapsed: 20.0ms [51/200]\n",
      "epoch: 32, loss: 0.004445, time elapsed: 20.8ms [101/200]\n",
      "epoch: 32, loss: 0.003231, time elapsed: 19.8ms [151/200]\n",
      "epoch: 32, loss: 0.002988, time elapsed: 19.6ms [200/200]\n",
      "epoch: 33, loss: 0.002982, time elapsed: 30.7ms [1/200]\n",
      "epoch: 33, loss: 0.002759, time elapsed: 20.1ms [51/200]\n",
      "epoch: 33, loss: 0.003906, time elapsed: 19.9ms [101/200]\n",
      "epoch: 33, loss: 0.003325, time elapsed: 19.3ms [151/200]\n",
      "epoch: 33, loss: 0.003068, time elapsed: 19.3ms [200/200]\n",
      "epoch: 34, loss: 0.003063, time elapsed: 32.8ms [1/200]\n",
      "epoch: 34, loss: 0.002852, time elapsed: 20.1ms [51/200]\n",
      "epoch: 34, loss: 0.002658, time elapsed: 20.1ms [101/200]\n",
      "epoch: 34, loss: 0.002489, time elapsed: 19.2ms [151/200]\n",
      "epoch: 34, loss: 0.003529, time elapsed: 19.5ms [200/200]\n",
      "epoch: 35, loss: 0.002867, time elapsed: 30.5ms [1/200]\n",
      "epoch: 35, loss: 0.002456, time elapsed: 19.2ms [51/200]\n",
      "epoch: 35, loss: 0.002273, time elapsed: 18.9ms [101/200]\n",
      "epoch: 35, loss: 0.003385, time elapsed: 19.9ms [151/200]\n",
      "epoch: 35, loss: 0.002626, time elapsed: 18.6ms [200/200]\n",
      "epoch: 36, loss: 0.002741, time elapsed: 31.6ms [1/200]\n",
      "epoch: 36, loss: 0.002381, time elapsed: 20.5ms [51/200]\n",
      "epoch: 36, loss: 0.002234, time elapsed: 24.8ms [101/200]\n",
      "epoch: 36, loss: 0.002098, time elapsed: 20.5ms [151/200]\n",
      "epoch: 36, loss: 0.001975, time elapsed: 20.5ms [200/200]\n",
      "epoch: 37, loss: 0.001973, time elapsed: 31.3ms [1/200]\n",
      "epoch: 37, loss: 0.001858, time elapsed: 19.9ms [51/200]\n",
      "epoch: 37, loss: 0.002012, time elapsed: 21.3ms [101/200]\n",
      "epoch: 37, loss: 0.001764, time elapsed: 21.0ms [151/200]\n",
      "epoch: 37, loss: 0.001694, time elapsed: 20.2ms [200/200]\n",
      "epoch: 38, loss: 0.001687, time elapsed: 31.3ms [1/200]\n",
      "epoch: 38, loss: 0.002022, time elapsed: 19.8ms [51/200]\n",
      "epoch: 38, loss: 0.001636, time elapsed: 20.7ms [101/200]\n",
      "epoch: 38, loss: 0.001549, time elapsed: 20.3ms [151/200]\n",
      "epoch: 38, loss: 0.001474, time elapsed: 20.5ms [200/200]\n",
      "epoch: 39, loss: 0.001474, time elapsed: 31.7ms [1/200]\n",
      "epoch: 39, loss: 0.002744, time elapsed: 20.0ms [51/200]\n",
      "epoch: 39, loss: 0.001422, time elapsed: 18.3ms [101/200]\n",
      "epoch: 39, loss: 0.001335, time elapsed: 19.3ms [151/200]\n",
      "epoch: 39, loss: 0.001283, time elapsed: 18.6ms [200/200]\n",
      "epoch: 40, loss: 0.001288, time elapsed: 31.3ms [1/200]\n",
      "epoch: 40, loss: 0.001277, time elapsed: 21.3ms [51/200]\n",
      "epoch: 40, loss: 0.001264, time elapsed: 19.7ms [101/200]\n",
      "epoch: 40, loss: 0.001166, time elapsed: 19.6ms [151/200]\n",
      "epoch: 40, loss: 0.001724, time elapsed: 19.1ms [200/200]\n",
      "epoch: 41, loss: 0.001966, time elapsed: 31.1ms [1/200]\n",
      "epoch: 41, loss: 0.001077, time elapsed: 19.6ms [51/200]\n",
      "epoch: 41, loss: 0.001031, time elapsed: 19.6ms [101/200]\n",
      "epoch: 41, loss: 0.001316, time elapsed: 19.8ms [151/200]\n",
      "epoch: 41, loss: 0.000971, time elapsed: 19.5ms [200/200]\n",
      "epoch: 42, loss: 0.000968, time elapsed: 30.0ms [1/200]\n",
      "epoch: 42, loss: 0.000926, time elapsed: 19.5ms [51/200]\n",
      "epoch: 42, loss: 0.000893, time elapsed: 20.0ms [101/200]\n",
      "epoch: 42, loss: 0.000866, time elapsed: 19.1ms [151/200]\n",
      "epoch: 42, loss: 0.000839, time elapsed: 20.2ms [200/200]\n",
      "epoch: 43, loss: 0.000840, time elapsed: 31.9ms [1/200]\n",
      "epoch: 43, loss: 0.000810, time elapsed: 20.6ms [51/200]\n",
      "epoch: 43, loss: 0.000788, time elapsed: 19.9ms [101/200]\n",
      "epoch: 43, loss: 0.000766, time elapsed: 20.1ms [151/200]\n",
      "epoch: 43, loss: 0.000747, time elapsed: 19.9ms [200/200]\n",
      "epoch: 44, loss: 0.000746, time elapsed: 33.0ms [1/200]\n",
      "epoch: 44, loss: 0.000729, time elapsed: 21.1ms [51/200]\n",
      "epoch: 44, loss: 0.000713, time elapsed: 19.5ms [101/200]\n",
      "epoch: 44, loss: 0.000696, time elapsed: 18.7ms [151/200]\n",
      "epoch: 44, loss: 0.000682, time elapsed: 20.2ms [200/200]\n",
      "epoch: 45, loss: 0.000682, time elapsed: 31.6ms [1/200]\n",
      "epoch: 45, loss: 0.000670, time elapsed: 19.3ms [51/200]\n",
      "epoch: 45, loss: 0.000658, time elapsed: 20.4ms [101/200]\n",
      "epoch: 45, loss: 0.000648, time elapsed: 19.2ms [151/200]\n",
      "epoch: 45, loss: 0.000639, time elapsed: 19.1ms [200/200]\n",
      "epoch: 46, loss: 0.000638, time elapsed: 31.3ms [1/200]\n",
      "epoch: 46, loss: 0.000630, time elapsed: 18.8ms [51/200]\n",
      "epoch: 46, loss: 0.000623, time elapsed: 19.8ms [101/200]\n",
      "epoch: 46, loss: 0.000616, time elapsed: 19.1ms [151/200]\n",
      "epoch: 46, loss: 0.000611, time elapsed: 19.2ms [200/200]\n",
      "epoch: 47, loss: 0.000612, time elapsed: 28.8ms [1/200]\n",
      "epoch: 47, loss: 0.000606, time elapsed: 19.8ms [51/200]\n",
      "epoch: 47, loss: 0.000601, time elapsed: 20.7ms [101/200]\n",
      "epoch: 47, loss: 0.000598, time elapsed: 20.0ms [151/200]\n",
      "epoch: 47, loss: 0.000595, time elapsed: 20.6ms [200/200]\n",
      "epoch: 48, loss: 0.000596, time elapsed: 30.9ms [1/200]\n",
      "epoch: 48, loss: 0.000594, time elapsed: 19.4ms [51/200]\n",
      "epoch: 48, loss: 0.000592, time elapsed: 20.4ms [101/200]\n",
      "epoch: 48, loss: 0.000590, time elapsed: 20.7ms [151/200]\n",
      "epoch: 48, loss: 0.000590, time elapsed: 20.0ms [200/200]\n",
      "epoch: 49, loss: 0.000590, time elapsed: 31.3ms [1/200]\n",
      "epoch: 49, loss: 0.000589, time elapsed: 19.4ms [51/200]\n",
      "epoch: 49, loss: 0.000588, time elapsed: 19.7ms [101/200]\n",
      "epoch: 49, loss: 0.000588, time elapsed: 19.7ms [151/200]\n",
      "epoch: 49, loss: 0.000588, time elapsed: 19.1ms [200/200]\n",
      "End-to-End total time: 214.40974640846252 s\n"
     ]
    }
   ],
   "source": [
    "time_beg = time.time()\n",
    "train()\n",
    "print(\"End-to-End total time: {} s\".format(time.time() - time_beg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "可通过以下函数来计算模型的L2相对误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 error (domain): 0.0000\n",
      "Relative L2 error (bc): 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.utils import calculate_l2_error\n",
    "\n",
    "n_samps = 5000  # Number of test samples\n",
    "ds_test, _ = create_dataset(geom_name, config, n_samps)\n",
    "calculate_l2_error(model, ds_test, n_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果\n",
    "\n",
    "在2维矩形区域中，模型训练结果如下：\n",
    "<p align = \"center\">\n",
    "<img src=\"./images/dirichlet-rectangle.png\" width=\"250\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
