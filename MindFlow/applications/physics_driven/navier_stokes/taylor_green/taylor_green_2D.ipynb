{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Taylor Green Vortex\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "This notebook requires **MindSpore version >= 2.0.0** to support new APIs including: *mindspore.jit, mindspore.jit_class, mindspore.data_sink*. Please check [MindSpore Installation](https://www.mindspore.cn/install/en) for details.\n",
    "\n",
    "In addition, **MindFlow version >=0.1.0** is also required. If it has not been installed in your environment, please select the right version and hardware, then install it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindflow_version = \"0.1.0\"  # update if needed\n",
    "# GPU Comment out the following code if you are using NPU.\n",
    "!pip uninstall -y mindflow-gpu\n",
    "!pip install mindflow-gpu==$mindflow_version\n",
    "\n",
    "# NPU Uncomment if needed.\n",
    "# !pip uninstall -y mindflow-ascend\n",
    "# !pip install mindflow-ascend==$mindflow_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In fluid dynamics, the Taylor–Green vortex is an unsteady flow of a decaying vortex, which has an exact closed form solution of the incompressible Navier–Stokes equations in Cartesian coordinates. It is named after the British physicist and mathematician Geoffrey Ingram Taylor and his collaborator A. E. Green.\n",
    "\n",
    "Physics-informed Neural Networks (PINNs) provides a new method for quickly solving complex fluid problems by using loss functions that approximate governing equations coupled with simple network configurations. In this case, the data-driven characteristic of neural network is used along with `PINNs` to solve the 2D taylor green vortex problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "The Navier-Stokes equation, referred to as `N-S` equation, is a classical partial differential equation in the field of fluid mechanics. In the case of viscous incompressibility, the dimensionless `N-S` equation has the following form:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u} {\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} = - \\frac{\\partial p}{\\partial x} + \\frac{1} {Re} (\\frac{\\partial^2u}{\\partial x^2} + \\frac{\\partial^2u}{\\partial y^2})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial v} {\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} = - \\frac{\\partial p}{\\partial y} + \\frac{1} {Re} (\\frac{\\partial^2v}{\\partial x^2} + \\frac{\\partial^2v}{\\partial y^2})\n",
    "$$\n",
    "\n",
    "where `Re` stands for Reynolds number.\n",
    "\n",
    "In this case, the PINNs method is used to learn the mapping from the location and time to flow field quantities to solve the `N-S` equation.\n",
    "\n",
    "$$\n",
    "(x, y, t) \\mapsto (u, v, p)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology Path\n",
    "\n",
    "MindFlow solves the problem as follows:\n",
    "\n",
    "1. Training Dataset Construction.\n",
    "2. Model Construction.\n",
    "3. Multi-task Learning for Adaptive Losses\n",
    "4. Optimizer.\n",
    "5. NavierStokes2D.\n",
    "6. Model Training.\n",
    "7. Model Evaluation and Visualization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import sympy\n",
    "import mindspore\n",
    "from mindspore import context, nn, ops, jit, set_seed\n",
    "from mindspore import numpy as mnp\n",
    "\n",
    "from mindflow.cell import MultiScaleFCSequential\n",
    "from mindflow.utils import load_yaml_config\n",
    "from mindflow.pde import NavierStokes, sympy_to_mindspore\n",
    "\n",
    "from src import create_training_dataset, create_test_dataset, calculate_l2_error, NavierStokes2D\n",
    "\n",
    "set_seed(123456)\n",
    "np.random.seed(123456)\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\", device_id=0, save_graphs=False)\n",
    "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
    "\n",
    "config = load_yaml_config('configs/taylor_green_2D.yaml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction\n",
    "\n",
    "Training dataset is imported through function _create_train_dataset_, contains domain points, initial condition points and boundary condition point. All datasets are sampled by APIs from mindflow.geometry.\n",
    "\n",
    "Test dataset is imported through function _create_test_dataset_. In this case, the exact solution used to construct test dataset is given by **J Kim, P Moin,Application of a fractional-step method to incompressible Navier-Stokes equations,Journal of Computational Physics,Volume 59, Issue 2,1985**.\n",
    "\n",
    "$$\n",
    "u(x,y,t) = -cos(x)sin(y)e^{-2t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v(x,y,t) = sin(x)cos(y)e^{-2t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x,y,t) = -0.25(cos(2x)+cos(2y))e^{-4t}\n",
    "$$\n",
    "\n",
    "The computation is carried out in the domain of $ 0 \\leq  x,y \\leq 2\\pi$,\n",
    "and $ 0 \\leq t \\leq 2$. The Reynolds number Re is equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training dataset\n",
    "taylor_dataset = create_training_dataset(config)\n",
    "train_dataset = taylor_dataset.create_dataset(batch_size=config[\"data\"][\"train\"][\"batch_size\"],\n",
    "                                              shuffle=True,\n",
    "                                              prebatched_data=True,\n",
    "                                              drop_remainder=True)\n",
    "\n",
    "# create test dataset\n",
    "inputs, label = create_test_dataset(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "\n",
    "This example uses a simple fully-connected network with a depth of 6 layers and the activation function is the `tanh` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_min = np.array(config[\"geometry\"][\"coord_min\"] + [config[\"geometry\"][\"time_min\"]]).astype(np.float32)\n",
    "coord_max = np.array(config[\"geometry\"][\"coord_max\"] + [config[\"geometry\"][\"time_max\"]]).astype(np.float32)\n",
    "input_center = list(0.5 * (coord_max + coord_min))\n",
    "input_scale = list(2.0 / (coord_max - coord_min))\n",
    "\n",
    "model = MultiScaleFCSequential(in_channels=config[\"model\"][\"in_channels\"],\n",
    "                               out_channels=config[\"model\"][\"out_channels\"],\n",
    "                               layers=config[\"model\"][\"layers\"],\n",
    "                               neurons=config[\"model\"][\"neurons\"],\n",
    "                               residual=config[\"model\"][\"residual\"],\n",
    "                               act=config[\"model\"][\"activation\"],\n",
    "                               num_scales=1,\n",
    "                               input_scale=input_scale,\n",
    "                               input_center=input_center)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.trainable_params()\n",
    "optimizer = nn.Adam(params, learning_rate=config[\"optimizer\"][\"learning_rate\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NavierStokes2D\n",
    "\n",
    "The following `NavierStokes2D` defines the navier_stokes' problem. Specifically, it includes 3 parts: governing equation, initial condition and boundary conditions. Initial condition and boundary conditions are constructed by the exact solution mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavierStokes2D(NavierStokes):\n",
    "    def __init__(self, model, re=100, loss_fn=nn.MSELoss()):\n",
    "        super(NavierStokes2D, self).__init__(model, re=re, loss_fn=loss_fn)\n",
    "        self.ic_nodes = sympy_to_mindspore(self.ic(), self.in_vars, self.out_vars)\n",
    "        self.bc_nodes = sympy_to_mindspore(self.bc(), self.in_vars, self.out_vars)\n",
    "\n",
    "    def ic(self):\n",
    "        \"\"\"\n",
    "        Define initial condition equations based on sympy, abstract method.\n",
    "        \"\"\"\n",
    "        ic_u = self.u + sympy.cos(self.x) * sympy.sin(self.y)\n",
    "        ic_v = self.v - sympy.sin(self.x) * sympy.cos(self.y)\n",
    "        ic_p = self.p + 0.25 * (sympy.cos(2*self.x) + sympy.cos(2*self.y))\n",
    "        equations = {\"ic_u\": ic_u, \"ic_v\": ic_v, \"ic_p\": ic_p}\n",
    "        return equations\n",
    "\n",
    "    def bc(self):\n",
    "        \"\"\"\n",
    "        Define boundary condition equations based on sympy, abstract method.\n",
    "        \"\"\"\n",
    "        bc_u = self.u + sympy.cos(self.x) * sympy.sin(self.y) * sympy.exp(-2*self.t)\n",
    "        bc_v = self.v - sympy.sin(self.x) * sympy.cos(self.y) * sympy.exp(-2*self.t)\n",
    "        bc_p = self.p + 0.25 * (sympy.cos(2*self.x) + sympy.cos(2*self.y)) * sympy.exp(-4*self.t)\n",
    "        equations = {\"bc_u\": bc_u, \"bc_v\": bc_v, \"bc_p\": bc_p}\n",
    "        return equations\n",
    "\n",
    "    def get_loss(self, pde_data, ic_data, bc_data):\n",
    "        \"\"\"\n",
    "        Compute loss of 3 parts: governing equation, initial condition and boundary conditions.\n",
    "\n",
    "        Args:\n",
    "            pde_data (Tensor): the input data of governing equations.\n",
    "            ic_data (Tensor): the input data of initial condition.\n",
    "            bc_data (Tensor): the input data of boundary condition.\n",
    "        \"\"\"\n",
    "        pde_res = self.parse_node(self.pde_nodes, inputs=pde_data)\n",
    "        pde_residual = ops.Concat(1)(pde_res)\n",
    "        pde_loss = self.loss_fn(pde_residual, mnp.zeros_like(pde_residual))\n",
    "\n",
    "        ic_res = self.parse_node(self.ic_nodes, inputs=ic_data)\n",
    "        ic_residual = ops.Concat(1)(ic_res)\n",
    "        ic_loss = self.loss_fn(ic_residual, mnp.zeros_like(ic_residual))\n",
    "\n",
    "        bc_res = self.parse_node(self.bc_nodes, inputs=bc_data)\n",
    "        bc_residual = ops.Concat(1)(bc_res)\n",
    "        bc_loss = self.loss_fn(bc_residual, mnp.zeros_like(bc_residual))\n",
    "\n",
    "        return pde_loss + ic_loss + bc_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "With **MindSpore version >= 2.0.0**, we can use the functional programming for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    problem = NavierStokes2D(model, re=config[\"summary\"][\"Re\"])\n",
    "\n",
    "    if use_ascend:\n",
    "        from mindspore.amp import DynamicLossScaler, auto_mixed_precision, all_finite\n",
    "        loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "        auto_mixed_precision(model, 'O3')\n",
    "\n",
    "    def forward_fn(pde_data, ic_data, bc_data):\n",
    "        loss = problem.get_loss(pde_data, ic_data, bc_data)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.scale(loss)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "    @jit\n",
    "    def train_step(pde_data, ic_data, bc_data):\n",
    "        loss, grads = grad_fn(pde_data, ic_data, bc_data)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.unscale(loss)\n",
    "            is_finite = all_finite(grads)\n",
    "            if is_finite:\n",
    "                grads = loss_scaler.unscale(grads)\n",
    "                loss = ops.depend(loss, optimizer(grads))\n",
    "            loss_scaler.adjust(is_finite)\n",
    "        else:\n",
    "            loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "\n",
    "    epochs = config[\"data\"][\"train\"][\"epochs\"]\n",
    "    steps_per_epochs = train_dataset.get_dataset_size()\n",
    "    sink_process = mindspore.data_sink(train_step, train_dataset, sink_size=1)\n",
    "    for epoch in range(1, 1 + epochs):\n",
    "        # train\n",
    "        time_beg = time.time()\n",
    "        model.set_train(True)\n",
    "        for _ in range(steps_per_epochs):\n",
    "            step_train_loss = sink_process()\n",
    "        print(f\"epoch: {epoch} train loss: {step_train_loss} epoch time: {(time.time() - time_beg) * 1000 :.3f} ms\")\n",
    "        model.set_train(False)\n",
    "\n",
    "        if epoch % config[\"summary\"][\"eval_interval_epochs\"] == 0:\n",
    "            calculate_l2_error(model, inputs, label, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "momentum_x: u(x, y, t)*Derivative(u(x, y, t), x) + v(x, y, t)*Derivative(u(x, y, t), y) + Derivative(p(x, y, t), x) + Derivative(u(x, y, t), t) - 1.0*Derivative(u(x, y, t), (x, 2)) - 1.0*Derivative(u(x, y, t), (y, 2))\n",
      "    Item numbers of current derivative formula nodes: 6\n",
      "momentum_y: u(x, y, t)*Derivative(v(x, y, t), x) + v(x, y, t)*Derivative(v(x, y, t), y) + Derivative(p(x, y, t), y) + Derivative(v(x, y, t), t) - 1.0*Derivative(v(x, y, t), (x, 2)) - 1.0*Derivative(v(x, y, t), (y, 2))\n",
      "    Item numbers of current derivative formula nodes: 6\n",
      "continuty: Derivative(u(x, y, t), x) + Derivative(v(x, y, t), y)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "ic_u: u(x, y, t) + sin(y)*cos(x)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "ic_v: v(x, y, t) - sin(x)*cos(y)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "ic_p: p(x, y, t) + 0.25*cos(2*x) + 0.25*cos(2*y)\n",
      "    Item numbers of current derivative formula nodes: 3\n",
      "bc_u: u(x, y, t) + exp(-2*t)*sin(y)*cos(x)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "bc_v: v(x, y, t) - exp(-2*t)*sin(x)*cos(y)\n",
      "    Item numbers of current derivative formula nodes: 2\n",
      "bc_p: p(x, y, t) + 0.25*exp(-4*t)*cos(2*x) + 0.25*exp(-4*t)*cos(2*y)\n",
      "    Item numbers of current derivative formula nodes: 3\n",
      "epoch: 20 train loss: 0.11818831 epoch time: 9838.472 ms\n",
      "    predict total time: 342.714786529541 ms\n",
      "    l2_error, U:  0.7095809547153462 , V:  0.7081305150496081 , P:  1.004580707024092 , Total:  0.7376210740866216\n",
      "==================================================================================================\n",
      "epoch: 40 train loss: 0.025397364 epoch time: 9853.950 ms\n",
      "    predict total time: 67.26336479187012 ms\n",
      "    l2_error, U:  0.09177234501446464 , V:  0.14504987645942635 , P:  1.0217915750380309 , Total:  0.3150453016208772\n",
      "==================================================================================================\n",
      "epoch: 60 train loss: 0.0049396083 epoch time: 10158.307 ms\n",
      "    predict total time: 121.54984474182129 ms\n",
      "    l2_error, U:  0.08648064925211238 , V:  0.07875554509736878 , P:  0.711385847511365 , Total:  0.2187113170206073\n",
      "==================================================================================================\n",
      "epoch: 80 train loss: 0.0018874758 epoch time: 10349.795 ms\n",
      "    predict total time: 85.42561531066895 ms\n",
      "    l2_error, U:  0.08687053366212526 , V:  0.10624717784645109 , P:  0.3269822261697911 , Total:  0.1319986181134018\n",
      "==================================================================================================\n",
      "......\n",
      "epoch: 460 train loss: 0.00015093417 epoch time: 9928.474 ms\n",
      "    predict total time: 81.79974555969238 ms\n",
      "    l2_error, U:  0.033782269766829076 , V:  0.025816595720090357 , P:  0.08782072926563861 , Total:  0.03824859644715835\n",
      "==================================================================================================\n",
      "epoch: 480 train loss: 6.400551e-05 epoch time: 9956.549 ms\n",
      "    predict total time: 104.77519035339355 ms\n",
      "    l2_error, U:  0.02242134127961232 , V:  0.021098481157660533 , P:  0.06210985820202502 , Total:  0.027418651376509482\n",
      "==================================================================================================\n",
      "epoch: 500 train loss: 8.7400025e-05 epoch time: 10215.720 ms\n",
      "    predict total time: 77.20041275024414 ms\n",
      "    l2_error, U:  0.021138056243295636 , V:  0.013343674071961624 , P:  0.045241559122240635 , Total:  0.02132725837819097\n",
      "==================================================================================================\n",
      "End-to-End total time: 5011.718255519867 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train()\n",
    "print(\"End-to-End total time: {} s\".format(time.time() - start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import visual\n",
    "\n",
    "# visualization\n",
    "visual(model=model, epoch=config[\"data\"][\"train\"][\"epochs\"], input_data=inputs, label=label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![time-error](./images/TimeError_30000.png \"error after training\")\n",
    "\n",
    "As the speed tends to decrease exponentially, the error becomes larger with time, but the overall is within the 5% error range. Picture below shows u, v, p during the process.\n",
    "\n",
    "![mid_stage](./images/mid_stage.png \"mid stage status\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57ace93c29d9374277a79956c3f1b916d7d9a05468d906842f9921d0d494a29f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
