{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Multi-timestep Complicated Flow Field Prediction with Transonic Airfoil under Data Driven Mode(with Two Kinds of Backbones: FNO2D and UNET2D)\n",
    "\n",
    "[![DownloadNotebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_notebook_en.png)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.3/mindflow/en/data_driven/mindspore_2D_unsteady.ipynb)&emsp;[![DownloadCode](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_download_code_en.png)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.3/mindflow/en/data_driven/mindspore_2D_unsteady.py)&emsp;[![ViewSource](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png)](https://gitee.com/mindspore/docs/blob/master/docs/mindflow/docs/source_en/data_driven/2D_unsteady.ipynb)\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "This notebook requires **MindSpore version >= 2.0.0** to support new APIs including: *mindspore.jit, mindspore.jit_class, mindspore.data_sink*. Please check [MindSpore Installation](https://www.mindspore.cn/install/en) for details.\n",
    "\n",
    "In addition, **MindFlow version >=0.1.0** is also required. If it has not been installed in your environment, please select the right version and hardware, then install it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindflow_version = \"0.1.0\"  # update if needed\n",
    "# GPU Comment out the following code if you are using NPU.\n",
    "!pip uninstall -y mindflow-gpu\n",
    "!pip install mindflow-gpu==$mindflow_version\n",
    "\n",
    "# NPU Uncomment if needed.\n",
    "# !pip uninstall -y mindflow-ascend\n",
    "# !pip install mindflow-ascend==$mindflow_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "High precision unsteady flow simulation is among the key topics in computational fluid dynamics(CFD), with a wide range of application scenarios and broad market value. However, traditional methods encountered problems such as long time-consumption, poor precision and hard convergence. AI methods provides a new perspective to explore the evolution mechanism of flow fields.\n",
    "\n",
    "The current application provides an end-to-end solution for predicting unsteady and complex flow fields in a two-dimensional transonic airfoil scenario. Two network backbones, Fourier Neural Operator (FNO) and Unet, are constructed in order to stably predict the flow field for subsequent *m* time steps based on the input flow field of *k* time steps, while ensuring a certain level of accuracy.\n",
    "\n",
    "The Mach number of the incoming flow reached *Ma*=0.73 in the current application. Thus it can verify the effectiveness of deep learning methods in predicting unsteady flow fields under multiple physical parameter changes in complex flow structures such as shock waves.\n",
    "\n",
    "![img_1.png](images/img_1.png)\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "We aim to predict next *k* steps of two-dimensional compressible unsteady flow by learning from past *m* steps flow field in the current application:\n",
    "\n",
    "$$\n",
    "u_{[t_0\\sim t_{k-1}]} \\mapsto u_{[t_k\\sim t_{k+m}]}\n",
    "$$\n",
    "\n",
    "## Technology Path\n",
    "\n",
    "The solution steps to the problem is presented as follows:\n",
    "\n",
    "1. Training Dataset Construction.\n",
    "2. Model Construction.\n",
    "3. Optimizer and Loss Function.\n",
    "4. Model Training.\n",
    "\n",
    "## **Preparation**\n",
    "\n",
    "Before practice, ensure that MindSpore of suitable version has been correctly installed. If not, you can run the following command:\n",
    "\n",
    "* [MindSpore installation page](https://www.mindspore.cn/install) Install MindSpore.\n",
    "\n",
    "## 2D Airfoil Unsteady Flow Field Prediction Implementation\n",
    "\n",
    "The implementation of 2D airfoil unsteady flow field prediction consists of the following seven steps:\n",
    "\n",
    "1. Configure network and training parameters\n",
    "2. Datasets preparation\n",
    "3. Model building\n",
    "4. Loss function and optimizer\n",
    "5. Train function\n",
    "6. Model training\n",
    "7. Result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from mindspore import nn, Tensor, context, ops, jit, set_seed, data_sink, save_checkpoint\n",
    "from mindspore import dtype as mstype\n",
    "from mindflow.common import get_warmup_cosine_annealing_lr\n",
    "from mindflow.loss import RelativeRMSELoss\n",
    "from mindflow.utils import load_yaml_config, print_log\n",
    "\n",
    "from src import Trainer, init_dataset, init_model, plt_log, check_file_path, count_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE,\n",
    "                    save_graphs=False,\n",
    "                    device_target=\"Ascend\",\n",
    "                    device_id=0)\n",
    "use_ascend = context.get_context(\"device_target\") == \"Ascend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configure Network and Training Parameters\n",
    "\n",
    "Load four types of parameters from the configuration file, which are model-related parameters (model), data-related parameters (data), optimizer-related parameters (optimizer) and callback-related parameters(callback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = load_yaml_config(\"./config/2D_unsteady.yaml\")\n",
    "data_params = config[\"data\"]\n",
    "model_params = config[\"model\"]\n",
    "optimizer_params = config[\"optimizer\"]\n",
    "summary_params = config[\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets Preparation\n",
    "\n",
    "Dataset download link: [data_driven/airfoil/2D_unsteady](https://download.mindspore.cn/mindscience/mindflow/dataset/applications/data_driven/airfoil/2D_unsteady/)\n",
    "\n",
    "The data is numpy npz file with a dimension(*t*, *H*, *W*, *C*) of (9997, 128, 128, 3), where *t* is the total time steps, *H* and *W* are flow field resolution, *C* is the channel number(3 channels are velocity *U*, *V* and pressure *P*, respectively).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size (3560, 8, 128, 128, 3)\n",
      "label size (3560, 32, 128, 128, 3)\n",
      "train_batch_size : 8\n",
      "train dataset size: 2967\n",
      "test dataset size: 593\n",
      "train batch dataset size: 370\n",
      "test batch dataset size: 74\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, means, stds = init_dataset(data_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "The model is built by calling initial_model() function. Before calling function, loss_scaler and compute_dtype should be customized according to the corresponding hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_dtype_of_FNO Float16\n"
     ]
    }
   ],
   "source": [
    "if use_ascend:\n",
    "    from mindspore.amp import DynamicLossScaler, all_finite, auto_mixed_precision\n",
    "    loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "    compute_dtype = mstype.float16\n",
    "    model = init_model(\"fno2d\", data_params, model_params, compute_dtype=compute_dtype)\n",
    "    auto_mixed_precision(model, optimizer_params[\"amp_level\"][\"fno2d\"])\n",
    "else:\n",
    "    context.set_context(enable_graph_kernel=True)\n",
    "    loss_scaler = None\n",
    "    compute_dtype = mstype.float32\n",
    "    model = init_model(\"fno2d\", data_params, model_params, compute_dtype=compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and optimizer\n",
    "\n",
    "In the current application, RelativeRMSELoss and AdamWeightDecay are selected to be the loss function and optimizer, respectively. Meanwhile the warmup_cosine_annealing_lr strategy is selected as the learning rate scheduler. Users could also customize their own loss function and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameter count: 9464259\n",
      "learing rate: 0.001, T_in: 8, T_out: 32\n"
     ]
    }
   ],
   "source": [
    "loss_fn = RelativeRMSELoss()\n",
    "summary_dir = os.path.join(summary_params[\"summary_dir\"], \"Exp01\", \"fno2d\")\n",
    "ckpt_dir = os.path.join(summary_dir, \"ckpt_dir\")\n",
    "check_file_path(ckpt_dir)\n",
    "check_file_path(os.path.join(ckpt_dir, 'img'))\n",
    "print_log('model parameter count:', count_params(model.trainable_params()))\n",
    "print_log(\n",
    "    f'learing rate: {optimizer_params[\"lr\"][\"fno2d\"]}, T_in: {data_params[\"T_in\"]}, T_out: {data_params[\"T_out\"]}')\n",
    "steps_per_epoch = train_dataset.get_dataset_size()\n",
    "\n",
    "lr = get_warmup_cosine_annealing_lr(optimizer_params[\"lr\"][\"fno2d\"], steps_per_epoch,\n",
    "                                    optimizer_params[\"epochs\"], optimizer_params[\"warm_up_epochs\"])\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(),\n",
    "                               learning_rate=Tensor(lr),\n",
    "                               weight_decay=optimizer_params[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function\n",
    "\n",
    "With **MindSpore>= 2.0.0**, users could train neural networks using functional programming paradigms, and single-step training functions are decorated with jit. The data_sink function is used to transfer the step-by-step training function and training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, data_params, loss_fn, means, stds)\n",
    "\n",
    "def forward_fn(inputs, labels):\n",
    "    loss, _, _, _ = trainer.get_loss(inputs, labels)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.scale(loss)\n",
    "    return loss\n",
    "\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "@jit\n",
    "def train_step(inputs, labels):\n",
    "    loss, grads = grad_fn(inputs, labels)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.unscale(loss)\n",
    "        if all_finite(grads):\n",
    "            grads = loss_scaler.unscale(grads)\n",
    "    loss_new = ops.depend(loss, optimizer(grads))\n",
    "    return loss_new\n",
    "\n",
    "def test_step(inputs, labels):\n",
    "    return trainer.get_loss(inputs, labels)\n",
    "\n",
    "train_size = train_dataset.get_dataset_size()\n",
    "test_size = test_dataset.get_dataset_size()\n",
    "train_sink = data_sink(train_step, train_dataset, sink_size=1)\n",
    "test_sink = data_sink(test_step, test_dataset, sink_size=1)\n",
    "test_interval = summary_params[\"test_interval\"]\n",
    "save_ckpt_interval = summary_params[\"save_ckpt_interval\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Inference is performed during model training. Users could directly load the test data set, output the inference precision and save visualization results on the test set every test_interval epochs. Simultaneously, users could save checkpoint file every save_checkpoint_interval steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step time: 2.652332, loss: 0.733017\n",
      "epoch: 2, step time: 0.688175, loss: 0.203251\n",
      "epoch: 3, step time: 0.686817, loss: 0.128816\n",
      "epoch: 4, step time: 0.685909, loss: 0.109786\n",
      "epoch: 5, step time: 0.688545, loss: 0.093725\n",
      "epoch: 6, step time: 0.685986, loss: 0.076027\n",
      "epoch: 7, step time: 0.686459, loss: 0.069847\n",
      "epoch: 8, step time: 0.688228, loss: 0.058694\n",
      "epoch: 9, step time: 0.688053, loss: 0.060886\n",
      "epoch: 10, step time: 0.692221, loss: 0.065305\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 10, loss: 0.03798117920381923\n",
      "---------------------------end test---------------------------\n",
      "epoch: 11, step time: 0.691715, loss: 0.059103\n",
      "epoch: 12, step time: 0.689456, loss: 0.059963\n",
      "epoch: 13, step time: 0.688435, loss: 0.056177\n",
      "epoch: 14, step time: 0.688293, loss: 0.044799\n",
      "epoch: 15, step time: 0.688754, loss: 0.048757\n",
      "epoch: 16, step time: 0.687413, loss: 0.051108\n",
      "epoch: 17, step time: 0.689244, loss: 0.041168\n",
      "epoch: 18, step time: 0.687268, loss: 0.044156\n",
      "epoch: 19, step time: 0.686074, loss: 0.044050\n",
      "epoch: 20, step time: 0.687592, loss: 0.041634\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 20, loss: 0.03290090155693375\n",
      "---------------------------end test---------------------------\n",
      "epoch: 21, step time: 0.686854, loss: 0.038138\n",
      "epoch: 22, step time: 0.688540, loss: 0.040290\n",
      "epoch: 23, step time: 0.687311, loss: 0.037129\n",
      "epoch: 24, step time: 0.685613, loss: 0.042077\n",
      "epoch: 25, step time: 0.688631, loss: 0.035659\n",
      "epoch: 26, step time: 0.687999, loss: 0.031553\n",
      "epoch: 27, step time: 0.689534, loss: 0.036624\n",
      "epoch: 28, step time: 0.687576, loss: 0.040991\n",
      "epoch: 29, step time: 0.685203, loss: 0.034616\n",
      "epoch: 30, step time: 0.687388, loss: 0.029517\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 30, loss: 0.024450113343207062\n",
      "---------------------------end test---------------------------\n",
      "epoch: 31, step time: 0.687894, loss: 0.030733\n",
      "epoch: 32, step time: 0.686834, loss: 0.033166\n",
      "epoch: 33, step time: 0.684156, loss: 0.034593\n",
      "epoch: 34, step time: 0.687594, loss: 0.030224\n",
      "epoch: 35, step time: 0.685184, loss: 0.028527\n",
      "epoch: 36, step time: 0.687365, loss: 0.030338\n",
      "epoch: 37, step time: 0.686692, loss: 0.032679\n",
      "epoch: 38, step time: 0.684623, loss: 0.030559\n",
      "epoch: 39, step time: 0.684311, loss: 0.028198\n",
      "epoch: 40, step time: 0.686643, loss: 0.030848\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 40, loss: 0.024750267230194516\n",
      "---------------------------end test---------------------------\n",
      "epoch: 41, step time: 0.696444, loss: 0.027831\n",
      "epoch: 42, step time: 0.696912, loss: 0.029151\n",
      "epoch: 43, step time: 0.697324, loss: 0.032190\n",
      "epoch: 44, step time: 0.695226, loss: 0.026790\n",
      "epoch: 45, step time: 0.696124, loss: 0.026855\n",
      "epoch: 46, step time: 0.695851, loss: 0.026540\n",
      "epoch: 47, step time: 0.695193, loss: 0.027463\n",
      "epoch: 48, step time: 0.694509, loss: 0.032763\n",
      "epoch: 49, step time: 0.696785, loss: 0.027059\n",
      "epoch: 50, step time: 0.696424, loss: 0.024739\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 50, loss: 0.02536101617246262\n",
      "---------------------------end test---------------------------\n",
      "epoch: 51, step time: 0.691392, loss: 0.025051\n",
      "epoch: 52, step time: 0.691471, loss: 0.021609\n",
      "epoch: 53, step time: 0.686759, loss: 0.022961\n",
      "epoch: 54, step time: 0.684971, loss: 0.025058\n",
      "epoch: 55, step time: 0.687838, loss: 0.024519\n",
      "epoch: 56, step time: 0.688630, loss: 0.025127\n",
      "epoch: 57, step time: 0.684747, loss: 0.023062\n",
      "epoch: 58, step time: 0.685800, loss: 0.024089\n",
      "epoch: 59, step time: 0.687165, loss: 0.023332\n",
      "epoch: 60, step time: 0.687198, loss: 0.022027\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 60, loss: 0.012668337510911294\n",
      "---------------------------end test---------------------------\n",
      "epoch: 61, step time: 0.697993, loss: 0.023954\n",
      "epoch: 62, step time: 0.694950, loss: 0.022935\n",
      "epoch: 63, step time: 0.695095, loss: 0.019998\n",
      "epoch: 64, step time: 0.693050, loss: 0.021672\n",
      "epoch: 65, step time: 0.694423, loss: 0.020992\n",
      "epoch: 66, step time: 0.694051, loss: 0.024120\n",
      "epoch: 67, step time: 0.693227, loss: 0.021222\n",
      "epoch: 68, step time: 0.695719, loss: 0.018898\n",
      "epoch: 69, step time: 0.695193, loss: 0.020580\n",
      "epoch: 70, step time: 0.695925, loss: 0.021619\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 70, loss: 0.018595953204518033\n",
      "---------------------------end test---------------------------\n",
      "epoch: 71, step time: 0.699969, loss: 0.018871\n",
      "epoch: 72, step time: 0.693832, loss: 0.018930\n",
      "epoch: 73, step time: 0.696967, loss: 0.020276\n",
      "epoch: 74, step time: 0.698306, loss: 0.021508\n",
      "epoch: 75, step time: 0.696895, loss: 0.017177\n",
      "epoch: 76, step time: 0.698356, loss: 0.021475\n",
      "epoch: 77, step time: 0.694738, loss: 0.018566\n",
      "epoch: 78, step time: 0.691645, loss: 0.018249\n",
      "epoch: 79, step time: 0.693678, loss: 0.019016\n",
      "epoch: 80, step time: 0.693986, loss: 0.021684\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 80, loss: 0.03425668393233414\n",
      "---------------------------end test---------------------------\n",
      "epoch: 81, step time: 0.694019, loss: 0.018643\n",
      "epoch: 82, step time: 0.693350, loss: 0.019609\n",
      "epoch: 83, step time: 0.696632, loss: 0.017143\n",
      "epoch: 84, step time: 0.694124, loss: 0.016935\n",
      "epoch: 85, step time: 0.693287, loss: 0.017587\n",
      "epoch: 86, step time: 0.696237, loss: 0.016808\n",
      "epoch: 87, step time: 0.692938, loss: 0.016272\n",
      "epoch: 88, step time: 0.693298, loss: 0.016350\n",
      "epoch: 89, step time: 0.693079, loss: 0.016122\n",
      "epoch: 90, step time: 0.695640, loss: 0.016793\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 90, loss: 0.015049820608141631\n",
      "---------------------------end test---------------------------\n",
      "epoch: 91, step time: 0.695330, loss: 0.016076\n",
      "epoch: 92, step time: 0.694938, loss: 0.016224\n",
      "epoch: 93, step time: 0.696670, loss: 0.019537\n",
      "epoch: 94, step time: 0.694749, loss: 0.014278\n",
      "epoch: 95, step time: 0.696291, loss: 0.015967\n",
      "epoch: 96, step time: 0.695401, loss: 0.014452\n",
      "epoch: 97, step time: 0.692933, loss: 0.013790\n",
      "epoch: 98, step time: 0.693399, loss: 0.014776\n",
      "epoch: 99, step time: 0.691681, loss: 0.012814\n",
      "epoch: 100, step time: 0.694035, loss: 0.014525\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 100, loss: 0.014199618234596456\n",
      "---------------------------end test---------------------------\n",
      "epoch: 101, step time: 0.693424, loss: 0.015365\n",
      "epoch: 102, step time: 0.692964, loss: 0.013885\n",
      "epoch: 103, step time: 0.693850, loss: 0.015042\n",
      "epoch: 104, step time: 0.691925, loss: 0.013348\n",
      "epoch: 105, step time: 0.693731, loss: 0.012397\n",
      "epoch: 106, step time: 0.694072, loss: 0.015533\n",
      "epoch: 107, step time: 0.693095, loss: 0.013169\n",
      "epoch: 108, step time: 0.694365, loss: 0.013428\n",
      "epoch: 109, step time: 0.694493, loss: 0.012834\n",
      "epoch: 110, step time: 0.695011, loss: 0.011853\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 110, loss: 0.010528819402286506\n",
      "---------------------------end test---------------------------\n",
      "epoch: 111, step time: 0.699863, loss: 0.015232\n",
      "epoch: 112, step time: 0.695245, loss: 0.012432\n",
      "epoch: 113, step time: 0.696781, loss: 0.012335\n",
      "epoch: 114, step time: 0.696696, loss: 0.012129\n",
      "epoch: 115, step time: 0.696233, loss: 0.011410\n",
      "epoch: 116, step time: 0.691236, loss: 0.013234\n",
      "epoch: 117, step time: 0.694421, loss: 0.012024\n",
      "epoch: 118, step time: 0.694588, loss: 0.011595\n",
      "epoch: 119, step time: 0.695370, loss: 0.011778\n",
      "epoch: 120, step time: 0.695880, loss: 0.011173\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 120, loss: 0.009641800081275555\n",
      "---------------------------end test---------------------------\n",
      "epoch: 121, step time: 0.695209, loss: 0.011097\n",
      "epoch: 122, step time: 0.691141, loss: 0.010855\n",
      "epoch: 123, step time: 0.695012, loss: 0.011044\n",
      "epoch: 124, step time: 0.695229, loss: 0.011432\n",
      "epoch: 125, step time: 0.694662, loss: 0.010873\n",
      "epoch: 126, step time: 0.692417, loss: 0.010477\n",
      "epoch: 127, step time: 0.691926, loss: 0.010197\n",
      "epoch: 128, step time: 0.693930, loss: 0.011748\n",
      "epoch: 129, step time: 0.696376, loss: 0.009750\n",
      "epoch: 130, step time: 0.694301, loss: 0.009874\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 130, loss: 0.007853382740585084\n",
      "---------------------------end test---------------------------\n",
      "epoch: 131, step time: 0.696573, loss: 0.010120\n",
      "epoch: 132, step time: 0.693105, loss: 0.010578\n",
      "epoch: 133, step time: 0.696203, loss: 0.010406\n",
      "epoch: 134, step time: 0.692455, loss: 0.010169\n",
      "epoch: 135, step time: 0.695954, loss: 0.009985\n",
      "epoch: 136, step time: 0.691915, loss: 0.009575\n",
      "epoch: 137, step time: 0.691145, loss: 0.009403\n",
      "epoch: 138, step time: 0.693045, loss: 0.009086\n",
      "epoch: 139, step time: 0.695551, loss: 0.009042\n",
      "epoch: 140, step time: 0.694040, loss: 0.009282\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 140, loss: 0.009246890225472884\n",
      "---------------------------end test---------------------------\n",
      "epoch: 141, step time: 0.688479, loss: 0.008823\n",
      "epoch: 142, step time: 0.689157, loss: 0.009289\n",
      "epoch: 143, step time: 0.685248, loss: 0.009576\n",
      "epoch: 144, step time: 0.687585, loss: 0.009185\n",
      "epoch: 145, step time: 0.685840, loss: 0.009380\n",
      "epoch: 146, step time: 0.689897, loss: 0.008490\n",
      "epoch: 147, step time: 0.690710, loss: 0.008606\n",
      "epoch: 148, step time: 0.688072, loss: 0.008654\n",
      "epoch: 149, step time: 0.689746, loss: 0.008934\n",
      "epoch: 150, step time: 0.688304, loss: 0.008629\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 150, loss: 0.007401696321156468\n",
      "---------------------------end test---------------------------\n",
      "epoch: 151, step time: 0.697744, loss: 0.009447\n",
      "epoch: 152, step time: 0.699723, loss: 0.008901\n",
      "epoch: 153, step time: 0.699869, loss: 0.008984\n",
      "epoch: 154, step time: 0.699634, loss: 0.008882\n",
      "epoch: 155, step time: 0.697683, loss: 0.009762\n",
      "epoch: 156, step time: 0.696223, loss: 0.009919\n",
      "epoch: 157, step time: 0.690939, loss: 0.009107\n",
      "epoch: 158, step time: 0.691592, loss: 0.008638\n",
      "epoch: 159, step time: 0.693206, loss: 0.008478\n",
      "epoch: 160, step time: 0.692778, loss: 0.008224\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 160, loss: 0.007048012673810779\n",
      "---------------------------end test---------------------------\n",
      "epoch: 161, step time: 0.701309, loss: 0.009469\n",
      "epoch: 162, step time: 0.697829, loss: 0.008522\n",
      "epoch: 163, step time: 0.698344, loss: 0.008658\n",
      "epoch: 164, step time: 0.699316, loss: 0.008556\n",
      "epoch: 165, step time: 0.696708, loss: 0.008601\n",
      "epoch: 166, step time: 0.700000, loss: 0.009487\n",
      "epoch: 167, step time: 0.696938, loss: 0.009309\n",
      "epoch: 168, step time: 0.696608, loss: 0.009240\n",
      "epoch: 169, step time: 0.698408, loss: 0.008620\n",
      "epoch: 170, step time: 0.699614, loss: 0.008341\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 170, loss: 0.00696752735077737\n",
      "---------------------------end test---------------------------\n",
      "epoch: 171, step time: 0.700533, loss: 0.008454\n",
      "epoch: 172, step time: 0.701728, loss: 0.008413\n",
      "epoch: 173, step time: 0.700658, loss: 0.008100\n",
      "epoch: 174, step time: 0.697647, loss: 0.008213\n",
      "epoch: 175, step time: 0.700830, loss: 0.007987\n",
      "epoch: 176, step time: 0.697824, loss: 0.007898\n",
      "epoch: 177, step time: 0.698811, loss: 0.007717\n",
      "epoch: 178, step time: 0.698952, loss: 0.007696\n",
      "epoch: 179, step time: 0.695718, loss: 0.007801\n",
      "epoch: 180, step time: 0.694938, loss: 0.007583\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 180, loss: 0.006799178198569869\n",
      "---------------------------end test---------------------------\n",
      "epoch: 181, step time: 0.694381, loss: 0.007583\n",
      "epoch: 182, step time: 0.693612, loss: 0.007847\n",
      "epoch: 183, step time: 0.693077, loss: 0.007647\n",
      "epoch: 184, step time: 0.693908, loss: 0.007516\n",
      "epoch: 185, step time: 0.694870, loss: 0.007329\n",
      "epoch: 186, step time: 0.696124, loss: 0.007347\n",
      "epoch: 187, step time: 0.695269, loss: 0.007284\n",
      "epoch: 188, step time: 0.695620, loss: 0.007207\n",
      "epoch: 189, step time: 0.692392, loss: 0.007136\n",
      "epoch: 190, step time: 0.695566, loss: 0.007082\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 190, loss: 0.006618331926457924\n",
      "---------------------------end test---------------------------\n",
      "epoch: 191, step time: 0.693253, loss: 0.007012\n",
      "epoch: 192, step time: 0.691330, loss: 0.007043\n",
      "epoch: 193, step time: 0.692804, loss: 0.006986\n",
      "epoch: 194, step time: 0.690053, loss: 0.006973\n",
      "epoch: 195, step time: 0.692159, loss: 0.006967\n",
      "epoch: 196, step time: 0.690170, loss: 0.006944\n",
      "epoch: 197, step time: 0.690344, loss: 0.006930\n",
      "epoch: 198, step time: 0.690674, loss: 0.006911\n",
      "epoch: 199, step time: 0.690877, loss: 0.006904\n",
      "epoch: 200, step time: 0.689170, loss: 0.006892\n",
      "---------------------------start test-------------------------\n",
      " test epoch: 200, loss: 0.005457837492891436\n",
      "---------------------------end test---------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, optimizer_params[\"epochs\"] + 1):\n",
    "    time_beg = time.time()\n",
    "    train_l2_step = 0.0\n",
    "    model.set_train()\n",
    "    for step in range(1, train_size + 1):\n",
    "        loss = train_sink()\n",
    "        train_l2_step += loss.asnumpy()\n",
    "    train_l2_step = train_l2_step / train_size / data_params[\"T_out\"]\n",
    "    print_log(\n",
    "        f\"epoch: {epoch}, step time: {(time.time() - time_beg) / steps_per_epoch:>7f}, loss: {train_l2_step:>7f}\")\n",
    "\n",
    "    if epoch % test_interval == 0:\n",
    "        model.set_train(False)\n",
    "        test_l2_by_step = [0.0 for _ in range(data_params[\"T_out\"])]\n",
    "        print_log(\"---------------------------start test-------------------------\")\n",
    "        for step in range(test_size):\n",
    "            _, pred, truth, step_losses = test_sink()\n",
    "            for i in range(data_params[\"T_out\"]):\n",
    "                test_l2_by_step[i] += step_losses[i].asnumpy()\n",
    "        test_l2_by_step = [error / test_size for error in test_l2_by_step]\n",
    "        test_l2_step = np.mean(test_l2_by_step)\n",
    "        print_log(f' test epoch: {epoch}, loss: {test_l2_step}')\n",
    "        print_log(\"---------------------------end test---------------------------\")\n",
    "\n",
    "        plt_log(predicts=pred.asnumpy(),\n",
    "                labels=truth.asnumpy(),\n",
    "                img_dir=os.path.join(ckpt_dir, 'img'),\n",
    "                epoch=epoch\n",
    "                )\n",
    "\n",
    "    if epoch % save_ckpt_interval == 0:\n",
    "        save_checkpoint(model, ckpt_file_name=os.path.join(ckpt_dir, 'airfoil2D_unsteady.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Result visualization**\n",
    "\n",
    "The label value, prediction value and error of pressure P distribution in the flow field of different time steps under UNET2D backbone：\n",
    "\n",
    "![Unet_P.png](images/Unet_P.png)\n",
    "\n",
    "The label value, prediction value and error of velocity U distribution in the flow field of different time steps under UNET2D backbone：\n",
    "\n",
    "![Unet_U.png](images/Unet_U.png)\n",
    "\n",
    "The label value, prediction value and error of velocity V distribution in the flow field of different time steps under UNET2D backbone：\n",
    "\n",
    "![Unet_V.png](images/Unet_V.png)\n",
    "\n",
    "The label value, prediction value and error of pressure P distribution in the flow field of different time steps under FNO2D backbone：\n",
    "\n",
    "![FNO_P.png](images/FNO_P.png)\n",
    "\n",
    "The label value, prediction value and error of velocity U distribution in the flow field of different time steps under FNO2D backbone：\n",
    "\n",
    "![FNO_U.png](images/FNO_U.png)\n",
    "\n",
    "The label value, prediction value and error of velocity V distribution in the flow field of different time steps under FNO2D backbone：\n",
    "\n",
    "![FNO_V.png](images/FNO_V.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zys_test]",
   "language": "python",
   "name": "conda-env-zys_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9063439a3781aed32d6b0dd4804a0c8b51ecec7893a0f31b99846bc91ef39eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
