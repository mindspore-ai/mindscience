{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda19887",
   "metadata": {},
   "source": [
    "\n",
    "# Modeling method of Fluid-structure interaction system based on deep neural network\n",
    "\n",
    "## Overview\n",
    "\n",
    "Aeroelastic problem of aircraft is a typical Fluid–structure interaction (FSI) problem, which studies the coupling relationship between aircraft structure and aerodynamic force. High accuracy Computational fluid dynamics (CFD) technology can accurately simulate the evolution process of the flow field around the structure to obtain the force situation of the structure, but the huge number of grids leads to high computing costs. Many researchers try to use the data-driven method to build the flow field evolution model to achieve rapid prediction of the flow field with high accuracy, so as to improve the simulation efficiency of the Fluid–structure interaction system. In recent years, the rapidly developing deep neural network technology relies on its powerful nonlinear learning ability and deep feature capture ability, and has achieved many successful applications in flow field modeling problems. Among them, flow field reconstruction achieves rapid prediction of different flow fields by constructing a mapping model between geometric shapes and flow conditions to flow field information at spatial points, which is highly concerned for its ability to quickly provide the current flow field state.\n",
    "\n",
    "In order to efficiently solve the flow field reconstruction of the Fluid–structure interaction problem, this paper coupled the neural network model with the computational structural dynamic equation, realized the modeling of the Fluid–structure interaction system, further improved the neural network structure, optimized the data structure, so as to obtain more accurate flow field prediction results and achieve more accurate Fluid–structure interaction response prediction.\n",
    "\n",
    "## Problem description\n",
    "\n",
    "The traditional Fluid–structure interaction numerical simulation framework consists of a Computational fluid dynamics solver and a computational Solid mechanics solver. The two solvers solve the state of the fluid and structure at the next moment in the fluid domain and the solid domain respectively, and transmit information at the interface as the input for the next calculation. The coupling process is shown in the following figure. The Fluid–structure interaction modeling framework based on the depth neural network proposed in this paper still uses the same strategy. The framework uses the depth neural network instead of the CFD solver to predict the flow field evolution. The structural response is still calculated by the CSD solver. The structural displacement and flow field surface pressure are transferred between the depth neural network and the computational Solid mechanics solver.\n",
    "\n",
    "## Technology path\n",
    "\n",
    "The specific process of mindflow to solve this problem is as follows:\n",
    "\n",
    "1.Create data sets based on CFD numerical simulation results.\n",
    "\n",
    "2.The model is built using mindspire deep learning framework.\n",
    "\n",
    "3.Define the optimizer and loss function.\n",
    "\n",
    "4.Use mindspire's instant compilation to accelerate model training.\n",
    "\n",
    "5.Use the trained model for reasoning and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af9033",
   "metadata": {},
   "source": [
    "![p1.png](./images/p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39552eaf",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The basic framework of HDNN consists of convolutional neural network (CNN), convolutional long short-term memory network (ConvLSTM) and deconvolution neural network (DeCNN). CNN reduces the dimensionality of the time series flow field and achieves feature extraction; ConvLSTM learns low dimensional spatiotemporal features and makes predictions; Finally, DeCNN achieves reconstruction of predicted flow fields\n",
    "\n",
    "+ Input layer: current flow field state and boundary conditions\n",
    "+ Convolutional layer: Capturing the spatial features of the flow field and reducing its dimensionality, using low dimensional flow field features to predict flow field evolution can improve computational efficiency\n",
    "+ LSTM layer: predicts the flow field characteristics of the next moment based on the captured current flow field characteristics and structural motion conditions\n",
    "+ Deconvolution output layer: Restores the low-dimensional features of the predicted flow field to high-dimensional space, reconstructs the transient flow field at the next moment through multi-layer DeCNN, and outputs visual prediction results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d5c41",
   "metadata": {},
   "source": [
    "![HDNN.jpg](./images/HDNN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f317de2",
   "metadata": {},
   "source": [
    "## Training dataset\n",
    "\n",
    "The dataset is constructed from multidimensional matrix flow field snapshot matrix constructed from numerical simulation of unsteady two-dimensional cylindrical flow field data\n",
    "\n",
    "+ The moving structure (cylinder) in the flow field makes one-dimensional Simple harmonic motion in the vertical direction. Physical modeling of two-dimensional cylindrical flow field, mesh discretization/partitioning, and solving control equations using Reynolds time averaged simulation method to obtain flow field information. Dimensionalize the physical quantities of the flow field and place grid sampling points in the sampling area to obtain a sample set for training and testing\n",
    "+ Each flow field snapshot contains three channels, representing the pressure distribution information, horizontal velocity information, and vertical velocity information of the flow field\n",
    "+ Dataset:[Download location](https://download.mindspore.cn/mindscience/mindflow/dataset/applications/data_driven/fluid_structure_interaction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce12042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import nn, ops, context, save_checkpoint, set_seed, data_sink, jit\n",
    "from mindflow.utils import load_yaml_config\n",
    "\n",
    "from src import generate_dataset, AEnet, save_loss_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7406dd",
   "metadata": {},
   "source": [
    "## Training environment\n",
    "\n",
    "+ The training adopts the static graphical model of Mindspot framework (GRAPH)\n",
    "+ Train on CPU, GPU, or Ascend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c6d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5ca2c",
   "metadata": {},
   "source": [
    "## Training hyperparameter\n",
    "\n",
    "Obtain hyperparameters for models, data, and optimizers from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f20dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"cylinder around flow ROM\")\n",
    "\n",
    "parser.add_argument(\"--mode\", type=str, default=\"GRAPH\", choices=[\"GRAPH\", \"PYNATIVE\"],\n",
    "                    help=\"Context mode, support 'GRAPH', 'PYNATIVE'\")\n",
    "parser.add_argument(\"--save_graphs\", type=bool, default=False, choices=[True, False],\n",
    "                    help=\"Whether to save intermediate compilation graphs\")\n",
    "parser.add_argument(\"--save_graphs_path\", type=str, default=\"./summary\")\n",
    "parser.add_argument(\"--device_target\", type=str, default=\"Ascend\", choices=[\"GPU\", \"Ascend\"],\n",
    "                    help=\"The target device to run, support 'GPU','Ascend'\")\n",
    "parser.add_argument(\"--device_id\", type=int, default=0, help=\"ID of the target device\")\n",
    "parser.add_argument(\"--data_list\", type=list, default=['5.0', '5.5', '6.0', '6.5'], help=\"The type for training\")\n",
    "parser.add_argument('--batch_size', type=int, default=32, help=\"batch size\")\n",
    "parser.add_argument(\"--config_file_path\", type=str, default=\"./config.yaml\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE if args.mode.upper().startswith(\"GRAPH\") else context.PYNATIVE_MODE,\n",
    "                    save_graphs=args.save_graphs, save_graphs_path=args.save_graphs_path,\n",
    "                    device_target=args.device_target, device_id=args.device_id)\n",
    "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
    "\n",
    "config = load_yaml_config(args.config_file_path)\n",
    "data_params = config[\"data\"]\n",
    "model_params = config[\"model\"]\n",
    "optimizer_params = config[\"optimizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53d5ec",
   "metadata": {},
   "source": [
    "## Training process file save path\n",
    "\n",
    "Save the trained model file in a folder every certain number of training sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa53aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = optimizer_params[\"ckpt_dir\"]\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.mkdir(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505908fc",
   "metadata": {},
   "source": [
    "## Constructing neural network and optimizer\n",
    "\n",
    "The convolutional layer of the neural network has a total of 12 layers, ConvLSTM has 1 layer, and deconvolution has a total of 12 layers\n",
    "\n",
    "The Loss function uses the Mean squared error Loss function, and the optimizer uses the Adam (Adaptive Moment Estimation) optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e0f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEnet(in_channels=model_params[\"in_channels\"],\n",
    "              num_layers=model_params[\"num_layers\"],\n",
    "              kernel_size=model_params[\"kernel_size\"],\n",
    "              num_convlstm_layers=model_params[\"num_convlstm_layers\"])\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = nn.Adam(params=model.trainable_params(), learning_rate=optimizer_params[\"lr\"])\n",
    "if use_ascend:\n",
    "    from mindspore.amp import DynamicLossScaler, auto_mixed_precision, all_finite\n",
    "    loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "    auto_mixed_precision(model, 'O1')\n",
    "else:\n",
    "    loss_scaler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d32ff9",
   "metadata": {},
   "source": [
    "## Training framework\n",
    "\n",
    "Define the forward propagation function forward_ Fn, compare the predicted value with the true value to obtain the loss value and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9864f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fn(inputs, velocity, ur, label):\n",
    "    pred = model(inputs, velocity, ur)\n",
    "    loss = loss_func(pred, label)\n",
    "\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.scale(loss)\n",
    "    return loss\n",
    "\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacf783",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "\n",
    "To generate_dataset parameter transfer to obtain training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe1356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==================Load data sample ===================\")\n",
    "dataset_train, dataset_eval = generate_dataset(data_params[\"data_dir\"],\n",
    "                                               data_params[\"time_steps\"],\n",
    "                                               args.data_list)\n",
    "print(f\"======================End Load========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7331a",
   "metadata": {},
   "source": [
    "## Data sink and model training\n",
    "\n",
    "Define train_ Step and Eval_ Step and use data_ Sink acceleration training, output the loss value and usage time during the training process, and save the model file every certain training round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e692f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"====================Start train=======================\")\n",
    "@jit\n",
    "def train_step(inputs, velocity, ur, label):\n",
    "    loss, grads = grad_fn(inputs, velocity, ur, label)\n",
    "    if use_ascend:\n",
    "        loss = loss_scaler.unscale(loss)\n",
    "        if all_finite(grads):\n",
    "            grads = loss_scaler.unscale(grads)\n",
    "    loss = ops.depend(loss, optimizer(grads))\n",
    "    return loss\n",
    "\n",
    "@jit\n",
    "def eval_step(inputs, velocity, ur, label):\n",
    "    loss = forward_fn(inputs, velocity, ur, label)\n",
    "    loss = ops.sqrt(loss)\n",
    "    return loss\n",
    "\n",
    "train_sink_process = data_sink(train_step, dataset_train, sink_size=1)\n",
    "eval_sink_process = data_sink(eval_step, dataset_eval, sink_size=1)\n",
    "train_data_size, eval_data_size = dataset_train.get_dataset_size(), dataset_eval.get_dataset_size()\n",
    "\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "\n",
    "for epoch in range(1, optimizer_params[\"epochs\"] + 1):\n",
    "    train_losses = 0\n",
    "    valid_losses = 0\n",
    "\n",
    "    local_time_beg = time.time()\n",
    "    model.set_train(True)\n",
    "\n",
    "    for _ in range(train_data_size):\n",
    "        step_train_loss = ops.squeeze(train_sink_process(), axis=())\n",
    "        step_train_loss = step_train_loss.asnumpy().item()\n",
    "        train_losses += step_train_loss\n",
    "\n",
    "    train_loss = train_losses / train_data_size\n",
    "    avg_train_losses.append(train_loss)\n",
    "\n",
    "    print(f\"epoch: {epoch}, epoch average train loss: {train_loss :.6f}, \"\n",
    "          f\"epoch time: {(time.time() - local_time_beg):.2f}s\")\n",
    "\n",
    "    if epoch % optimizer_params[\"eval_interval\"] == 0:\n",
    "        print(f\"=================Start Evaluation=====================\")\n",
    "\n",
    "        eval_time_beg = time.time()\n",
    "        model.set_train(False)\n",
    "        for _ in range(eval_data_size):\n",
    "            step_eval_loss = ops.squeeze(eval_sink_process(), axis=())\n",
    "            step_eval_loss = step_eval_loss.asnumpy().item()\n",
    "            valid_losses += step_eval_loss\n",
    "\n",
    "        valid_loss = valid_losses / eval_data_size\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        print(f\"epoch: {epoch}, epoch average valid loss: {valid_loss :.6f}, \"\n",
    "              f\"epoch time: {(time.time() - eval_time_beg):.2f}s\")\n",
    "        print(f\"==================End Evaluation======================\")\n",
    "\n",
    "    if epoch % optimizer_params[\"save_ckpt_interval\"] == 0:\n",
    "        save_checkpoint(model, f\"{ckpt_dir}/net_{epoch}.ckpt\")\n",
    "\n",
    "save_loss_curve(avg_train_losses, 'Epoch', 'avg_train_losses', 'Avg_train_losses Curve', 'Avg_train_losses.png')\n",
    "save_loss_curve(avg_valid_losses, 'Epoch', 'avg_valid_losses', 'Avg_valid_losses Curve', 'Avg_valid_losses.png')\n",
    "\n",
    "print(f\"=====================End train========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1262b22",
   "metadata": {},
   "source": [
    "## Set training conditions for parameter transmission\n",
    "\n",
    "When running the file, pass in the necessary parameters through the parameter parser to start training, and print the process and device id, as well as the total training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26ff8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Process ID:\", os.getpid())\n",
    "    print(f\"device id: {args.device_id}\")\n",
    "    start_time = time.time()\n",
    "    train()\n",
    "    print(f\"End-to-End total time: {(time.time() - start_time):.2f}s\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bcfdbdd",
   "metadata": {},
   "source": [
    "Process ID: 529681\n",
    "device id: 0\n",
    "==================Load data sample ===================\n",
    "======================End Load========================\n",
    "\n",
    "====================Start train=======================\n",
    "epoch: 1, epoch average train loss: 0.092563, epoch time: 60.48s\n",
    "epoch: 2, epoch average train loss: 0.033426, epoch time: 39.88s\n",
    "epoch: 3, epoch average train loss: 0.009924, epoch time: 33.47s\n",
    "epoch: 4, epoch average train loss: 0.003757, epoch time: 34.95s\n",
    "epoch: 5, epoch average train loss: 0.002671, epoch time: 34.38s\n",
    "epoch: 6, epoch average train loss: 0.002416, epoch time: 38.55s\n",
    "epoch: 7, epoch average train loss: 0.001826, epoch time: 38.72s\n",
    "epoch: 8, epoch average train loss: 0.001770, epoch time: 35.42s\n",
    "epoch: 9, epoch average train loss: 0.001415, epoch time: 36.65s\n",
    "epoch: 10, epoch average train loss: 0.001385, epoch time: 35.20s\n",
    "=================Start Evaluation=====================\n",
    "epoch: 10, epoch average valid loss: 0.033140, epoch time: 10.51s\n",
    "==================End Evaluation======================\n",
    "\n",
    "...\n",
    "\n",
    "epoch: 191, epoch average train loss: 0.000208, epoch time: 38.77s\n",
    "epoch: 192, epoch average train loss: 0.000159, epoch time: 39.22s\n",
    "epoch: 193, epoch average train loss: 0.000320, epoch time: 38.57s\n",
    "epoch: 194, epoch average train loss: 0.000156, epoch time: 39.06s\n",
    "epoch: 195, epoch average train loss: 0.000164, epoch time: 39.48s\n",
    "epoch: 196, epoch average train loss: 0.000175, epoch time: 39.90s\n",
    "epoch: 197, epoch average train loss: 0.000210, epoch time: 38.63s\n",
    "epoch: 198, epoch average train loss: 0.000178, epoch time: 38.70s\n",
    "epoch: 199, epoch average train loss: 0.000246, epoch time: 34.93s\n",
    "epoch: 200, epoch average train loss: 0.000165, epoch time: 35.63s\n",
    "=================Start Evaluation=====================\n",
    "epoch: 200, epoch average valid loss: 0.011407, epoch time: 9.19s\n",
    "==================End Evaluation======================\n",
    "=====================End train========================\n",
    "End-to-End total time: 7694.45s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aac646",
   "metadata": {},
   "source": [
    "## Visualization of predicted flow field results\n",
    "\n",
    "+ Moving boundary flow field prediction starts by executing eval.py. The coupled model can complete the prediction task of the entire Fluid–structure interaction evolution process under the condition that only the initial flow field state and cylinder position are given\n",
    "+ The following figure shows the flow field prediction status of a fully trained HDNN model for a deep neural network at different times within a cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e40259",
   "metadata": {},
   "source": [
    "![pred_cycle_puv.jpg](./images/pred_cycle_puv.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
