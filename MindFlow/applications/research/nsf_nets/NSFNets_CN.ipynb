{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7a09e7-32fc-4059-9478-6c6fc6667f57",
   "metadata": {},
   "source": [
    "# NSFNets: 用于不可压缩 Navier-Stokes 方程求解的物理信息神经网络\n",
    "\n",
    "## 环境安装\n",
    "\n",
    "本案例要求 MindSpore >= 2.0.0 版本以调用如下接口: mindspore.jit, mindspore.jit_class, mindspore.data_sink。具体请查看MindSpore安装。\n",
    "\n",
    "此外，你需要安装 MindFlow >=0.1.0 版本。如果当前环境还没有安装，请按照下列方式选择后端和版本进行安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a449343-731b-46bb-b8dd-f7a8c1ba8071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping mindflow-gpu as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "mindflow_version = \"0.1.0\"  # update if needed\n",
    "# GPU Comment out the following code if you are using NPU.\n",
    "!pip uninstall -y mindflow-gpu\n",
    "!pip install mindflow-gpu==$mindflow_version\n",
    "\n",
    "# NPU Uncomment if needed.\n",
    "# !pip uninstall -y mindflow-ascend\n",
    "# !pip install mindflow-ascend==$mindflow_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe4081-9d91-44c1-b7de-b50a7645e090",
   "metadata": {},
   "source": [
    "## 背景介绍\n",
    "\n",
    "在AI-CFD的工业应用方面，对NavierStokes方程的求解以及对多精度数据的融合的场景十分广泛，具有重要的经济和社会意义。求解不适定问题（例如部分边界条件缺失）或反演问题是其中的重点和难点之一，且往往代价高昂，需要推导适用于特定问题的不同公式、编写全新代码。如何用一套统一代码以相同计算代价解决上述问题亟需深入研究。在此，金晓威和李惠等使用物理信息神经网络（PINNs），通过自动微分将控制方程直接编码到深度神经网络中，以克服上述一些模拟不可压缩层流和湍流流动的限制。并开发了Navier-Stokes流动网络（NSFnets，Navier-Stokes flow nets）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50540e43-3c8e-45f0-8e2b-78831ec89684",
   "metadata": {},
   "source": [
    "## 模型框架\n",
    "\n",
    "模型框架图如下所示:\n",
    "\n",
    "![NSFNet](images/NSFNet.png)\n",
    "\n",
    "图中表示的是压力-速度形式的纳维-斯托克斯方程求解网络，利用神经网络的自动微分计算方程中所需要的偏导项，损失函数包括，边界条件损失，初始条件损失以及为了满足方程平衡的物理损失。\n",
    "\n",
    "## 准备环节\n",
    "\n",
    "实践前，确保已经正确安装合适版本的MindSpore。如果没有，可以通过：\n",
    "\n",
    "* [MindSpore安装页面](https://www.mindspore.cn/install) 安装MindSpore。\n",
    "\n",
    "## 数据集的准备\n",
    "\n",
    "数据集可以通过提供的代码直接生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23990483-d613-4c8b-aa7e-0a8b16a89b13",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "引入代码包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d17b8a-bde1-4ccd-9345-ccfaa2cd1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import set_seed, context, nn, Tensor\n",
    "from src.network import VPNSFNets\n",
    "from src.datasets import read_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e1d64-580b-4ae9-a7f6-164b0ee0adf3",
   "metadata": {},
   "source": [
    "模型相关参数的设置以及训练模型的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a84c0ff-d4cf-41dd-8afc-db8f234d2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, default='NSFNet')\n",
    "parser.add_argument('--case', type=str, default='Three dimensional Beltrami flow')\n",
    "parser.add_argument('--device', type=str, default='GPU')\n",
    "parser.add_argument('--device_id', type=str, default='1')\n",
    "parser.add_argument('--load_params', type=str, default=False)\n",
    "parser.add_argument('--second_path', type=str, default='train')\n",
    "parser.add_argument('--network_size', type=int, default=[4] + 10 * [100 * 1] + [4])\n",
    "parser.add_argument('--learning_rate', type=int, default=[1.0e-03, 1.0e-04, 1.0e-05, 1.0e-06])\n",
    "parser.add_argument('--epochs', type=int, default=[5e3, 5e3, 5e4, 5e4])\n",
    "parser.add_argument('--batch_size', type=int, default=10000)\n",
    "parser.add_argument('--re', type=int, default=1)\n",
    "args = parser.parse_known_args()[0]\n",
    "# args = parser.parse_args()\n",
    "\n",
    "model_name = args.model_name\n",
    "case = args.case\n",
    "device = args.device\n",
    "device_id = args.device_id\n",
    "network_size = args.network_size\n",
    "learning_rate = args.learning_rate\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "load_params = args.load_params\n",
    "second_path = args.second_path\n",
    "re = args.re\n",
    "\n",
    "use_ascend = context.get_context(attr_key='device_target') == \"Ascend\"\n",
    "\n",
    "if use_ascend:\n",
    "    msfloat_type = ms.float16\n",
    "    npfloat_type = np.float16\n",
    "else:\n",
    "    msfloat_type = ms.float32\n",
    "    npfloat_type = np.float32\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = device_id\n",
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target=device)\n",
    "\n",
    "x_b, y_b, z_b, t_b, x_i, y_i, z_i, t_i, u_b, v_b, w_b, u_i, v_i, w_i, x_f, y_f, z_f, t_f, X_min, X_max = read_training_data()\n",
    "\n",
    "model = VPNSFNets(x_b, y_b, z_b, t_b, x_i, y_i, z_i, t_i, u_b, v_b, w_b, u_i, v_i, w_i, x_f, y_f, z_f, t_f, network_size, re, \\\n",
    "                  X_min, X_max, use_ascend, msfloat_type, npfloat_type, load_params, second_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cb210-078e-41e1-b960-e432af9c8d5f",
   "metadata": {},
   "source": [
    "设置种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e625a-262a-4f06-a4a7-834766ed6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "set_seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8c191-ea21-4055-9d32-dfae30c96a33",
   "metadata": {},
   "source": [
    "代码训练与输出结果部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1733f-6e6a-47ff-928f-be3a348e98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, NIter, lr, batch_size=batch_size):\n",
    "    params = model.dnn.trainable_params()\n",
    "    optimizer_Adam = nn.Adam(params, learning_rate=lr)\n",
    "    grad_fn = ms.value_and_grad(model.loss_fn, None, optimizer_Adam.parameters, has_aux=True)\n",
    "    model.dnn.set_train()\n",
    "    N_data = model.t_f.shape[0]\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, 1+NIter):\n",
    "        idx_data_0 = np.random.choice(N_data, batch_size)\n",
    "        idx_data = Tensor(idx_data_0)\n",
    "        x_batch = model.x_f[idx_data, :]\n",
    "        y_batch = model.y_f[idx_data, :]\n",
    "        z_batch = model.z_f[idx_data, :]\n",
    "        t_batch = model.t_f[idx_data, :]\n",
    "        (loss, loss_b, loss_i, loss_f), grads = grad_fn(model.xb, model.yb, model.zb, model.tb, model.xi, model.yi, model.zi, model.ti, x_batch, y_batch, z_batch, t_batch, model.ub, model.vb, model.wb, model.ui, model.vi, model.wi)\n",
    "        optimizer_Adam(grads)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('It: %d, Total_loss: %.3e, Loss_b: %.3e, Loss_i: %.3e, Loss_f: %.3e, Time: %.2f' %\\\n",
    "                    (epoch, loss.item(), loss_b.item(), loss_i.item(), loss_f.item(), elapsed))\n",
    "            loss_history_adam_pretrain = np.empty([0])\n",
    "            loss_b_history_adam_pretrain = np.empty([0])\n",
    "            loss_i_history_adam_pretrain = np.empty([0])\n",
    "            loss_f_history_adam_pretrain = np.empty([0])\n",
    "\n",
    "            loss_history_adam_pretrain = np.append(loss_history_adam_pretrain, loss.numpy())\n",
    "            loss_b_history_adam_pretrain = np.append(loss_b_history_adam_pretrain, loss_b.numpy())\n",
    "            loss_i_history_adam_pretrain = np.append(loss_i_history_adam_pretrain, loss_i.numpy())\n",
    "            loss_f_history_adam_pretrain = np.append(loss_f_history_adam_pretrain, loss_f.numpy())\n",
    "\n",
    "            start_time = time.time()\n",
    "    np.save(f'Loss-Coe/train/loss_history_adam_pretrain', loss_history_adam_pretrain)\n",
    "    np.save(f'Loss-Coe/train/loss_b_history_adam_pretrain', loss_b_history_adam_pretrain)\n",
    "    np.save(f'Loss-Coe/train/loss_i_history_adam_pretrain', loss_i_history_adam_pretrain)\n",
    "    np.save(f'Loss-Coe/train/loss_f_history_adam_pretrain', loss_f_history_adam_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedd05b-3d38-4156-b061-8278d232a7f9",
   "metadata": {},
   "source": [
    "运行训练与保存训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658395b-cade-408e-b999-585ba1ae73d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Total_loss: 1.583e+02, Loss_b: 6.385e-01, Loss_i: 7.558e-01, Loss_f: 1.891e+01, Time: 14.63\n",
      "It: 10, Total_loss: 1.339e+02, Loss_b: 5.654e-01, Loss_i: 6.433e-01, Loss_f: 1.302e+01, Time: 4.17\n",
      "It: 20, Total_loss: 8.946e+01, Loss_b: 3.713e-01, Loss_i: 4.430e-01, Loss_f: 8.031e+00, Time: 4.22\n",
      "It: 30, Total_loss: 6.822e+01, Loss_b: 3.025e-01, Loss_i: 2.891e-01, Loss_f: 9.061e+00, Time: 4.23\n",
      "It: 40, Total_loss: 5.122e+01, Loss_b: 2.249e-01, Loss_i: 2.195e-01, Loss_f: 6.779e+00, Time: 4.23\n",
      "It: 50, Total_loss: 4.095e+01, Loss_b: 1.834e-01, Loss_i: 1.549e-01, Loss_f: 7.122e+00, Time: 4.22\n",
      "It: 60, Total_loss: 3.723e+01, Loss_b: 1.664e-01, Loss_i: 1.392e-01, Loss_f: 6.662e+00, Time: 4.25\n",
      "It: 70, Total_loss: 3.123e+01, Loss_b: 1.457e-01, Loss_i: 1.092e-01, Loss_f: 5.742e+00, Time: 4.26\n",
      "It: 80, Total_loss: 2.830e+01, Loss_b: 1.286e-01, Loss_i: 9.858e-02, Loss_f: 5.586e+00, Time: 4.23\n",
      "It: 90, Total_loss: 2.403e+01, Loss_b: 1.046e-01, Loss_i: 7.816e-02, Loss_f: 5.755e+00, Time: 4.28\n",
      "It: 100, Total_loss: 2.149e+01, Loss_b: 9.192e-02, Loss_i: 6.689e-02, Loss_f: 5.609e+00, Time: 4.27\n",
      "It: 110, Total_loss: 1.913e+01, Loss_b: 8.201e-02, Loss_i: 5.710e-02, Loss_f: 5.223e+00, Time: 4.29\n",
      "It: 120, Total_loss: 2.604e+01, Loss_b: 1.202e-01, Loss_i: 9.009e-02, Loss_f: 5.009e+00, Time: 4.31\n",
      "It: 130, Total_loss: 1.589e+01, Loss_b: 6.606e-02, Loss_i: 4.568e-02, Loss_f: 4.713e+00, Time: 4.31\n",
      "It: 140, Total_loss: 1.695e+01, Loss_b: 7.117e-02, Loss_i: 5.391e-02, Loss_f: 4.445e+00, Time: 4.29\n",
      "It: 150, Total_loss: 1.529e+01, Loss_b: 6.290e-02, Loss_i: 4.427e-02, Loss_f: 4.573e+00, Time: 4.33\n",
      "It: 160, Total_loss: 1.585e+01, Loss_b: 6.501e-02, Loss_i: 5.110e-02, Loss_f: 4.235e+00, Time: 4.31\n",
      "It: 170, Total_loss: 1.325e+01, Loss_b: 5.349e-02, Loss_i: 3.818e-02, Loss_f: 4.081e+00, Time: 4.30\n",
      "It: 180, Total_loss: 1.365e+01, Loss_b: 5.630e-02, Loss_i: 4.049e-02, Loss_f: 3.966e+00, Time: 4.37\n",
      "It: 190, Total_loss: 1.153e+01, Loss_b: 4.635e-02, Loss_i: 3.206e-02, Loss_f: 3.685e+00, Time: 4.30\n",
      "It: 200, Total_loss: 1.048e+01, Loss_b: 4.066e-02, Loss_i: 2.881e-02, Loss_f: 3.531e+00, Time: 4.35\n",
      "It: 210, Total_loss: 9.349e+00, Loss_b: 3.611e-02, Loss_i: 2.546e-02, Loss_f: 3.193e+00, Time: 4.35\n",
      "It: 220, Total_loss: 3.285e+01, Loss_b: 1.496e-01, Loss_i: 1.481e-01, Loss_f: 3.084e+00, Time: 4.36\n",
      "It: 230, Total_loss: 1.309e+01, Loss_b: 5.685e-02, Loss_i: 4.612e-02, Loss_f: 2.790e+00, Time: 4.35\n",
      "It: 240, Total_loss: 8.711e+00, Loss_b: 3.276e-02, Loss_i: 2.655e-02, Loss_f: 2.780e+00, Time: 4.37\n",
      "It: 250, Total_loss: 8.350e+00, Loss_b: 3.038e-02, Loss_i: 2.387e-02, Loss_f: 2.925e+00, Time: 4.31\n",
      "It: 260, Total_loss: 3.457e+01, Loss_b: 1.534e-01, Loss_i: 1.672e-01, Loss_f: 2.513e+00, Time: 4.39\n",
      "It: 270, Total_loss: 1.192e+01, Loss_b: 5.266e-02, Loss_i: 3.971e-02, Loss_f: 2.680e+00, Time: 4.35\n",
      "It: 280, Total_loss: 1.006e+01, Loss_b: 4.145e-02, Loss_i: 3.491e-02, Loss_f: 2.421e+00, Time: 4.33\n",
      "It: 290, Total_loss: 6.854e+00, Loss_b: 2.660e-02, Loss_i: 1.906e-02, Loss_f: 2.288e+00, Time: 4.35\n",
      "It: 300, Total_loss: 6.489e+00, Loss_b: 2.398e-02, Loss_i: 1.775e-02, Loss_f: 2.316e+00, Time: 4.31\n",
      "It: 310, Total_loss: 5.934e+00, Loss_b: 2.192e-02, Loss_i: 1.608e-02, Loss_f: 2.135e+00, Time: 4.36\n",
      "It: 320, Total_loss: 5.373e+00, Loss_b: 1.975e-02, Loss_i: 1.401e-02, Loss_f: 1.997e+00, Time: 4.32\n",
      "It: 330, Total_loss: 2.589e+01, Loss_b: 1.093e-01, Loss_i: 1.278e-01, Loss_f: 2.179e+00, Time: 4.35\n",
      "It: 340, Total_loss: 7.844e+00, Loss_b: 3.105e-02, Loss_i: 2.841e-02, Loss_f: 1.897e+00, Time: 4.33\n",
      "It: 350, Total_loss: 5.789e+00, Loss_b: 2.054e-02, Loss_i: 1.715e-02, Loss_f: 2.020e+00, Time: 4.30\n",
      "It: 360, Total_loss: 4.929e+00, Loss_b: 1.746e-02, Loss_i: 1.271e-02, Loss_f: 1.912e+00, Time: 4.35\n",
      "It: 370, Total_loss: 4.540e+00, Loss_b: 1.605e-02, Loss_i: 1.103e-02, Loss_f: 1.832e+00, Time: 4.33\n",
      "It: 380, Total_loss: 4.350e+00, Loss_b: 1.506e-02, Loss_i: 1.077e-02, Loss_f: 1.767e+00, Time: 4.34\n",
      "It: 390, Total_loss: 4.076e+00, Loss_b: 1.387e-02, Loss_i: 9.670e-03, Loss_f: 1.721e+00, Time: 4.32\n",
      "It: 400, Total_loss: 3.871e+00, Loss_b: 1.333e-02, Loss_i: 9.054e-03, Loss_f: 1.633e+00, Time: 4.35\n",
      "It: 410, Total_loss: 3.836e+00, Loss_b: 1.356e-02, Loss_i: 9.326e-03, Loss_f: 1.547e+00, Time: 4.39\n",
      "It: 420, Total_loss: 1.469e+01, Loss_b: 6.135e-02, Loss_i: 7.144e-02, Loss_f: 1.410e+00, Time: 4.35\n",
      "It: 430, Total_loss: 9.486e+00, Loss_b: 3.899e-02, Loss_i: 4.083e-02, Loss_f: 1.503e+00, Time: 4.33\n",
      "It: 440, Total_loss: 4.276e+00, Loss_b: 1.586e-02, Loss_i: 1.093e-02, Loss_f: 1.597e+00, Time: 4.31\n",
      "It: 450, Total_loss: 4.369e+00, Loss_b: 1.655e-02, Loss_i: 1.267e-02, Loss_f: 1.447e+00, Time: 4.38\n",
      "It: 460, Total_loss: 3.708e+00, Loss_b: 1.338e-02, Loss_i: 9.417e-03, Loss_f: 1.428e+00, Time: 4.35\n",
      "It: 470, Total_loss: 3.409e+00, Loss_b: 1.175e-02, Loss_i: 7.869e-03, Loss_f: 1.447e+00, Time: 4.33\n",
      "It: 480, Total_loss: 3.202e+00, Loss_b: 1.058e-02, Loss_i: 7.070e-03, Loss_f: 1.437e+00, Time: 4.30\n",
      "It: 490, Total_loss: 3.011e+00, Loss_b: 1.008e-02, Loss_i: 6.602e-03, Loss_f: 1.342e+00, Time: 4.34\n",
      "It: 500, Total_loss: 2.883e+00, Loss_b: 9.606e-03, Loss_i: 6.225e-03, Loss_f: 1.300e+00, Time: 4.43\n",
      "It: 510, Total_loss: 3.084e+00, Loss_b: 1.055e-02, Loss_i: 7.329e-03, Loss_f: 1.296e+00, Time: 4.34\n",
      "It: 520, Total_loss: 5.132e+00, Loss_b: 2.041e-02, Loss_i: 1.931e-02, Loss_f: 1.160e+00, Time: 4.37\n",
      "It: 530, Total_loss: 4.985e+00, Loss_b: 1.992e-02, Loss_i: 1.770e-02, Loss_f: 1.223e+00, Time: 4.36\n",
      "It: 540, Total_loss: 2.961e+00, Loss_b: 9.838e-03, Loss_i: 6.820e-03, Loss_f: 1.295e+00, Time: 4.35\n",
      "It: 550, Total_loss: 2.722e+00, Loss_b: 9.219e-03, Loss_i: 6.062e-03, Loss_f: 1.194e+00, Time: 4.39\n",
      "It: 560, Total_loss: 2.651e+00, Loss_b: 8.777e-03, Loss_i: 5.731e-03, Loss_f: 1.200e+00, Time: 4.34\n",
      "It: 570, Total_loss: 2.435e+00, Loss_b: 8.007e-03, Loss_i: 5.153e-03, Loss_f: 1.119e+00, Time: 4.31\n",
      "It: 580, Total_loss: 2.359e+00, Loss_b: 7.729e-03, Loss_i: 4.839e-03, Loss_f: 1.103e+00, Time: 4.37\n",
      "It: 590, Total_loss: 2.411e+00, Loss_b: 7.893e-03, Loss_i: 5.292e-03, Loss_f: 1.093e+00, Time: 4.33\n",
      "It: 600, Total_loss: 6.628e+00, Loss_b: 2.613e-02, Loss_i: 2.924e-02, Loss_f: 1.091e+00, Time: 4.33\n",
      "It: 610, Total_loss: 3.092e+00, Loss_b: 1.098e-02, Loss_i: 9.677e-03, Loss_f: 1.026e+00, Time: 4.40\n",
      "It: 620, Total_loss: 2.359e+00, Loss_b: 7.858e-03, Loss_i: 5.543e-03, Loss_f: 1.018e+00, Time: 4.35\n",
      "It: 630, Total_loss: 2.168e+00, Loss_b: 7.162e-03, Loss_i: 4.739e-03, Loss_f: 9.775e-01, Time: 4.34\n",
      "It: 640, Total_loss: 2.054e+00, Loss_b: 6.717e-03, Loss_i: 4.270e-03, Loss_f: 9.550e-01, Time: 4.35\n",
      "It: 650, Total_loss: 1.949e+00, Loss_b: 6.251e-03, Loss_i: 3.827e-03, Loss_f: 9.413e-01, Time: 4.39\n",
      "It: 660, Total_loss: 7.125e+00, Loss_b: 2.947e-02, Loss_i: 3.179e-02, Loss_f: 9.988e-01, Time: 4.35\n",
      "It: 670, Total_loss: 8.039e+00, Loss_b: 3.376e-02, Loss_i: 3.641e-02, Loss_f: 1.022e+00, Time: 4.33\n",
      "It: 680, Total_loss: 2.239e+00, Loss_b: 7.544e-03, Loss_i: 5.005e-03, Loss_f: 9.844e-01, Time: 4.38\n",
      "It: 690, Total_loss: 2.498e+00, Loss_b: 8.721e-03, Loss_i: 7.084e-03, Loss_f: 9.173e-01, Time: 4.37\n",
      "It: 700, Total_loss: 2.013e+00, Loss_b: 6.654e-03, Loss_i: 4.436e-03, Loss_f: 9.038e-01, Time: 4.32\n",
      "It: 710, Total_loss: 1.930e+00, Loss_b: 6.223e-03, Loss_i: 3.988e-03, Loss_f: 9.092e-01, Time: 4.34\n",
      "It: 720, Total_loss: 1.806e+00, Loss_b: 5.569e-03, Loss_i: 3.448e-03, Loss_f: 9.047e-01, Time: 4.42\n",
      "It: 730, Total_loss: 1.723e+00, Loss_b: 5.383e-03, Loss_i: 3.242e-03, Loss_f: 8.601e-01, Time: 4.34\n",
      "It: 740, Total_loss: 3.905e+00, Loss_b: 1.531e-02, Loss_i: 1.506e-02, Loss_f: 8.681e-01, Time: 4.36\n",
      "It: 750, Total_loss: 2.988e+00, Loss_b: 1.194e-02, Loss_i: 1.040e-02, Loss_f: 7.538e-01, Time: 4.35\n",
      "It: 760, Total_loss: 2.161e+00, Loss_b: 7.457e-03, Loss_i: 5.830e-03, Loss_f: 8.320e-01, Time: 4.34\n",
      "It: 770, Total_loss: 1.972e+00, Loss_b: 6.726e-03, Loss_i: 4.789e-03, Loss_f: 8.208e-01, Time: 4.34\n",
      "It: 780, Total_loss: 1.701e+00, Loss_b: 5.803e-03, Loss_i: 3.587e-03, Loss_f: 7.619e-01, Time: 4.29\n",
      "It: 790, Total_loss: 1.591e+00, Loss_b: 5.103e-03, Loss_i: 3.114e-03, Loss_f: 7.698e-01, Time: 4.36\n",
      "It: 800, Total_loss: 1.535e+00, Loss_b: 4.848e-03, Loss_i: 2.912e-03, Loss_f: 7.591e-01, Time: 4.43\n",
      "It: 810, Total_loss: 1.466e+01, Loss_b: 6.288e-02, Loss_i: 7.408e-02, Loss_f: 9.674e-01, Time: 4.37\n",
      "It: 820, Total_loss: 4.528e+00, Loss_b: 1.733e-02, Loss_i: 1.952e-02, Loss_f: 8.425e-01, Time: 4.42\n",
      "It: 830, Total_loss: 2.498e+00, Loss_b: 9.084e-03, Loss_i: 8.434e-03, Loss_f: 7.457e-01, Time: 4.38\n",
      "It: 840, Total_loss: 1.777e+00, Loss_b: 5.724e-03, Loss_i: 4.544e-03, Loss_f: 7.503e-01, Time: 4.41\n",
      "It: 850, Total_loss: 1.575e+00, Loss_b: 5.009e-03, Loss_i: 3.406e-03, Loss_f: 7.332e-01, Time: 4.35\n",
      "It: 860, Total_loss: 1.418e+00, Loss_b: 4.292e-03, Loss_i: 2.733e-03, Loss_f: 7.161e-01, Time: 4.38\n",
      "It: 870, Total_loss: 1.320e+00, Loss_b: 4.021e-03, Loss_i: 2.429e-03, Loss_f: 6.748e-01, Time: 4.34\n",
      "It: 880, Total_loss: 1.290e+00, Loss_b: 3.938e-03, Loss_i: 2.351e-03, Loss_f: 6.612e-01, Time: 4.34\n",
      "It: 890, Total_loss: 1.261e+00, Loss_b: 3.838e-03, Loss_i: 2.264e-03, Loss_f: 6.513e-01, Time: 4.32\n",
      "It: 900, Total_loss: 1.255e+00, Loss_b: 3.763e-03, Loss_i: 2.241e-03, Loss_f: 6.545e-01, Time: 4.34\n",
      "It: 910, Total_loss: 1.232e+00, Loss_b: 3.749e-03, Loss_i: 2.343e-03, Loss_f: 6.233e-01, Time: 4.33\n",
      "It: 920, Total_loss: 6.618e+00, Loss_b: 2.682e-02, Loss_i: 3.284e-02, Loss_f: 6.520e-01, Time: 4.34\n",
      "It: 930, Total_loss: 2.011e+00, Loss_b: 7.589e-03, Loss_i: 6.499e-03, Loss_f: 6.022e-01, Time: 4.39\n",
      "It: 940, Total_loss: 1.656e+00, Loss_b: 5.748e-03, Loss_i: 4.692e-03, Loss_f: 6.119e-01, Time: 4.34\n",
      "It: 950, Total_loss: 1.524e+00, Loss_b: 5.275e-03, Loss_i: 4.060e-03, Loss_f: 5.907e-01, Time: 4.34\n",
      "It: 960, Total_loss: 1.150e+00, Loss_b: 3.567e-03, Loss_i: 2.078e-03, Loss_f: 5.854e-01, Time: 4.36\n",
      "It: 970, Total_loss: 1.136e+00, Loss_b: 3.517e-03, Loss_i: 2.214e-03, Loss_f: 5.624e-01, Time: 4.40\n",
      "It: 980, Total_loss: 1.098e+00, Loss_b: 3.371e-03, Loss_i: 2.037e-03, Loss_f: 5.576e-01, Time: 4.31\n",
      "It: 990, Total_loss: 1.058e+00, Loss_b: 3.218e-03, Loss_i: 1.868e-03, Loss_f: 5.492e-01, Time: 4.34\n",
      "It: 1000, Total_loss: 1.048e+00, Loss_b: 3.260e-03, Loss_i: 1.985e-03, Loss_f: 5.237e-01, Time: 4.35\n",
      "It: 1010, Total_loss: 9.346e+00, Loss_b: 3.993e-02, Loss_i: 4.750e-02, Loss_f: 6.030e-01, Time: 4.38\n",
      "It: 1020, Total_loss: 5.139e+00, Loss_b: 2.211e-02, Loss_i: 2.336e-02, Loss_f: 5.921e-01, Time: 4.34\n",
      "It: 1030, Total_loss: 2.316e+00, Loss_b: 8.761e-03, Loss_i: 8.724e-03, Loss_f: 5.671e-01, Time: 4.33\n",
      "It: 1040, Total_loss: 1.314e+00, Loss_b: 4.416e-03, Loss_i: 3.425e-03, Loss_f: 5.302e-01, Time: 4.34\n",
      "It: 1050, Total_loss: 1.101e+00, Loss_b: 3.623e-03, Loss_i: 2.288e-03, Loss_f: 5.101e-01, Time: 4.35\n",
      "It: 1060, Total_loss: 9.831e-01, Loss_b: 2.902e-03, Loss_i: 1.671e-03, Loss_f: 5.259e-01, Time: 4.31\n",
      "It: 1070, Total_loss: 9.554e-01, Loss_b: 2.894e-03, Loss_i: 1.657e-03, Loss_f: 5.003e-01, Time: 4.33\n",
      "It: 1080, Total_loss: 9.228e-01, Loss_b: 2.796e-03, Loss_i: 1.600e-03, Loss_f: 4.832e-01, Time: 4.33\n",
      "It: 1090, Total_loss: 9.017e-01, Loss_b: 2.708e-03, Loss_i: 1.540e-03, Loss_f: 4.769e-01, Time: 4.35\n",
      "It: 1100, Total_loss: 9.491e-01, Loss_b: 2.927e-03, Loss_i: 1.858e-03, Loss_f: 4.705e-01, Time: 4.37\n",
      "It: 1110, Total_loss: 6.445e+00, Loss_b: 2.654e-02, Loss_i: 3.269e-02, Loss_f: 5.224e-01, Time: 4.38\n",
      "It: 1120, Total_loss: 5.876e+00, Loss_b: 2.427e-02, Loss_i: 2.891e-02, Loss_f: 5.584e-01, Time: 4.34\n",
      "It: 1130, Total_loss: 1.434e+00, Loss_b: 5.268e-03, Loss_i: 4.353e-03, Loss_f: 4.723e-01, Time: 4.35\n"
     ]
    }
   ],
   "source": [
    "for epoch, lr in zip(epochs, learning_rate):\n",
    "    train(model, int(epoch), lr, batch_size=batch_size)\n",
    "ms.save_checkpoint(model.dnn, f'model/{second_path}/model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
