{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Environment Installation\n",
    "\n",
    "This example requires MindSpore version >= 2.2.12 to call the following interfaces: mindspore.nn, mindspore.jit_class, mindspore.data_sink, mindflow_ascend. Please refer to MindSpore installation for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from mindspore import nn, Tensor, context, ops, jit, set_seed, data_sink, save_checkpoint\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore.nn import L1Loss\n",
    "from mindflow.common import get_warmup_cosine_annealing_lr\n",
    "from mindflow.utils import load_yaml_config, print_log\n",
    "from src.utils import Trainer, init_model, check_file_path, count_params, plot_image, plot_image_first\n",
    "from src.dataset import init_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the code runs on the Ascend device and check whether the device target has been successfully set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"train\"\"\"\n",
    "    set_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    context.set_context(mode=context.GRAPH_MODE,\n",
    "                        save_graphs=False,\n",
    "                        device_target=\"Ascend\",\n",
    "                        device_id=0)\n",
    "    use_ascend = context.get_context(\"device_target\") == \"Ascend\"\n",
    "    print(use_ascend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    config = load_yaml_config(\"./configs/combined_methods.yaml\")\n",
    "    data_params = config[\"data\"]\n",
    "    model_params = config[\"model\"]\n",
    "    optimizer_params = config[\"optimizer\"]\n",
    "    summary_params = config[\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    train_dataset, test_dataset, means, stds = init_dataset(data_params)\n",
    "    print('train_dataset', train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    if use_ascend:\n",
    "        from mindspore.amp import DynamicLossScaler, all_finite, auto_mixed_precision\n",
    "        loss_scaler = DynamicLossScaler(1024, 2, 100)\n",
    "        compute_dtype = mstype.float16\n",
    "        model = init_model(\"unet2d\", data_params, model_params, compute_dtype=compute_dtype)\n",
    "        auto_mixed_precision(model, optimizer_params[\"amp_level\"][\"unet2d\"])\n",
    "    else:\n",
    "        context.set_context(enable_graph_kernel=False)\n",
    "        loss_scaler = None\n",
    "        compute_dtype = mstype.float32\n",
    "        model = init_model(\"unet2d\", data_params, model_params, compute_dtype=compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    loss_fn = L1Loss()\n",
    "    summary_dir = os.path.join(summary_params[\"summary_dir\"], \"Exp_datadriven\", \"unet2d\")\n",
    "    ckpt_dir = os.path.join(summary_dir, \"ckpt_dir\")\n",
    "    check_file_path(ckpt_dir)\n",
    "    check_file_path(os.path.join(ckpt_dir, 'img'))\n",
    "    print_log('model parameter count:', count_params(model.trainable_params()))\n",
    "    print_log(\n",
    "        f'learning rate: {optimizer_params[\"lr\"][\"unet2d\"]}, '\n",
    "        f'T_in: {data_params[\"T_in\"]}, T_out: {data_params[\"T_out\"]}')\n",
    "    steps_per_epoch = train_dataset.get_dataset_size()\n",
    "\n",
    "    lr = get_warmup_cosine_annealing_lr(optimizer_params[\"lr\"][\"unet2d\"], steps_per_epoch,\n",
    "                                        optimizer_params[\"epochs\"], optimizer_params[\"warm_up_epochs\"])\n",
    "    optimizer = nn.AdamWeightDecay(model.trainable_params(),\n",
    "                                   learning_rate=Tensor(lr),\n",
    "                                   weight_decay=optimizer_params[\"weight_decay\"])\n",
    "    trainer = Trainer(model, data_params, loss_fn, means, stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the forward propagation of the model and return the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def forward_fn(inputs, labels):\n",
    "        loss, _, _, _, _, _, _ = trainer.get_loss(inputs, labels)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.scale(loss)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to optimize the model to minimize the loss function through the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    @jit\n",
    "    def train_step(inputs, labels):\n",
    "        loss, grads = grad_fn(inputs, labels)\n",
    "        if use_ascend:\n",
    "            loss = loss_scaler.unscale(loss)\n",
    "            if all_finite(grads):\n",
    "                grads = loss_scaler.unscale(grads)\n",
    "        loss_new = ops.depend(loss, optimizer(grads))\n",
    "        return loss_new, inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_loss` method is responsible for performing the forward propagation of the model,calculating the difference between the predicted output and the true labels, and computing the loss based on this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def test_step(inputs, labels):\n",
    "        return trainer.get_loss(inputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size of the training dataset, which is the total number of batches in the training dataset. This value is used to determine the number of iterations in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    train_size = train_dataset.get_dataset_size()\n",
    "    test_size = test_dataset.get_dataset_size()\n",
    "    train_sink = data_sink(train_step, train_dataset, sink_size=1)\n",
    "    test_sink = data_sink(test_step, test_dataset, sink_size=1)\n",
    "    test_interval = summary_params[\"test_interval\"]\n",
    "    save_ckpt_interval = summary_params[\"save_ckpt_interval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    for epoch in range(1, optimizer_params[\"epochs\"] + 1):\n",
    "        time_beg = time.time()\n",
    "        train_l1 = 0.0\n",
    "        model.set_train()\n",
    "        for _ in range(1, train_size + 1):\n",
    "            loss_train, inputs, labels = train_sink()\n",
    "            train_l1 += loss_train.asnumpy()\n",
    "        train_loss = train_l1 / train_size\n",
    "        if epoch >= trainer.hatch_extent:\n",
    "            _, loss1, loss2, _, _, _, _ = trainer.get_loss(inputs, labels)\n",
    "            trainer.renew_loss_lists(loss1, loss2)\n",
    "            trainer.adjust_hatchs()\n",
    "        print_log(\n",
    "            f\"epoch: {epoch}, \"\n",
    "            f\"step time: {(time.time() - time_beg) / steps_per_epoch:>7f}, \"\n",
    "            f\"loss: {train_loss:>7f}\")\n",
    "\n",
    "        if epoch % test_interval == 0:\n",
    "            model.set_train(False)\n",
    "            test_l1 = 0.0\n",
    "            for _ in range(test_size):\n",
    "                loss_test, loss1, loss2, inputs, pred, labels, _ = test_sink()\n",
    "                test_l1 += loss_test.asnumpy()\n",
    "            test_loss = test_l1 / test_size\n",
    "            print_log(\n",
    "                f\"epoch: {epoch}, \"\n",
    "                f\"step time: {(time.time() - time_beg) / steps_per_epoch:>7f}, \"\n",
    "                f\"loss: {test_loss:>7f}\")\n",
    "\n",
    "            plot_image(inputs, 0)\n",
    "            plot_image_first(inputs, 0)\n",
    "            plot_image(pred, 0)\n",
    "            plot_image(labels, 0)\n",
    "\n",
    "        if epoch % save_ckpt_interval == 0:\n",
    "            save_checkpoint(model, ckpt_file_name=os.path.join(ckpt_dir, 'model_data.ckpt'))\n",
    "\n",
    "    print(\"Training Finished!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch: 1, step time: 7.204487, loss: 30.159577 epoch: 2, step time: 0.050003, loss: 16.109943 epoch: 3, step time: 23.267395, loss: 14.340383 epoch: 4, step time: 0.059521, loss: 11.345826 epoch: 5, step time: 0.059669, loss: 8.936566 epoch: 6, step time: 0.060332, loss: 7.976031 epoch: 7, step time: 0.059778, loss: 7.400826 epoch: 8, step time: 0.057766, loss: 7.384783 epoch: 9, step time: 0.058148, loss: 6.274703 epoch: 10, step time: 0.058232, loss: 7.144862 epoch: 10, step time: 0.930520, loss: 0.105834 epoch: 11, step time: 0.056829, loss: 6.353740 epoch: 12, step time: 0.055501, loss: 7.061435 epoch: 13, step time: 0.056191, loss: 5.679731 epoch: 14, step time: 0.056355, loss: 6.572665 epoch: 15, step time: 0.056130, loss: 5.429127 epoch: 16, step time: 0.056419, loss: 5.232585 epoch: 17, step time: 0.056626, loss: 4.742438 epoch: 18, step time: 0.056156, loss: 5.042458 epoch: 19, step time: 0.056799, loss: 4.441929 epoch: 20, step time: 0.055955, loss: 4.645259 epoch: 20, step time: 0.061364, loss: 0.061257 epoch: 21, step time: 0.061526, loss: 4.051717 epoch: 22, step time: 0.057296, loss: 4.084571 epoch: 23, step time: 0.056526, loss: 3.812290 epoch: 24, step time: 0.056620, loss: 4.036302 epoch: 25, step time: 0.056961, loss: 4.224667 epoch: 26, step time: 0.056612, loss: 3.680945 epoch: 27, step time: 0.056421, loss: 3.704518 epoch: 28, step time: 0.056409, loss: 3.288220 epoch: 29, step time: 0.056580, loss: 3.349201 epoch: 30, step time: 0.056552, loss: 4.562499 epoch: 30, step time: 0.062123, loss: 0.056036 epoch: 31, step time: 0.058193, loss: 5.577391 epoch: 32, step time: 0.056697, loss: 4.279838 epoch: 33, step time: 0.055909, loss: 4.410978 epoch: 34, step time: 0.056718, loss: 3.254215 epoch: 35, step time: 0.056718, loss: 3.463492 epoch: 36, step time: 0.056144, loss: 3.151621 epoch: 37, step time: 0.056268, loss: 2.815826 epoch: 38, step time: 0.055588, loss: 2.828014 epoch: 39, step time: 0.060484, loss: 2.759430 epoch: 40, step time: 0.056569, loss: 2.598535 epoch: 40, step time: 0.062142, loss: 0.030793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot1](images/plot1.png)![plot2](images/plot2.png)\n",
    "\"The first image is the input temperature field, and the second image is the training prediction.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
