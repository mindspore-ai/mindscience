{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAE-Transformer flow field prediction model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In order to effectively reduce the design cost and cycle time of using CFD methods, the reduced-order model (ROM) has gained wide attention in recent years. For complex compressible flows, using linear methods such as Proper Orthogonal Decomposition (POD) for flow field dimensionality reduction requires a large number of modes to ensure the reconstruction accuracy. It has been shown that the modes number can be effectively reduced by using nonlinear dimensionality reduction methods. Convolutional Autoencoder (CAE) is a kind of neural network composed of encoder and decoder, which can realize data dimensionality reduction and recon-struction, and can be regarded as a nonlinear extension of POD method. CAE is used for nonlinear dimension-ality reduction, and Transformer is used for time evolution. The CAE-Transformer can obtain high reconstruction and prediction accuracy on the premise of using less latents for unsteady compressible flows.\n",
    "\n",
    "### Framework of CAE-Transformer\n",
    "\n",
    "The basic framework of CAE-Transformer is mainly based on [paper1](https://doi.org/10.13700/j.bh.1001-5965.2022.0085) and [paper2](https://doi.org/10.1609/aaai.v35i12.17325). It consists of CAE and Transformer, where the encoder in CAE reduces the dimensionality of the time series flow field to achieve feature extraction, Transformer learns low dimensional spatiotemporal features and makes predictions, and the decoder in CAE realizes flow field reconstruction:\n",
    "\n",
    "+ Input：Input the flow field for a period of time;\n",
    "\n",
    "+ Compression：Extract high-dimensional spatiotemporal flow characteristics by dimensionality reduction of the flow field using the encoder of CAE;\n",
    "\n",
    "+ Evolution：Learning the evolution of spatiotemporal characteristics of low dimensional spatial flow fields through Transformer and predicting the next moment;\n",
    "\n",
    "+ Reconstruction：Restore the predicted low-dimensional features of the flow field to high-dimensional space through the decoder of CAE;\n",
    "\n",
    "+ Output：Output the predicted results of the transient flow field at the next moment.\n",
    "\n",
    "![CAE-Transformer.png](./images/cae_transformer_structure.png)\n",
    "\n",
    "### Dataset\n",
    "\n",
    "**Source**: Numerical simulation flow field data of two-dimensional flow around a cylinder, provided by the team of Associate Professor Yu Jian, School of Aeronautical Science and Engineering, Beijing University of Aeronautics and Astronautics.\n",
    "\n",
    "**Generation**: The calculation status and establishment method of the dataset of two-dimensional cylindrical flow are described in the paper.\n",
    "\n",
    "**Format**: The data set is numerically simulated for the flow around a cylinder with 10 Reynolds numbers. The flow field data at each Reynolds number contains 401 time steps, and the flow field data at each time step is a 256*256 two-dimensional flow field. The data type of each variable is float32, and the total size of the dataset is about 1.96GB.\n",
    "\n",
    "**Link**: [2D_cylinder_flow.npy](https://download.mindspore.cn/mindscience/mindflow/dataset/applications/data_driven/cae-transformer/2D_cylinder_flow.npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation works\n",
    "\n",
    "Import the library required for training, where the src folder includes dataset processing functions, network models, and training loss visualization functions.\n",
    "\n",
    "The training defaults to the dynamic graph mode (PYNATIVE) of the Mindspore framework, and trains on the GPU (default) or Ascend (single card)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import nn, ops, context, save_checkpoint, set_seed, jit, data_sink\n",
    "from src import create_caetransformer_dataset, plot_train_loss, CaeInformer\n",
    "from eval import cae_transformer_prediction, cae_transformer_eval\n",
    "\n",
    "np.random.seed(0)\n",
    "set_seed(0)\n",
    "parser = argparse.ArgumentParser(description=\"CAE-Transformer for 2D cylinder flow\")\n",
    "parser.add_argument(\n",
    "    \"--mode\",\n",
    "    type=str,\n",
    "    default=\"PYNATIVE\",\n",
    "    choices=[\"GRAPH\", \"PYNATIVE\"],\n",
    "    help=\"Context mode, support 'GRAPH', 'PYNATIVE'\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_graphs\",\n",
    "    type=bool,\n",
    "    default=False,\n",
    "    choices=[True, False],\n",
    "    help=\"Whether to save intermediate compilation graphs\",\n",
    ")\n",
    "parser.add_argument(\"--save_graphs_path\", type=str, default=\"./graphs\")\n",
    "parser.add_argument(\n",
    "    \"--device_target\",\n",
    "    type=str,\n",
    "    default=\"GPU\",\n",
    "    choices=[\"GPU\", \"Ascend\"],\n",
    "    help=\"The target device to run, support 'Ascend', 'GPU'\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--device_id\", type=int, default=0, help=\"ID of the target device\"\n",
    ")\n",
    "parser.add_argument(\"--config_file_path\", type=str, default=\"./config.yaml\")\n",
    "args = parser.parse_args()\n",
    "context.set_context(mode=context.GRAPH_MODE if args.mode.upper().startswith(\"GRAPH\") else context.PYNATIVE_MODE,\n",
    "                    save_graphs=args.save_graphs,\n",
    "                    save_graphs_path=args.save_graphs_path,\n",
    "                    device_target=args.device_target,\n",
    "                    device_id=args.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config\n",
    "\n",
    "Import the corresponding dataset, CAE model, and optimizer parameter configuration from the `config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"./config.yaml\"\n",
    "# prepare params\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "data_params = config[\"data\"]\n",
    "model_params = config[\"cae_transformer\"]\n",
    "optimizer_params = config[\"optimizer\"]\n",
    "\n",
    "# prepare summary file\n",
    "summary_dir = optimizer_params[\"summary_dir\"]\n",
    "ckpt_dir = os.path.join(summary_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization\n",
    "\n",
    "Initialize the model according to the configuration in config.yaml, including CAE and Transformer networks.\n",
    "\n",
    "Use MSE loss function and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "model = CaeInformer(**model_params)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = nn.AdamWeightDecay(\n",
    "    model.trainable_params(),\n",
    "    optimizer_params[\"lr\"],\n",
    "    weight_decay=optimizer_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss\n",
    "\n",
    "time_now = time.time()\n",
    "model.set_train()\n",
    "\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=False)\n",
    "\n",
    "@jit\n",
    "def train_step(data, label):\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    loss = ops.depend(loss, optimizer(grads))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset\n",
    "\n",
    "Construct a CAE-Transforme dataset according to the data path read in config.yaml, and do data sinking.\n",
    "\n",
    "The link of the dataset is : [2D_cylinder_flow.npy](https://download.mindspore.cn/mindscience/mindflow/dataset/applications/data_driven/cae-transformer/2D_cylinder_flow.npy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "dataset, eval_data = create_caetransformer_dataset(\n",
    "    data_params['data_path'],\n",
    "    data_params[\"batch_size\"],\n",
    "    data_params[\"seq_len\"],\n",
    "    data_params[\"pred_len\"],\n",
    ")\n",
    "\n",
    "# data sink\n",
    "sink_process = data_sink(train_step, dataset, sink_size=1)\n",
    "train_data_size = dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "With version MindSpore >= 2.0.0, neural networks can be trained using the functional programming paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Start cae transformer train=====================\n",
      "epoch: 1 train loss: 1.6767721 epoch time: 20.05s\n",
      "epoch: 2 train loss: 1.3125554 epoch time: 11.43s\n",
      "epoch: 3 train loss: 1.0195727 epoch time: 11.36s\n",
      "epoch: 4 train loss: 0.78560066 epoch time: 11.45s\n",
      "epoch: 6 train loss: 0.45436704 epoch time: 11.63s\n",
      "epoch: 7 train loss: 0.34071732 epoch time: 11.28s\n",
      "epoch: 8 train loss: 0.25292587 epoch time: 11.78s\n",
      "epoch: 9 train loss: 0.18576089 epoch time: 11.50s\n",
      "epoch: 10 train loss: 0.13489997 epoch time: 11.36s\n",
      "=================Start cae-transformer evaluation=====================\n",
      "eval loss: 0.1344970128662193\n",
      "===================End transformer evaluation====================\n",
      "epoch: 11 train loss: 0.09680262 epoch time: 11.57s\n",
      "epoch: 12 train loss: 0.06859333 epoch time: 11.35s\n",
      "epoch: 13 train loss: 0.04796055 epoch time: 11.57s\n",
      "epoch: 14 train loss: 0.03306444 epoch time: 11.67s\n",
      "epoch: 15 train loss: 0.022457568 epoch time: 11.77s\n",
      "epoch: 16 train loss: 0.0150149 epoch time: 10.95s\n",
      "epoch: 17 train loss: 0.009872699 epoch time: 11.57s\n",
      "epoch: 18 train loss: 0.0063783163 epoch time: 11.32s\n",
      "epoch: 19 train loss: 0.004044703 epoch time: 11.27s\n",
      "epoch: 20 train loss: 0.0025148452 epoch time: 11.24s\n",
      "=================Start cae-transformer evaluation=====================\n",
      "eval loss: 0.0025034045636145035\n",
      "===================End transformer evaluation====================\n",
      "epoch: 21 train loss: 0.0015316431 epoch time: 10.97s\n",
      "epoch: 22 train loss: 0.00091252924 epoch time: 10.95s\n",
      "epoch: 23 train loss: 0.0005314545 epoch time: 11.07s\n",
      "epoch: 24 train loss: 0.00030210355 epoch time: 11.14s\n",
      "epoch: 25 train loss: 0.0001675461 epoch time: 11.27s\n",
      "epoch: 26 train loss: 9.066265e-05 epoch time: 11.31s\n",
      "epoch: 27 train loss: 4.7929763e-05 epoch time: 11.05s\n",
      "epoch: 28 train loss: 2.4869172e-05 epoch time: 11.33s\n",
      "epoch: 29 train loss: 1.2829796e-05 epoch time: 11.46s\n",
      "epoch: 30 train loss: 6.614075e-06 epoch time: 11.41s\n",
      "=================Start cae-transformer evaluation=====================\n",
      "eval loss: 6.312924863346573e-06\n",
      "===================End transformer evaluation====================\n",
      "====================End cae transformer train=======================\n",
      "=================Start cae-transformer prediction=====================\n",
      "===================End transformer prediction====================\n"
     ]
    }
   ],
   "source": [
    "print(f\"====================Start cae transformer train=======================\")\n",
    "train_loss = []\n",
    "model.set_train()\n",
    "for epoch in range(1, optimizer_params[\"epochs\"] + 1):\n",
    "    local_time_beg = time.time()\n",
    "    epoch_train_loss = 0\n",
    "    for _ in range(train_data_size):\n",
    "        epoch_train_loss = ops.squeeze(sink_process(), axis=())\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    print(f\"epoch: {epoch} train loss: {epoch_train_loss} epoch time: {time.time() - local_time_beg:.2f}s\")\n",
    "\n",
    "    if epoch % optimizer_params[\"save_ckpt_interval\"] == 0:\n",
    "        save_checkpoint(model, f\"{ckpt_dir}/model_{epoch}.ckpt\")\n",
    "    if epoch % optimizer_params[\"eval_interval\"] == 0:\n",
    "        model.set_train(False)\n",
    "        cae_transformer_eval(model, eval_data, data_params)\n",
    "        model.set_train(True)\n",
    "print(f\"=====================End cae train========================\")\n",
    "plot_train_loss(train_loss, summary_dir, optimizer_params[\"epochs\"], \"cae\")\n",
    "cae_transformer_prediction(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction result\n",
    "\n",
    "The following is a comparison of CAE-Transformer and the real value:\n",
    "\n",
    "<figure class=\"harf\">\n",
    "    <img src=\"./images/prediction_result.gif\" title=\"prediction result\" width=\"500\"/>\n",
    "</figure>\n",
    "\n",
    "The results show the velocity of different locations in the flow field over time. The average relative error between the predicted results and the true values is 6.3e-06"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
