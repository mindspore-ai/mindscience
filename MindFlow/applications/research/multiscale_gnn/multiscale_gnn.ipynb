{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiscale GNN Network\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "This notebook requires **MindSpore version >= 2.0.0** to support new APIs including: *mindspore.jit, mindspore.jit_class, mindspore.data_sink*. Please check [MindSpore Installation](https://www.mindspore.cn/install/en) for details.\n",
    "\n",
    "In addition, **MindFlow version >=0.1.0** is also required. If it has not been installed in your environment, please select the right version and hardware, then install it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindflow_version = \"0.1.0\"  # update if needed\n",
    "# GPU Comment out the following code if you are using NPU.\n",
    "!pip uninstall -y mindflow-gpu\n",
    "!pip install mindflow-gpu==$mindflow_version\n",
    "\n",
    "# NPU Uncomment if needed.\n",
    "# !pip uninstall -y mindflow-ascend\n",
    "# !pip install mindflow-ascend==$mindflow_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When solving the incompressible Navier-Stokes equations using projection method (or fractional step method), the projection step involves solving the large-scale Pressure Poisson Equation (PPE), which is typically the most computationally expensive and time-consuming step in the entire calculation process. A machine learning-based approach for solving the PPE problem is proposed, and a novel multi-scale Graph Neural Network (GNN) embedded solver is designed to accelerate the numerical solution of the incompressible Navier-Stokes equations. By replacing the traditional iterative solver for solving the PPE, the multi-scale GNN is seamlessly integrated into the numerical solution framework of the incompressible Navier-Stokes equations. In the multi-scale GNN framework, the original high-resolution graph corresponds to the discretized grid of the solution domain, graphs of the same resolution are connected through graph convolution operations, and graphs of different resolutions are connected through up-sampling and down-sampling operations. The well-trained multi-scale GNN serves as a universal PPE solver for a certain class of flow problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model framework\n",
    "\n",
    "The model framework is as shown in the following diagram:\n",
    "\n",
    "![MultiscaleGNN](images/MultiscaleGNN.png)\n",
    "\n",
    "where\n",
    "\n",
    "a. Numerical solution framework for the incompressible Navier-Stokes equations embedded with an ML-block (second-order explicit-implicit temporal discretization scheme)ï¼›\n",
    "\n",
    "b. ML-block (multi-scale GNN), $\\mathcal{G}^{1h}$ is the original high-resolution graph, $\\mathcal{G}^{2h}$, $\\mathcal{G}^{3h}$ and  $\\mathcal{G}^{4h}$ are the low-resolution graph of level-2, level-3 and level-4, respectively, and the number represents the number of neurons in the corresponding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Import code packs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import mindspore\n",
    "from mindspore import nn, ops, context, Tensor, COOTensor\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "from src.models import MultiScaleGNN, MultiScaleGNNStructure\n",
    "from src.datasets import read_training_data, read_test_data\n",
    "from src.datasets import second_order_derivative_matix2d\n",
    "from src.visualization import losses_curve, contourf_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setting of optional parameters, the setting of training hyperparameters and the construction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--grid_type', type=str, default='unstructure')\n",
    "parser.add_argument('--activ_fun', type=str, default='swish')\n",
    "parser.add_argument('--device', type=str, default='CPU')\n",
    "parser.add_argument('--batch_size', type=int, default=5)\n",
    "parser.add_argument('--lambda_p', type=int, default=1)\n",
    "parser.add_argument('--lambda_eq', type=int, default=1)\n",
    "args = parser.parse_args()\n",
    "\n",
    "grid_type = args.grid_type\n",
    "activ_fun = args.activ_fun\n",
    "device = args.device\n",
    "batch_size = args.batch_size\n",
    "lambda_p = args.lambda_p\n",
    "lambda_eq = args.lambda_eq\n",
    "\n",
    "context.set_context(device_target=device)\n",
    "\n",
    "nx = 512\n",
    "ny = 512\n",
    "dx = 2.0 * np.pi / nx\n",
    "dy = 2.0 * np.pi / ny\n",
    "\n",
    "t_train = np.arange(0.1, 5.1, 0.1)\n",
    "t_test = [2.0, 4.0, 6.0, 8.0]\n",
    "\n",
    "div_u_star_train, p_train = read_training_data(t_train, dx, dy)\n",
    "\n",
    "print('divUstar_train shape :', div_u_star_train.shape)\n",
    "print('p_train shape :', p_train.shape)\n",
    "print('divUstar_train max :', np.abs(div_u_star_train).max())\n",
    "print('p_train max :', np.abs(p_train).max())\n",
    "\n",
    "divUstar_test, p_test = read_test_data(t_test, dx, dy)\n",
    "print('divUstar_test shape :', divUstar_test.shape)\n",
    "print('p_test shape :', p_test.shape)\n",
    "print('divUstar_test max :', np.abs(divUstar_test).max())\n",
    "print('p_test max :', np.abs(p_test).max())\n",
    "\n",
    "batch = div_u_star_train.shape[0]//batch_size\n",
    "\n",
    "A0 = second_order_derivative_matix2d(nx, ny, dx, dy)\n",
    "\n",
    "save_dir = f'./Savers/{grid_type}/{activ_fun}_lambda_p{lambda_p}lambda_eq{lambda_eq}'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "A0_tensor = COOTensor(Tensor(np.stack((A0.row, A0.col), axis=1)), \\\n",
    "                      Tensor(A0.data, dtype=mindspore.float32), A0.shape)\n",
    "\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "if activ_fun == 'swish':\n",
    "    activation = nn.SiLU()\n",
    "if activ_fun == 'elu':\n",
    "    activation = nn.ELU()\n",
    "if activ_fun == 'gelu':\n",
    "    activation = nn.GELU()\n",
    "\n",
    "if grid_type == 'structure':\n",
    "    model = MultiScaleGNNStructure(in_channels, out_channels, activation)\n",
    "if grid_type == 'unstructure':\n",
    "    model = MultiScaleGNN(in_channels, out_channels, activation, A0)\n",
    "\n",
    "train_dataset = ds.NumpySlicesDataset(data=(div_u_star_train, p_train), shuffle=True)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size)\n",
    "\n",
    "learning_rate = nn.piecewise_constant_lr([500*batch, 1000*batch, 1500*batch, 2000*batch], [1e-3, 1e-4, 1e-5, 1e-6])\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=learning_rate, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of the training code includes the calculation of loss function, the forward calculation process and the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_norm(model):\n",
    "    weights_norm = 0.0\n",
    "    for name, param in model.parameters_and_names():\n",
    "        if 'weight' in name:\n",
    "            weights_norm += param.norm()**2\n",
    "    return weights_norm\n",
    "\n",
    "def calculate_loss_p(p_hat, p):\n",
    "    loss_p = ops.mean((p_hat - p)**2)\n",
    "    return loss_p\n",
    "\n",
    "def calculate_loss_eq(p_hat, b):\n",
    "    p_hat = p_hat * 0.01\n",
    "    p_hat = ops.cat((p_hat[:, -1:], p_hat, p_hat[:, 0:1]), axis=1)\n",
    "    p_hat = ops.cat((p_hat[:, :, -1:], p_hat, p_hat[:, :, 0:1]), axis=2)\n",
    "    b_hat = (p_hat[:, 0:-2, 1:-1] - 2 * p_hat[:, 1:-1, 1:-1] + p_hat[:, 2:, 1:-1]) / dx**2 + \\\n",
    "            (p_hat[:, 1:-1, 0:-2] - 2 * p_hat[:, 1:-1, 1:-1] + p_hat[:, 1:-1, 2:]) / dy**2\n",
    "    loss_eq = ops.mean((b_hat - b)**2)\n",
    "    return loss_eq\n",
    "\n",
    "def forward_fn(b, p):\n",
    "    if grid_type == 'unstructure':\n",
    "        inputs = b.reshape([-1, 1])\n",
    "        p_hat = model(inputs)\n",
    "        p_hat = p_hat.reshape([batch_size, nx, ny])\n",
    "    if grid_type == 'structure':\n",
    "        inputs = b.unsqueeze(1)\n",
    "        p_hat = model(inputs)\n",
    "        p_hat = p_hat.squeeze(1)\n",
    "    loss_p = calculate_loss_p(p_hat, p)\n",
    "    loss_eq = calculate_loss_eq(p_hat, b)\n",
    "    loss = lambda_p * loss_p + lambda_eq * loss_eq\n",
    "    return loss, loss_p, loss_eq\n",
    "\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_step(b, p):\n",
    "    (loss, loss_p, loss_eq), grads = grad_fn(b, p)\n",
    "    optimizer(grads)\n",
    "    return loss, loss_p, loss_eq\n",
    "\n",
    "def run_train(model, num_epochs, restore=False):\n",
    "    if not os.path.exists(save_dir+'/checkpoint/'):\n",
    "        os.makedirs(save_dir+'/checkpoint/')\n",
    "    if not os.path.exists(save_dir+'/Figures/'):\n",
    "        os.makedirs(save_dir+'/Figures/')\n",
    "    if restore:\n",
    "        loss_all = sio.loadmat(save_dir+'/loss_all.mat')['loss_all'].tolist()\n",
    "        param_dict = mindspore.load_checkpoint(save_dir+'/checkpoint/model.ckpt')\n",
    "        param_not_load, _ = mindspore.load_param_into_net(model, param_dict)\n",
    "        print(param_not_load)\n",
    "        model.set_train()\n",
    "\n",
    "    else:\n",
    "        loss_all = []\n",
    "\n",
    "    print('Training......')\n",
    "    model.set_train()\n",
    "    weights_norm = get_weights_norm(model)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        loss_p_it, loss_eq_it = [], []\n",
    "        for b, p in train_dataset.create_tuple_iterator():\n",
    "            _, loss_p, loss_eq = train_step(b, p)\n",
    "\n",
    "            loss_p_it.append(loss_p.asnumpy())\n",
    "            loss_eq_it.append(loss_eq.asnumpy())\n",
    "\n",
    "            print('Epoch: %d, weights_norm: %.3e, loss_p: %.3e, loss_eq: %.3e' % (epoch, weights_norm.asnumpy(), np.mean(loss_p_it), np.mean(loss_eq_it)))\n",
    "        weights_norm = get_weights_norm(model)\n",
    "        print('***********************************************************')\n",
    "        loss_all.append([weights_norm.asnumpy(), np.mean(loss_p_it), np.mean(loss_eq_it)])\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            mindspore.save_checkpoint(model, save_dir+'/checkpoint/model.ckpt')\n",
    "            sio.savemat(save_dir+'/loss_all.mat', {'loss_all':np.array(loss_all)})\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            losses_curve(np.array(loss_all), save_dir+'/Figures/')\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            model.set_train(False)\n",
    "            if grid_type == 'unstructure':\n",
    "                inputs = Tensor(divUstar_test).reshape([-1, 1])\n",
    "                p_hat = model(inputs)\n",
    "                p_hat = p_hat.reshape([4, nx, ny]).asnumpy()\n",
    "            if grid_type == 'structure':\n",
    "                inputs = Tensor(divUstar_test).unsqueeze(1)\n",
    "                p_hat = model(inputs)\n",
    "                p_hat = p_hat.squeeze(1).asnumpy()\n",
    "            for k in range(4):\n",
    "                contourf_comparison(p_test[k], p_hat[k], save_dir+f'/Figures/{k}')\n",
    "\n",
    "            model.set_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(model, num_epochs=2000, restore=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test parameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_type = 'structure'\n",
    "activ_fun = 'swish'\n",
    "device = 'CPU'\n",
    "lambda_p = 50\n",
    "lambda_eq = 1\n",
    "plot_figure = 1\n",
    "\n",
    "context.set_context(device_target=device)\n",
    "\n",
    "nx = 512\n",
    "ny = 512\n",
    "dx = 2.0*np.pi/nx\n",
    "dy = 2.0*np.pi/ny\n",
    "\n",
    "t_test = [2.0, 4.0, 6.0, 8.0, 10.0]\n",
    "\n",
    "divUstar_test, p_test = read_test_data(t_test, dx, dy)\n",
    "print('divUstar_test shape :', divUstar_test.shape)\n",
    "print('p_test shape :', p_test.shape)\n",
    "print('divUstar_test max :', np.abs(divUstar_test).max())\n",
    "print('p_test max :', np.abs(p_test).max())\n",
    "\n",
    "A0 = second_order_derivative_matix2d(nx, ny, dx, dy)\n",
    "\n",
    "save_dir = f'./Savers/{grid_type}/{activ_fun}_lambda_p{lambda_p}lambda_eq{lambda_eq}'\n",
    "\n",
    "A0_tensor = COOTensor(Tensor(np.stack((A0.row, A0.col), axis=1)), \\\n",
    "                      Tensor(A0.data, dtype=mindspore.float32), A0.shape)\n",
    "\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "if activ_fun == 'swish':\n",
    "    activation = nn.SiLU()\n",
    "if activ_fun == 'elu':\n",
    "    activation = nn.ELU()\n",
    "if activ_fun == 'gelu':\n",
    "    activation = nn.GELU()\n",
    "\n",
    "if grid_type == 'structure':\n",
    "    model = MultiScaleGNNStructure(in_channels, out_channels, activation)\n",
    "if grid_type == 'unstructure':\n",
    "    model = MultiScaleGNN(in_channels, out_channels, activation, A0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the main function for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model):\n",
    "    loss_all = sio.loadmat(save_dir+'/loss_all.mat')['loss_all'].tolist()\n",
    "    param_dict = mindspore.load_checkpoint(save_dir+'/checkpoint/model.ckpt')\n",
    "    param_not_load, _ = mindspore.load_param_into_net(model, param_dict)\n",
    "    print(param_not_load)\n",
    "    model.set_train(False)\n",
    "\n",
    "    losses_curve(np.array(loss_all), save_dir+'/Figures/')\n",
    "\n",
    "    p_hat = []\n",
    "    for it in range(divUstar_test.shape[0]//5):\n",
    "        if grid_type == 'unstructure':\n",
    "            b = Tensor(divUstar_test[it*5:(it+1)*5]).reshape([-1, 1])\n",
    "            p = model(b)\n",
    "            p = p.reshape([5, nx, ny]).asnumpy()\n",
    "            p_hat.append(p)\n",
    "        if grid_type == 'structure':\n",
    "            b = Tensor(divUstar_test[it*5:(it+1)*5]).unsqueeze(1)\n",
    "            p = model(b)\n",
    "            p = p.squeeze(1).asnumpy()\n",
    "            p_hat.append(p)\n",
    "\n",
    "    p_hat = np.concatenate(p_hat, axis=0)\n",
    "\n",
    "    print('####################################')\n",
    "    MAE = np.mean((p_hat-p_test)**2, axis=(1, 2))\n",
    "    print('mean absolute error:')\n",
    "    print(MAE)\n",
    "    print('average mean absolute error:')\n",
    "    print(MAE.mean())\n",
    "\n",
    "    print('####################################')\n",
    "    RL2E = np.linalg.norm(p_hat-p_test, axis=(1, 2))**2/np.linalg.norm(p_test, axis=(1, 2))**2\n",
    "    print('relative L2 error:')\n",
    "    print(RL2E)\n",
    "    print('average relative L2 error:')\n",
    "    print(RL2E.mean())\n",
    "\n",
    "    if plot_figure:\n",
    "        for k in range(divUstar_test.shape[0]):\n",
    "            contourf_comparison(p_test[k], p_hat[k], save_dir+f'/Figures/{k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "57ace93c29d9374277a79956c3f1b916d7d9a05468d906842f9921d0d494a29f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
