{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MindSpore自动微分快速教程\n",
    "\n",
    "MindSpore拥有完善的自动微分系统。本文将会借着对自动微分思想的介绍来展示MindSpore自动微分的各项能力，方便读者运用在自己的项目中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import numpy as np\n",
    "import mindspore.ops as ops\n",
    "from mindspore import context\n",
    "from mindspore import Tensor\n",
    "from mindspore import grad\n",
    "from mindspore import dtype as mstype\n",
    "\n",
    "context.set_context(mode=ms.GRAPH_MODE)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.070650816\n"
     ]
    }
   ],
   "source": [
    "grad_tanh = grad(ops.tanh)\n",
    "print(grad_tanh(Tensor(2, mstype.float32)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad` 的入参为一个函数，返回的是求导后的函数。定义一个Python函数`f`用来计算数学函数$f$，`grad(f)` 就是表达$\\nabla f$的Python函数。 `grad(f)(x)` 就是$\\nabla f(x)$的值。\n",
    "\n",
    "由于 `grad` 作用在函数上，所以`grad`也可以用来处理它自己的输出：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13621867\n",
      "0.25265405\n"
     ]
    }
   ],
   "source": [
    "print(grad(grad(ops.tanh))(Tensor(2, mstype.float32)))\n",
    "print(grad(grad(grad(ops.tanh)))(Tensor(2, mstype.float32)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个计算线性回归模型的梯度的例子，首先：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5 * (ops.tanh(x / 2) + 1)\n",
    "\n",
    "# Outputs probability of a label being true.\n",
    "\n",
    "\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(ops.inner(inputs, W) + b)\n",
    "\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = Tensor(np.array([[0.52, 1.12, 0.77],\n",
    "                          [0.88, -1.08, 0.15],\n",
    "                          [0.52, 0.06, -1.30],\n",
    "                          [0.74, -2.49, 1.39]]), ms.float32)\n",
    "targets = Tensor(np.array([True, True, False, True]))\n",
    "\n",
    "# Training loss is the negative log-likelihood of the training examples.\n",
    "\n",
    "\n",
    "def loss(W, b):\n",
    "    preds = predict(W, b, inputs)\n",
    "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -ops.sum(ops.log(label_probs))\n",
    "\n",
    "\n",
    "# Initialize random model coefficients\n",
    "W = Tensor(np.random.rand(3,), ms.float32)\n",
    "b = Tensor(np.random.rand(), ms.float32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在`grad` 中使用 `grad_position`对指定的位置参数进行微分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad [-0.5185027  1.5961987 -1.5178145]\n",
      "W_grad [-0.5185027  1.5961987 -1.5178145]\n",
      "b_grad -0.49954596\n",
      "W_grad [-0.5185027  1.5961987 -1.5178145]\n",
      "b_grad -0.49954596\n"
     ]
    }
   ],
   "source": [
    "# Differentiate `loss` with respect to the first positional argument:\n",
    "W_grad = grad(loss, grad_position=0)(W, b)\n",
    "print('W_grad', W_grad)\n",
    "\n",
    "# Since argnums=0 is the default, this does the same thing:\n",
    "W_grad = grad(loss)(W, b)\n",
    "print('W_grad', W_grad)\n",
    "\n",
    "# But we can choose different values too, and drop the keyword:\n",
    "b_grad = grad(loss, 1)(W, b)\n",
    "print('b_grad', b_grad)\n",
    "\n",
    "# Including tuple values\n",
    "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
    "print('W_grad', W_grad)\n",
    "print('b_grad', b_grad)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本质上来说，使用`grad_position`时，如果`f`是一个Python函数，那么表达式`grad(f, i)`就是在求偏微分$\\partial_i f$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `value_and_grad`:同时获得函数值与梯度\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value_and_grad`可以方便地同时计算函数值和梯度值：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value 2.0792074\n",
      "loss value 2.0792074\n"
     ]
    }
   ],
   "source": [
    "from mindspore import value_and_grad\n",
    "loss_value, Wb_grad = value_and_grad(loss, (0, 1))(W, b)\n",
    "print('loss value', loss_value)\n",
    "print('loss value', loss(W, b))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与数值计算结果比较\n",
    "\n",
    "自动微分可以很直接地与有限微分比较：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_grad_numerical -0.500679\n",
      "b_grad_autodiff -0.49954596\n",
      "W_dirderiv_numerical -1.7213821\n",
      "W_dirderiv_autodiff -1.71724\n"
     ]
    }
   ],
   "source": [
    "# Set a step size for finite differences calculations\n",
    "eps = 1e-4\n",
    "\n",
    "# Check b_grad with scalar finite differences\n",
    "b_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\n",
    "print('b_grad_numerical', b_grad_numerical)\n",
    "print('b_grad_autodiff', grad(loss, 1)(W, b))\n",
    "\n",
    "# Check W_grad with finite differences in a random direction\n",
    "# key, subkey = random.split(key)\n",
    "vec = Tensor(np.random.normal(size=W.shape), mstype.float32)\n",
    "unitvec = vec / ops.sqrt(ops.inner(vec, vec))\n",
    "unitvec = unitvec.reshape(W.shape)\n",
    "W_grad_numerical = (loss(W + eps / 2. * unitvec, b) -\n",
    "                    loss(W - eps / 2. * unitvec, b)) / eps\n",
    "print('W_dirderiv_numerical', W_grad_numerical)\n",
    "print('W_dirderiv_autodiff', ops.inner(grad(loss)(W, b), unitvec))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `grad`+`grad`得到Hessian向量积\n",
    "\n",
    "使用高阶`grad`可以构造Hessian向量积。(后面我们会用前向模式和反向模式写一个更高效的实现)\n",
    "\n",
    "Hessian向量积可用来在[截断的牛顿共轭梯度法](https://en.wikipedia.org/wiki/Truncated_Newton_method)中最小化一个光滑的凸函数，或者用来判断神经网络训练目标的曲率性质。(如 [1](https://arxiv.org/abs/1406.2572), [2](https://arxiv.org/abs/1811.07062), [3](https://arxiv.org/abs/1706.04454), [4](https://arxiv.org/abs/1802.03451)).\n",
    "\n",
    "对于一个有着连续二阶导的标量函数（这种函数的Hessian矩阵是对称的）$f : \\mathbb{R}^n \\to \\mathbb{R}$，点$x \\in \\mathbb{R}^n$处的Hessian算子为$\\partial^2 f(x)$。一个Hessian向量积用来计算映射：\n",
    "\n",
    "$\\qquad v \\mapsto \\partial^2 f(x) \\cdot v$\n",
    "\n",
    "其中 $v \\in \\mathbb{R}^n$。\n",
    "\n",
    "有一个技巧是我们不能实例化整个Hessian矩阵：如果$n$很大的话（神经网络中可能达到百万或上亿的量级），完整的Hessian矩阵是没法存储的。\n",
    "\n",
    "幸运的是, `grad` 提供了一种高效计算Hessian向量积的方式。我们只需要有恒等式：\n",
    "\n",
    "$\\qquad \\partial^2 f (x) v = \\partial [x \\mapsto \\partial f(x) \\cdot v] = \\partial g(x)$，\n",
    "\n",
    "其中 $g(x) = \\partial f(x) \\cdot v$ 是一个新的标量函数，其表示 $f$在$x$的梯度与向量$v$的点乘。这里只涉及对标量函数的向量值的微分，这种情形下 `grad` 是高效的。\n",
    "\n",
    "用MindSpore代码，我们可以写出：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(f, x, v):\n",
    "    return grad(lambda x: ops.inner(grad(f)(x), v))(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个例子表明我们可以自由的使用词汇闭包，MindSpore都可以正确处理。在后面我会看到Hessian矩阵是怎么被计算出来的，知晓了原理之后我们会同时运用前向模式和反向模式提供一个更高效的写法。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运用 `jacfwd` 和 `jacrev` 计算Jacobians 和 Hessians 矩阵\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户可以用 `jacfwd` 和 `jacrev`计算Jacobian矩阵：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacfwd result, with shape (4, 3)\n",
      "[[ 0.05072299  0.10924952  0.07510904]\n",
      " [ 0.21355031 -0.26208448  0.03640062]\n",
      " [ 0.12973952  0.01496994 -0.3243488 ]\n",
      " [ 0.18499702 -0.62249     0.3474944 ]]\n",
      "jacrev result, with shape (4, 3)\n",
      "[[ 0.05072299  0.10924952  0.07510904]\n",
      " [ 0.21355031 -0.26208448  0.03640062]\n",
      " [ 0.12973952  0.01496994 -0.3243488 ]\n",
      " [ 0.18499702 -0.62249     0.3474944 ]]\n"
     ]
    }
   ],
   "source": [
    "from mindspore import jacfwd, jacrev\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "\n",
    "\n",
    "def f(W):\n",
    "    return predict(W, b, inputs)\n",
    "\n",
    "\n",
    "J = jacfwd(f)(W)\n",
    "print(\"jacfwd result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "J = jacrev(f)(W)\n",
    "print(\"jacrev result, with shape\", J.shape)\n",
    "print(J)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两个函数得到的结果应该是一样的，二者只是实现方式不通： `jacfwd` 使用的是前向模式的自动微分，在比较\"高\"的Jacobian矩阵上较高效。 `jacrev` 使用的是反向模式，在\"宽\"的矩阵上更高效。对于比较方正的矩阵， `jacfwd` 效果稍好于`jacrev`。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于前向模式和反向模式的更多信息，请继续阅读！\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用一种组合的方式计算dense的Hessian矩阵：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hessian, with shape (4, 3, 3)\n",
      "[[[-2.0597292e-02 -4.4363402e-02 -3.0499836e-02]\n",
      "  [-4.4363398e-02 -9.5551945e-02 -6.5691955e-02]\n",
      "  [-3.0499836e-02 -6.5691963e-02 -4.5163218e-02]]\n",
      "\n",
      " [[-3.2176636e-02  3.9489504e-02 -5.4846536e-03]\n",
      "  [ 3.9489508e-02 -4.8464395e-02  6.7311660e-03]\n",
      "  [-5.4846536e-03  6.7311660e-03 -9.3488418e-04]]\n",
      "\n",
      " [[-3.0198938e-03 -3.4844928e-04  7.5497343e-03]\n",
      "  [-3.4844928e-04 -4.0205687e-05  8.7112316e-04]\n",
      "  [ 7.5497343e-03  8.7112322e-04 -1.8874336e-02]]\n",
      "\n",
      " [[-5.4928247e-04  1.8482616e-03 -1.0317604e-03]\n",
      "  [ 1.8482613e-03 -6.2191500e-03  3.4717342e-03]\n",
      "  [-1.0317604e-03  3.4717345e-03 -1.9380364e-03]]]\n"
     ]
    }
   ],
   "source": [
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "\n",
    "H = hessian(f)(W)\n",
    "print(\"hessian, with shape\", H.shape)\n",
    "print(H)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的shape是合理的：$f : \\mathbb{R}^n \\to \\mathbb{R}^m$, 在点 $x \\in \\mathbb{R}^n$ 上，会有shape\n",
    "\n",
    "- $f(x) \\in \\mathbb{R}^m$, $f$ 在 $x$ 处的值，\n",
    "- $\\partial f(x) \\in \\mathbb{R}^{m \\times n}$, $x$ 处的Jacobian矩阵，\n",
    "- $\\partial^2 f(x) \\in \\mathbb{R}^{m \\times n \\times n}$, $x$ 处的Hessian矩阵\n",
    "\n",
    "`jacfwd(jacrev(f))` 或 `jacrev(jacfwd(f))` 或者二者任意的组合皆可实现一个`hessian`矩阵，只是 forward+reverse一般情况下是效率最高的方式。 这是因为里面一层的Jacobian计算经常会有针对宽Jacobian矩阵的微分（比如loss function $f : \\mathbb{R}^n \\to \\mathbb{R}$），在外面那一层的Jacobian 计算 通常是微分一个正方矩阵（因为会有$\\nabla f : \\mathbb{R}^n \\to \\mathbb{R}^n$），这时forward-mode速度更快。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深入理解两个基本的自动微分函数\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian向量积 (JVPs, 前向模式自动微分)\n",
    "\n",
    "MindSpore对前向和反向的自动微分都提供了高效且泛用性强的实现。我们熟悉的 `grad` 是基于反向模式实现的，不过为了理解二者的区别，我们需要一点数学背景。\n",
    "\n",
    "### JVPs的数学背景\n",
    "\n",
    "从数学的角度看，给定一个函数 $f : \\mathbb{R}^n \\to \\mathbb{R}^m$，$f$ 在输入点 $x \\in \\mathbb{R}^n$ 的Jacobian矩阵可被记作 $\\partial f(x)$，通常型如 $\\mathbb{R}^m \\times \\mathbb{R}^n$:\n",
    "\n",
    "$\\qquad \\partial f(x) \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "我们可以将 $\\partial f(x)$ 视为线性映射，把在点 $x$ 处 $f$ 定义域上的正切空间（ 其实就是 $\\mathbb{R}^n$ 的一份拷贝）映射到了在点 $f(x)$ 处 $f$ 陪域上的正切空间（$\\mathbb{R}^m$ 的拷贝）。\n",
    "\n",
    "$\\qquad \\partial f(x) : \\mathbb{R}^n \\to \\mathbb{R}^m$.\n",
    "\n",
    "这个映射又被称作 $f$ 在 $x$ 的[前推映射](https://en.wikipedia.org/wiki/Pushforward_(differential))。Jacobian矩阵只是这个线性映射在标准情况下的矩阵形式。\n",
    "\n",
    "如果我们不拘泥于一个特定的点 $x$，那么函数 $\\partial f$ 可被视为先取一个输入点然后返回那个点上的Jacobian线性映射：\n",
    "\n",
    "$\\qquad \\partial f : \\mathbb{R}^n \\to \\mathbb{R}^n \\to \\mathbb{R}^m$.\n",
    "\n",
    "尤其是，做反curring时，给定输入 $x \\in \\mathbb{R}^n$ 和切向量 $v \\in \\mathbb{R}^n$，返回一个输出切向量 $\\mathbb{R}^m$。我们把从 $(x, v)$ 到输出切向量的映射称之为 _Jacobian向量积_，写作：\n",
    "\n",
    "$\\qquad (x, v) \\mapsto \\partial f(x) v$\n",
    "\n",
    "### MindSpore中的JVP\n",
    "\n",
    "回到Python代码上，MindSpore的 `jvp` 函数模拟了上述转换。 给定一个Python函数 $f$, MindSpore的 `jvp` 可以得到一个表达 $(x, v) \\mapsto (f(x), \\partial f(x) v)$ 的函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89045584 0.5856106  0.52238137 0.5020062 ] [ 0.01188576  0.00967572 -0.15435933  0.17893277]\n"
     ]
    }
   ],
   "source": [
    "from mindspore import jvp\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "\n",
    "\n",
    "def f(W):\n",
    "    return predict(W, b, inputs)\n",
    "\n",
    "\n",
    "v = Tensor(np.random.normal(size=W.shape), mstype.float32)\n",
    "# Push forward the vector `v` along `f` evaluated at `W`\n",
    "y, u = jvp(f, (W), (v))\n",
    "print(y, u)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照[Haskell类型风格](https://wiki.haskell.org/Type_signature), 有：\n",
    "\n",
    "```haskell\n",
    "jvp :: (a -> b) -> a -> T a -> (b, T b)\n",
    "```\n",
    "\n",
    "在这里，我们用 `T a` 表示 `a` 切空间的类型。简而言之， `jvp` 的参数有 `a -> b`类型函数,、 `a` 类型的值和`T a`切向量。返回的是`b`类型的值和`T b` 类型的切向量。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jvp`的计算方式与原始函数很相似，但它与每个`a`类型的原始值配对，并推送`T a`类型的切线值。对于每个原始函数会应用的原始数值操作，`jvp`转换后的函数为该原始函数执行一个 \"JVP规则\"，既对原始值进行评估，又在这些原始值上应用原始的JVP。\n",
    "\n",
    "这种计算策略对计算的复杂度有直接的影响：因为在计算JVP的过程中不用存储任何东西，所以空间开销和计算的深度完全无关。除此之外， `jvp` 转换过的函数FLOP开销约是原函数的3倍 （一份来自原函数的计算，比如 `sin(x)`； 一份来自线性化，如 `cos(x)`；还有一份来自于将线性化函数施加在向量上，如 `cos_x * v`）。 换句话说，对于固定的点 $x$，我们计算 $v \\mapsto \\partial f(x) \\cdot v$ 和计算 $f$的边际成本是相近的。\n",
    "\n",
    "这里的空间复杂度看起来很有说服力，但我们在机器学习中并不经常见到前向模式。\n",
    "\n",
    "为了回答这个问题，首先假设要用JVP构建一个完整的Jacobian矩阵。如果我们是对一个one-hot切向量用了JVP，结果反映的是Jacobian矩阵的一列，对应填入的非零项。所以我们是可以通过一次构建一列的方式构建一个完整的Jacobian矩阵的，而且每一列的开销和一次函数计算差不多。这就意味这对于\"高\"的Jaocbian矩阵来说比较合算，但对于\"宽\"的就较为低效。\n",
    "\n",
    "如果在机器学习中做基于梯度的优化，你可能想要最小化损失函数，这个损失函数以 $\\mathbb{R}^n$ 为参数，返回一个标量值$\\mathbb{R}$。 这就意味着该函数的Jacobian矩阵会很宽了：$\\partial f(x) \\in \\mathbb{R}^{1 \\times n}$，一般我们会认为和梯度向量 $\\nabla f(x) \\in \\mathbb{R}^n$ 一样。一次一列地构建这个矩阵，而且每列的FLOP和原函数计算一次的开销差不多，这个开销当然是不小的。尤其是，对于训练神经网络来说，损失函数 $f$ 的 $n$ 可以达到上亿的量级，这就更暴露出前向模式的问题了。\n",
    "\n",
    "为了解决这种问题，就需要反向模式了。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量Jacobian 积 (VJP, 反向模式自动微分)\n",
    "\n",
    "和前向模式的一次一列的方式不同，反向模式的构造方式是一次一行。\n",
    "\n",
    "### VJPs 的数学背景\n",
    "\n",
    "首先考虑有 $f : \\mathbb{R}^n \\to \\mathbb{R}^m$。 其VJP表达为：\n",
    "\n",
    "$\\qquad (x, v) \\mapsto v \\partial f(x)$,\n",
    "\n",
    "其中 $v$ 是 $f$ 在 $x$ 的余切空间($\\mathbb{R}^m$ 的同构)。严谨来说，$v$ 是线性映射 $v : \\mathbb{R}^m \\to \\mathbb{R}$， $v \\partial f(x)$ 指的是复合函数 $v \\circ \\partial f(x)$，在 $\\partial f(x) : \\mathbb{R}^n \\to \\mathbb{R}^m$ 时成立。 不过通常 $v$ 都可以视为 $\\mathbb{R}^m$ 中的向量，这两个写法基本可以互换。\n",
    "\n",
    "有了这些说明后，我们把VJP的线性部分视为JVP线性部分的转置（或伴随、共轭）：\n",
    "\n",
    "$\\qquad (x, v) \\mapsto \\partial f(x)^\\mathsf{T} v$.\n",
    "\n",
    "对点 $x$，有：\n",
    "\n",
    "$\\qquad \\partial f(x)^\\mathsf{T} : \\mathbb{R}^m \\to \\mathbb{R}^n$.\n",
    "\n",
    "对余切空间的映射通常称为 $f$ 在 $x$ 的[拉回](https://en.wikipedia.org/wiki/Pullback_(differential_geometry))。理解的关键在于拉回会从形似输出 $f$ 的形式得到形似输入 $f$ 的形式，就像线性函数转置一样。\n",
    "\n",
    "### MindSpore中使用VJP\n",
    "\n",
    "MindSpore `vjp` 以一个python函数 $f$ 为输入，返回表示 VJP $(x, v) \\mapsto (f(x), v^\\mathsf{T} \\partial f(x))$ 的函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6064372  -1.1690241   0.32237193]\n"
     ]
    }
   ],
   "source": [
    "from mindspore import vjp\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "\n",
    "\n",
    "def f(W):\n",
    "    return predict(W, b, inputs)\n",
    "\n",
    "\n",
    "y, vjp_fun = vjp(f, W)\n",
    "\n",
    "u = Tensor(np.random.normal(size=y.shape), mstype.float32)\n",
    "\n",
    "# Pull back the covector `u` along `f` evaluated at `W`\n",
    "v = vjp_fun(u)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仿照 [Haskell-like type signatures](https://wiki.haskell.org/Type_signature), 有\n",
    "\n",
    "```haskell\n",
    "vjp :: (a -> b) -> a -> (b, CT b -> CT a)\n",
    "```\n",
    "\n",
    "其中，我们用`CT a`来表示`a`的余切空间的类型。换句话说，`vjp`将一个`a -> b`类型的函数和一个`a`类型的点作为参数，并返回一个由`b`类型的值和`CT b -> CT a`类型的线性映射组成的对。\n",
    "\n",
    "VJP一个优良的性质在于VJP是按行构建Jacobian矩阵， $(x, v) \\mapsto (f(x), v^\\mathsf{T} \\partial f(x))$ 的FLOP仅为计算 $f$ 的三倍左右。而且计算 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 的梯度，我们只需要一次VJP就够了。这就是为什么 `grad` 在大的神经网络中做梯度优化依然高效。\n",
    "\n",
    "不过还有一点需要考虑一下： 尽管 FLOP开销不高，VJP的空间复杂度是随计算深度上升而上升的。而且实现上通常比前向模式复杂。\n",
    "\n",
    "反向模式的更多说明请参阅 [this tutorial video from the Deep Learning Summer School in 2017](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VJP计算梯度向量\n",
    "\n",
    "可以用VJP得到梯度向量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6.]\n",
      " [6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "from mindspore import vjp\n",
    "\n",
    "context.set_context(mode=ms.PYNATIVE_MODE)\n",
    "\n",
    "\n",
    "def vgrad(f, x):\n",
    "    y, vjp_fn = vjp(f, x)\n",
    "    return vjp_fn(ops.ones(y.shape))[0]\n",
    "\n",
    "\n",
    "print(vgrad(lambda x: 3*x**2, ops.ones((2, 2))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用前向和反向模式得到Hessian向量积\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仅用反向模式得到Hessian向量积的实现：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(f, x, v):\n",
    "    return grad(lambda x: ops.inner(grad(f)(x), v))(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过组合使用前反向的方法我们可以得到更高效的实现。\n",
    "\n",
    "设有待微分函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$ ， 在点 $x \\in \\mathbb{R}^n$ 线性化函数，并有向量 $v \\in \\mathbb{R}^n$。 Hessian向量积函数为：\n",
    "\n",
    "$(x, v) \\mapsto \\partial^2 f(x) v$\n",
    "\n",
    "构造helper function $g : \\mathbb{R}^n \\to \\mathbb{R}^n$，定义为 $f$ 的导数（或梯度）, 即 $g(x) = \\partial f(x)$。使用一次JVP，便得到：\n",
    "\n",
    "$(x, v) \\mapsto \\partial g(x) v = \\partial^2 f(x) v$。\n",
    "\n",
    "用代码写作：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import jvp, grad\n",
    "\n",
    "# forward-over-reverse\n",
    "\n",
    "\n",
    "def hvp(f, primals, tangents):\n",
    "    return jvp(grad(f), primals, tangents)[1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们不需要 `ops.inner`，该 `hvp` 函数对任何shape的数组都成立。\n",
    "\n",
    "以下是该函数的一个样例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def f(X):\n",
    "    return ops.sum(ops.tanh(X)**2)\n",
    "\n",
    "\n",
    "X = Tensor(np.random.normal(size=(30, 40)), mstype.float32)\n",
    "V = Tensor(np.random.normal(size=(30, 40)), mstype.float32)\n",
    "\n",
    "ans1 = hvp(f, (X), (V))\n",
    "ans2 = ms.numpy.tensordot(hessian(f)(X), V, 2)\n",
    "\n",
    "print(np.allclose(ans1.numpy(), ans2.numpy(), 1e-4, 1e-4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以考虑写一种先前向后反向的方式：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse-over-forward\n",
    "def hvp_revfwd(f, primals, tangents):\n",
    "    def g(primals):\n",
    "        return jvp(f, primals, tangents)[1]\n",
    "    return grad(g)(primals)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过这就不是很高效了，因为前向模式比反向模式的开销低一些，而且由于外层微分算子计算量比内层的要大，继续在外层用前向模式反而更好：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward over reverse\n",
      "297 ms ± 9.5 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "Reverse over forward\n",
      "2.48 ms ± 257 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "Reverse over reverse\n",
      "4.44 ms ± 51.9 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "Naive full Hessian materialization\n",
      "1.23 s ± 13.6 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# reverse-over-reverse, only works for single arguments\n",
    "context.set_context(mode=ms.PYNATIVE_MODE)\n",
    "\n",
    "\n",
    "def hvp_revrev(f, primals, tangents):\n",
    "    x = primals\n",
    "    v = tangents\n",
    "    return grad(lambda x: ops.inner(grad(f)(x), v))(x)\n",
    "\n",
    "\n",
    "print(\"Forward over reverse\")\n",
    "%timeit - n10 - r3 hvp(f, (X), (V))\n",
    "print(\"Reverse over forward\")\n",
    "%timeit - n10 - r3 hvp_revfwd(f, (X), (V))\n",
    "print(\"Reverse over reverse\")\n",
    "%timeit - n10 - r3 hvp_revrev(f, (X), (V))\n",
    "print(\"Naive full Hessian materialization\")\n",
    "%timeit - n10 - r3 ms.numpy.tensordot(hessian(f)(X), V, 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
